---
layout: post
title: Stf CS149 flash attention
date: 2024-11-12 07:59:00-0400
description:  
tags:  ai ml transformer  
categories: ml
featured: false
---


[Stf CS149 flash attention lab assignment repo](https://github.com/stanford-cs149/cs149gpt)


## Install library to compile code 

It shows error about error loading shared object.

```
(cs149gpt) ➜  cs149gpt git:(main) ✗ python3 gpt149.py 4Daccess
/home/zt/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)                                                                                        return torch._C._cuda_getDeviceCount() > 0
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 14, in <module>
    import module_ref as ms
ImportError: /home/zt/stf-cs149-pp/cs149gpt/module_ref.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE
```


I tried to use conda to create new env and install low version of pytorch but conda 
always installs 2.3.x version of torch for me.
```
 conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 cpuonly python=3.10 numpy=1.26 ninja tiktoken -c pytorch -c conda-forge
```
What it is required is 2.1.x version of torch.


Then I manually use pip to uninstall torch and reinstall 2.1.x version of torch
```
 pip3 uninstall torch
pip3 install torch==2.1.2

```


Got another issue saying that I should use numpy version < 2.0.

Then I uninstall numpy and reinstall it with version 1.2.x
```
pip3 uninstall numpy
 pip3 install numpy==1.26
```


So now I can run the code successfully.


## Part 1: naive attention 


My code produce the value that is 0.0003 less than what solution produces for each element.

I don't know why.

Should I use double ? 

First implementation:
```cpp
torch::Tensor myNaiveAttention(torch::Tensor QTensor, torch::Tensor KTensor, torch::Tensor VTensor, torch::Tensor QK_tTensor,
                int B, int H, int N, int d){

    // Q, K, V are passed in with Shape: (B, H, N, d)
    //QK^t Intermediate Tensor has Shape (N, N)
    
    //Make O Tensor with Shape (B, H, N, d) 
    at::Tensor OTensor = at::zeros({B, H, N, d}, at::kFloat);

    //Format O, Q, K, and V tensors into 4D vectors
    std::vector<float> O = formatTensor(OTensor);
    std::vector<float> Q = formatTensor(QTensor);
    std::vector<float> K = formatTensor(KTensor);
    std::vector<float> V = formatTensor(VTensor);

    //Format QK_t Tensor into a 2D vector.
    std::vector<float> QK_t = formatTensor(QK_tTensor);
    
    /* Here is an example of how to read/write 0's to  Q (B, H, N, d) using the 4D accessors

        //loop over Batch Size
         for (int b = 0; b < B; b++) {

             //loop over Heads
             for (int h = 0; h < H; h++) {

                 //loop over Sequence Length
                 for (int i = 0; i < N; i++) {

                     //loop over Embedding Dimensionality
                     for (int j = 0; j < d; j++) {
                        float val = fourDimRead(Q, b, h, i, j, H, N, d);
                        val = 0.0;
                        fourDimWrite(Q, b, h, i, j, H, N, d, val);
                     }
                 }
             }
         }
    */

    /* Here is an example of how to read/write 0's to  QK_t (N, N) using the 2D accessors

           for (int i = 0; i < N; i++) {
	       for (int j = 0; j < N; j++) {
	           float val = twoDimRead(QK_t, i, j, N);
               val = 0.0;
	           twoDimWrite(QK_t, i, j, N, val);
             }
         }
    */
    
    // -------- YOUR CODE HERE  -------- //
    for (int b = 0; b < B; b++) {
       //loop over Heads
       for (int h = 0; h < H; h++) {
           //loop over Sequence Length
           for (int i = 0; i < N; i++) {
            for(int seq_i=0; seq_i < N; seq_i++) {
             //loop over Embedding Dimensionality
              float val = 0.0;
               for (int j = 0; j < d; j++) {
                  int q_row  = i; 
                  int q_col = j;
                  int k_row = j;
                  int k_col = seq_i;
                  // float val = fourDimRead(Q, b, h, i, j, H, N, d);
          float q_val = fourDimRead(Q, b, h, q_row, q_col, H, N, d);
          float k_val = fourDimRead(K, b, h, k_row, k_col, H, N, d);
          val += q_val * k_val;


                  // val = 0.0;
                  // fourDimWrite(Q, b, h, i, j, H, N, d, val);
               }
          fourDimWrite(QK_t, b, h, i, seq_i, H, N, d, val );

            }

           }
          std::vector<float> tmp_row_res(N);
          for(int row_idx=0; row_idx < N; row_idx++) {

            float row_sum = 0.0;
            for(int cold_idx=0; cold_idx < N ;cold_idx++) {
               float val = twoDimRead(QK_t, row_idx, cold_idx, N);
              float exp_val = std::exp(val);
              row_sum += exp_val;
              tmp_row_res[cold_idx] = exp_val;

            }

            for(int cold_idx=0; cold_idx < N ; cold_idx++) {
              float prob = tmp_row_res[cold_idx] / row_sum;
              twoDimWrite(QK_t, row_idx, cold_idx, N, prob);
            }
          }


        for(int qkt_row_idx=0; qkt_row_idx < N; qkt_row_idx++) {
        for(int output_d_idx=0; output_d_idx < d; output_d_idx++) {
          float val =0.0;
          for(int m_idx=0; m_idx < N ; m_idx++) {
            float qkt_val =  twoDimRead(QK_t, qkt_row_idx, m_idx, N);
            int v_row = m_idx;
            int v_col = output_d_idx;
            float v_val = fourDimRead(V, b, h, v_row, v_col, H, N, d);
            val += qkt_val * v_val;
          }
          fourDimWrite(O, b, h, qkt_row_idx, output_d_idx, H, N, d ,val);
        }
        }
       }
   }




    
    // DO NOT EDIT THIS RETURN STATEMENT //
    // It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //
    return torch::from_blob(O.data(), {B, H, N, d}, torch::TensorOptions().dtype(torch::kFloat32)).clone();
}
```
```
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-17 21:15:18 207308:207308 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0768, 0.0776, 0.0784, 0.0792, 0.0800, 0.0808, 0.0816, 0.0824, 0.0832,
        0.0840, 0.0848, 0.0856, 0.0864, 0.0872, 0.0880, 0.0888, 0.0896, 0.0904,
        0.0912, 0.0920, 0.0928, 0.0936, 0.0944, 0.0952, 0.0960, 0.0968, 0.0976,
        0.0984, 0.0992, 0.1000, 0.1008, 0.1016])
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in <module>
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 311, in main
    part1Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 221, in part1Test
    testTemplate(attentionModuleReference.myUnfusedAttention, params, "STUDENT - NAIVE ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
```


Try to use double to store itermediate result to fix this problem.


I have two issues in previous version of code
```cpp
    for (int b = 0; b < B; b++) {
       //loop over Heads
       for (int h = 0; h < H; h++) {
          //loop over Sequence Length
          for (int i = 0; i < N; i++) {
            for(int seq_i=0; seq_i < N; seq_i++) {
             //loop over Embedding Dimensionality
              float val = 0.0;
               for (int j = 0; j < d; j++) {
                  int q_row  = i; 
                  int q_col = j;

                    // this is the correct indexing for the second matrix.
                    // Since K is not transposed.
                     // K should be indexed with (seq_i, j) instead of (j, seq_i) like normal matrix multiplciation
                  int k_row = seq_i;
                  int k_col = j;
                  float q_val = fourDimRead(Q, b, h, q_row, q_col, H, N, d);
                  float k_val = fourDimRead(K, b, h, k_row, k_col, H, N, d);
                  val += q_val * k_val;
               }
                // This is the second place that is fixed. 
                 // QK_t is two dimenional. 
                 // Should use twoDimWrite
              twoDimWrite(QK_t, i, seq_i, N, val );
            }

           }
 
```
