---
layout: post
title: Stf CS149 flash attention
date: 2024-11-12 07:59:00-0400
description:  
tags:  ai ml transformer  
categories: ml
featured: false
---


[Stf CS149 flash attention lab assignment repo](https://github.com/stanford-cs149/cs149gpt)


## Install library to compile code 

It shows error about error loading shared object.

```
(cs149gpt) ➜  cs149gpt git:(main) ✗ python3 gpt149.py 4Daccess
/home/zt/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)                                                                                        return torch._C._cuda_getDeviceCount() > 0
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 14, in <module>
    import module_ref as ms
ImportError: /home/zt/stf-cs149-pp/cs149gpt/module_ref.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE
```


I tried to use conda to create new env and install low version of pytorch but conda 
always installs 2.3.x version of torch for me.
```
 conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 cpuonly python=3.10 numpy=1.26 ninja tiktoken -c pytorch -c conda-forge
```
What it is required is 2.1.x version of torch.


Then I manually use pip to uninstall torch and reinstall 2.1.x version of torch
```
 pip3 uninstall torch
pip3 install torch==2.1.2

```


Got another issue saying that I should use numpy version < 2.0.

Then I uninstall numpy and reinstall it with version 1.2.x
```
pip3 uninstall numpy
 pip3 install numpy==1.26
```


So now I can run the code successfully.


## Part 1: naive attention 


My code produce the value that is 0.0003 less than what solution produces for each element.

I don't know why.

Should I use double ? 

First implementation:
```cpp
torch::Tensor myNaiveAttention(torch::Tensor QTensor, torch::Tensor KTensor, torch::Tensor VTensor, torch::Tensor QK_tTensor,
                int B, int H, int N, int d){

    // Q, K, V are passed in with Shape: (B, H, N, d)
    //QK^t Intermediate Tensor has Shape (N, N)
    
    //Make O Tensor with Shape (B, H, N, d) 
    at::Tensor OTensor = at::zeros({B, H, N, d}, at::kFloat);

    //Format O, Q, K, and V tensors into 4D vectors
    std::vector<float> O = formatTensor(OTensor);
    std::vector<float> Q = formatTensor(QTensor);
    std::vector<float> K = formatTensor(KTensor);
    std::vector<float> V = formatTensor(VTensor);

    //Format QK_t Tensor into a 2D vector.
    std::vector<float> QK_t = formatTensor(QK_tTensor);
    
    /* Here is an example of how to read/write 0's to  Q (B, H, N, d) using the 4D accessors

        //loop over Batch Size
         for (int b = 0; b < B; b++) {

             //loop over Heads
             for (int h = 0; h < H; h++) {

                 //loop over Sequence Length
                 for (int i = 0; i < N; i++) {

                     //loop over Embedding Dimensionality
                     for (int j = 0; j < d; j++) {
                        float val = fourDimRead(Q, b, h, i, j, H, N, d);
                        val = 0.0;
                        fourDimWrite(Q, b, h, i, j, H, N, d, val);
                     }
                 }
             }
         }
    */

    /* Here is an example of how to read/write 0's to  QK_t (N, N) using the 2D accessors

           for (int i = 0; i < N; i++) {
	       for (int j = 0; j < N; j++) {
	           float val = twoDimRead(QK_t, i, j, N);
               val = 0.0;
	           twoDimWrite(QK_t, i, j, N, val);
             }
         }
    */
    
    // -------- YOUR CODE HERE  -------- //
    for (int b = 0; b < B; b++) {
       //loop over Heads
       for (int h = 0; h < H; h++) {
           //loop over Sequence Length
           for (int i = 0; i < N; i++) {
            for(int seq_i=0; seq_i < N; seq_i++) {
             //loop over Embedding Dimensionality
              float val = 0.0;
               for (int j = 0; j < d; j++) {
                  int q_row  = i; 
                  int q_col = j;
                  int k_row = j;
                  int k_col = seq_i;
                  // float val = fourDimRead(Q, b, h, i, j, H, N, d);
          float q_val = fourDimRead(Q, b, h, q_row, q_col, H, N, d);
          float k_val = fourDimRead(K, b, h, k_row, k_col, H, N, d);
          val += q_val * k_val;


                  // val = 0.0;
                  // fourDimWrite(Q, b, h, i, j, H, N, d, val);
               }
          fourDimWrite(QK_t, b, h, i, seq_i, H, N, d, val );

            }

           }
          std::vector<float> tmp_row_res(N);
          for(int row_idx=0; row_idx < N; row_idx++) {

            float row_sum = 0.0;
            for(int cold_idx=0; cold_idx < N ;cold_idx++) {
               float val = twoDimRead(QK_t, row_idx, cold_idx, N);
              float exp_val = std::exp(val);
              row_sum += exp_val;
              tmp_row_res[cold_idx] = exp_val;

            }

            for(int cold_idx=0; cold_idx < N ; cold_idx++) {
              float prob = tmp_row_res[cold_idx] / row_sum;
              twoDimWrite(QK_t, row_idx, cold_idx, N, prob);
            }
          }


        for(int qkt_row_idx=0; qkt_row_idx < N; qkt_row_idx++) {
        for(int output_d_idx=0; output_d_idx < d; output_d_idx++) {
          float val =0.0;
          for(int m_idx=0; m_idx < N ; m_idx++) {
            float qkt_val =  twoDimRead(QK_t, qkt_row_idx, m_idx, N);
            int v_row = m_idx;
            int v_col = output_d_idx;
            float v_val = fourDimRead(V, b, h, v_row, v_col, H, N, d);
            val += qkt_val * v_val;
          }
          fourDimWrite(O, b, h, qkt_row_idx, output_d_idx, H, N, d ,val);
        }
        }
       }
   }




    
    // DO NOT EDIT THIS RETURN STATEMENT //
    // It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //
    return torch::from_blob(O.data(), {B, H, N, d}, torch::TensorOptions().dtype(torch::kFloat32)).clone();
}
```
```
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-17 21:15:18 207308:207308 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0768, 0.0776, 0.0784, 0.0792, 0.0800, 0.0808, 0.0816, 0.0824, 0.0832,
        0.0840, 0.0848, 0.0856, 0.0864, 0.0872, 0.0880, 0.0888, 0.0896, 0.0904,
        0.0912, 0.0920, 0.0928, 0.0936, 0.0944, 0.0952, 0.0960, 0.0968, 0.0976,
        0.0984, 0.0992, 0.1000, 0.1008, 0.1016])
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in <module>
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 311, in main
    part1Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 221, in part1Test
    testTemplate(attentionModuleReference.myUnfusedAttention, params, "STUDENT - NAIVE ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
```


Try to use double to store itermediate result to fix this problem.


I have two issues in previous version of code
```cpp
    for (int b = 0; b < B; b++) {
       //loop over Heads
       for (int h = 0; h < H; h++) {
          //loop over Sequence Length
          for (int i = 0; i < N; i++) {
            for(int seq_i=0; seq_i < N; seq_i++) {
             //loop over Embedding Dimensionality
              float val = 0.0;
               for (int j = 0; j < d; j++) {
                  int q_row  = i; 
                  int q_col = j;

                    // this is the correct indexing for the second matrix.
                    // Since K is not transposed.
                     // K should be indexed with (seq_i, j) instead of (j, seq_i) like normal matrix multiplciation
                  int k_row = seq_i;
                  int k_col = j;
                  float q_val = fourDimRead(Q, b, h, q_row, q_col, H, N, d);
                  float k_val = fourDimRead(K, b, h, k_row, k_col, H, N, d);
                  val += q_val * k_val;
               }
                // This is the second place that is fixed. 
                 // QK_t is two dimenional. 
                 // Should use twoDimWrite
              twoDimWrite(QK_t, i, seq_i, N, val );
            }

           }
 
```

Output:
```

REFERENCE - NAIVE ATTENTION statistics
cpu time:  293.585ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:10:17 1148267:1148267 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:10:17 1148267:1148267 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 13:10:17 1148267:1148267 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.2747969627380371

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::empty         0.01%      16.000us         0.01%      16.000us       2.000us       5.00 Mb       5.00 Mb             8
    STUDENT - NAIVE ATTENTION        99.49%     273.438ms        99.68%     273.946ms     273.946ms       4.50 Mb      -1.00 Mb             1
                  aten::zeros         0.01%      14.000us         0.05%     133.000us      66.500us       4.50 Mb           0 b             2
                  aten::clone         0.01%      20.000us         0.13%     346.000us     173.000us       1.00 Mb           0 b             2
          aten::empty_strided         0.00%      12.000us         0.00%      12.000us       2.400us     512.51 Kb     512.51 Kb             5
              model_inference         0.20%     549.000us       100.00%     274.838ms     274.838ms     512.00 Kb      -4.00 Mb             1
                aten::flatten         0.01%      19.000us         0.08%     209.000us      41.800us     512.00 Kb           0 b             5
             aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                     aten::to         0.00%       6.000us         0.01%      31.000us       5.167us         520 b           0 b             6
               aten::_to_copy         0.01%      14.000us         0.01%      25.000us       6.250us         520 b           0 b             4
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 274.838ms

STUDENT - NAIVE ATTENTION statistics
cpu time:  273.946ms
mem usage:  4718592 bytes
```

## Part2: blocked matrix multiplication
Initial version 


Unlike that of code written in cuda, I have to manually iterate all rows and cols 
of input matrix.


Don't know if this is correct way to do that.

```cpp
#define TILE_SIZE 16
torch::Tensor myUnfusedAttentionBlocked(torch::Tensor QTensor, torch::Tensor KTensor, torch::Tensor VTensor, torch::Tensor QK_tTensor,
                int B, int H, int N, int d){
  
  // Q, K, V are passed in with Shape: (B, H, N, d)
  //QK^t Intermediate Tensor has Shape (N, N)

  //Make O Tensor with Shape (B, H, N, d) 
  at::Tensor OTensor = at::zeros({B, H, N, d}, at::kFloat);

  //Format O, Q, K, and V tensors into 4D vectors
  std::vector<float> O = formatTensor(OTensor);
  std::vector<float> Q = formatTensor(QTensor);
  std::vector<float> K = formatTensor(KTensor);
  std::vector<float> V = formatTensor(VTensor);

  //Format QK_t Tensor into a 2D vector.
  std::vector<float> QK_t = formatTensor(QK_tTensor);

  // -------- YOUR CODE HERE  -------- //
  for(int b=0; b < B; b++) {
    for(int h=0; h < H; h++) {
      for(int q_row_tile_idx=0; q_row_tile_idx < (N+TILE_SIZE-1)/TILE_SIZE; q_row_tile_idx++) {
        // K is not transposed so we traverse k by row.
        for(int k_row_tile_idx=0; k_row_tile_idx < (N+TILE_SIZE-1)/TILE_SIZE; k_row_tile_idx++ ) {
          for(int d_col_tile_idx=0; d_col_tile_idx < (d+TILE_SIZE-1)/TILE_SIZE; d_col_tile_idx++ ) {
            for(int tile_row_idx=0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
              // int out_row_idx = q_row_tile_idx * TILE_SIZE + tile_row_idx;
              for(int tile_col_idx=0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
                // int out_col_idx = k_row_tile_idx * TILE_SIZE + tile_col_idx;
                int q_col_idx = d_col_tile_idx * TILE_SIZE + tile_col_idx;
                int q_row_idx =q_row_tile_idx * TILE_SIZE + tile_row_idx; 
                int k_row_idx = k_row_tile_idx * TILE_SIZE + tile_row_idx;
                int k_col_idx = d_col_tile_idx * TILE_SIZE + tile_col_idx;
                if(q_row_idx < N && q_col_idx < d && k_row_idx < N && k_col_idx < d) {
                  float q_tile_val = fourDimRead(Q, b, h, q_row_idx, q_col_idx, H, N, d);
                  float k_tile_val = fourDimRead(K, b, h, k_row_idx, k_col_idx, H, N, d);
                  float orig_val = twoDimRead(QK_t, q_row_tile_idx, k_row_idx, N);
                  float val = q_tile_val * k_tile_val + orig_val;
                  twoDimWrite(QK_t, q_row_tile_idx, k_row_tile_idx, N, val );
                }
              }
            }
          }

        }
      }

      for(int row_idx=0; row_idx < N; row_idx++) {
        std::vector<double> tmp_row_res(N, 0.0);
        double row_sum = 0.0;
        for(int cold_idx=0; cold_idx < N ;cold_idx++) {
           float val = twoDimRead(QK_t, row_idx, cold_idx, N);
          double exp_val = std::exp(val);
          row_sum += exp_val;
          tmp_row_res[cold_idx] = exp_val;
        }
        for(int cold_idx=0; cold_idx < N ; cold_idx++) {
          float prob = tmp_row_res[cold_idx] / row_sum;
          twoDimWrite(QK_t, row_idx, cold_idx, N, prob);
        }
      }

      for(int qkt_row_idx=0; qkt_row_idx < N; qkt_row_idx++) {
        for(int output_d_idx=0; output_d_idx < d; output_d_idx++) {
          float val =0.0;
          for(int m_idx=0; m_idx < N ; m_idx++) {
            float qkt_val =  twoDimRead(QK_t, qkt_row_idx, m_idx, N);
            int v_row = m_idx;
            int v_col = output_d_idx;
            float v_val = fourDimRead(V, b, h, v_row, v_col, H, N, d);
            val += qkt_val * v_val;
          }
          fourDimWrite(O, b, h, qkt_row_idx, output_d_idx, H, N, d ,val);
        }
      }

    }

  }


  // DO NOT EDIT THIS RETURN STATEMENT //
  // It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //
  return torch::from_blob(O.data(), {B, H, N, d}, torch::TensorOptions().dtype(torch::kFloat32)).clone();
}
```


It's not correct
```cpp
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 11:29:24 607233:607233 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0767, 0.0775, 0.0783, 0.0791, 0.0799, 0.0807, 0.0815, 0.0823, 0.0831,
        0.0839, 0.0847, 0.0855, 0.0863, 0.0871, 0.0879, 0.0887, 0.0895, 0.0903,
        0.0911, 0.0919, 0.0927, 0.0935, 0.0943, 0.0951, 0.0959, 0.0967, 0.0975,
        0.0983, 0.0991, 0.0999, 0.1007, 0.1015])
STAGE:2024-11-18 11:29:24 607233:607233 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 11:29:24 607233:607233 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in <module>
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 313, in main
    part2Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 233, in part2Test
    testTemplate(attentionModuleReference.myUnfusedAttentionBlocked, params, "STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
```


Fixed the two indexing issue in original issue.


This is the correct implementation given by gpt. However, it's too slow
```
      for (int q_row_tile_idx = 0; q_row_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; q_row_tile_idx++) {
    for (int k_row_tile_idx = 0; k_row_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; k_row_tile_idx++) {
        for (int d_col_tile_idx = 0; d_col_tile_idx < (d + TILE_SIZE - 1) / TILE_SIZE; d_col_tile_idx++) {
            for (int tile_row_idx = 0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
                for (int tile_col_idx = 0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
                    int q_row_idx = q_row_tile_idx * TILE_SIZE + tile_row_idx;
                    int k_row_idx = k_row_tile_idx * TILE_SIZE + tile_col_idx; // Fix indexing
                    for (int d_idx = d_col_tile_idx * TILE_SIZE; d_idx < (d_col_tile_idx + 1) * TILE_SIZE; d_idx++) {
                        if (q_row_idx < N && k_row_idx < N && d_idx < d) {
                            float q_tile_val = fourDimRead(Q, b, h, q_row_idx, d_idx, H, N, d);
                            float k_tile_val = fourDimRead(K, b, h, k_row_idx, d_idx, H, N, d);
                            float orig_val = twoDimRead(QK_t, q_row_idx, k_row_idx, N);
                            float val = q_tile_val * k_tile_val + orig_val;
                            twoDimWrite(QK_t, q_row_idx, k_row_idx, N, val);
                        }
                    }
                }
            }
        }
    }
}
```

Explanation from gpt

You're right to question the necessity of traversing the dimension \( d \) fully. Let's clarify why it's important in some cases and whether it applies here.

### Understanding the Role of Dimension \( d \)

In block matrix multiplication:
1. **Matrix Multiplication Basics**: 
   - To compute an element \((i, j)\) of the resulting matrix \( QK_t \), you need to calculate the dot product of the \(i\)-th row of \(Q\) and the \(j\)-th row of \(K\).
   - This involves summing the product of corresponding elements from both rows over the shared dimension \( d \):  
     \[
     QK_t[i, j] = \sum_{k=0}^{d-1} Q[i, k] \times K[j, k]
     \]

2. **Tiling**: 
   - In tiled matrix multiplication, the computation of a block of \( QK_t \) (e.g., a \( \text{TILE_SIZE} \times \text{TILE_SIZE} \) block) is split into smaller sub-blocks to improve memory locality. 
   - Each sub-block computation iterates over the shared dimension \( d \) in chunks.

### Why Traverse \( d \)?

If you don’t traverse \( d \), the computation will only consider partial contributions to the dot product. This would leave the values in \( QK_t \) incomplete.

For example, if \( d = 6 \), and your block size is \( \text{TILE_SIZE} = 3 \):
- You compute the dot product for \( d \in [0, 2] \), but ignore \( d \in [3, 5] \), leading to incorrect results.

### Why the Code Needs It
Your original code suggests you're updating \( QK_t \) incrementally:
```cpp
float val = q_tile_val * k_tile_val + orig_val;
```
Here, `orig_val` accumulates partial results. Without iterating fully over \( d \), the result will be incomplete because only a subset of the dot product contributions is added.

### When \( d \) Traversal Isn't Necessary
In rare cases, you might not need to traverse \( d \) if:
1. **Each block fully covers \( d \)**:
   - This is possible only when \( \text{TILE_SIZE} \) equals \( d \).
   - In this case, the tiling logic aligns perfectly with the shared dimension, and a single traversal suffices.

2. **Precomputed Values**:
   - If \( Q \) or \( K \) has been preprocessed (e.g., multiplied previously), the shared dimension \( d \) might not need iteration.

### Conclusion
For the general case (and your code):
- Traversing \( d \) fully ensures the correctness of the dot product computation across the shared dimension.
- If you're sure \( d \) traversal isn't necessary, it usually means \( Q \) and \( K \) are already structured to encapsulate the full dot product for each tile.

If you're still unsure about why it’s needed in your case, we can examine your problem more closely to confirm!


I can view the outside for loop from two kind of perspective. 

One is from input matrix.


One is from output matrix.
This one makes more sense after reading code generated by gpt

Two outer for loop traverse row and column tiles of output matrix.
The third for loop traverse the row in input matrix and the column in second input matrix
The fourth and fifth for loop  traverse the row and column elements in current tile.
The sixth for loop does dot product for a single row in input tile and single column in output tile
and accumulate the sum. And then write back the sum to output matrix.
```
for (int row_tile_idx = 0; row_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; row_tile_idx++) {
    for (int col_tile_idx = 0; col_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; col_tile_idx++) {
        // Initialize the current block of the output matrix
        for (int i = 0; i < TILE_SIZE; i++) {
            for (int j = 0; j < TILE_SIZE; j++) {
                int row_idx = row_tile_idx * TILE_SIZE + i;
                int col_idx = col_tile_idx * TILE_SIZE + j;
                if (row_idx < N && col_idx < N) {
                    twoDimWrite(C, row_idx, col_idx, N, 0.0); // Initialize to zero
                }
            }
        }

        for (int k_tile_idx = 0; k_tile_idx < (d + TILE_SIZE - 1) / TILE_SIZE; k_tile_idx++) {
            for (int i = 0; i < TILE_SIZE; i++) {
                for (int j = 0; j < TILE_SIZE; j++) {
                    int row_idx = row_tile_idx * TILE_SIZE + i;
                    int col_idx = col_tile_idx * TILE_SIZE + j;
                    if (row_idx >= N || col_idx >= N) continue;

                    float sum = twoDimRead(C, row_idx, col_idx, N);

                    for (int k = 0; k < TILE_SIZE; k++) {
                        int k_idx = k_tile_idx * TILE_SIZE + k;
                        if (k_idx >= d) break;

                        float a_val = twoDimRead(A, row_idx, k_idx, d);
                        float b_val = twoDimRead(B, col_idx, k_idx, d); // Column index now indexes rows in B
                        sum += a_val * b_val;
                    }

                    twoDimWrite(C, row_idx, col_idx, N, sum);
                }
            }
        }
    }
}

```




The code above use global buffer.
```
      for(int row_tile_idx=0; row_tile_idx < (N+TILE_SIZE-1)/TILE_SIZE; row_tile_idx++) {
        for(int col_tile_idx=0; col_tile_idx < (N+TILE_SIZE-1)/TILE_SIZE; col_tile_idx++) {
          for(int k_tile_idx=0; k_tile_idx < (d+TILE_SIZE-1)/TILE_SIZE; k_tile_idx++  ) {
            for(int tile_row_idx=0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
              for(int tile_col_idx=0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                if(row_idx >= N || col_idx >= N) {
                  continue;
                }
                float sum = twoDimRead(QK_t, row_idx, col_idx, N);

                for(int k=0; k < TILE_SIZE; k++) {
                  int k_idx = k_tile_idx * TILE_SIZE + k;
                  if(k_idx >= d) break;
                  float q_val =  fourDimRead(Q,b, h, row_idx, k_idx, H, N, d);
                  float k_val = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);
                  sum += q_val * k_val;
                }
                twoDimWrite(QK_t, row_idx, col_idx, N, sum);
              }

            }
          }
        }
      }
```

Output:
```

REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  238.938ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:39:56 1318038:1318038 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:39:56 1318038:1318038 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 13:39:56 1318038:1318038 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.29826903343200684

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.01%      21.000us         0.01%      21.000us       2.625us       5.00 Mb       5.00 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.35%     296.386ms        99.70%     297.443ms     297.443ms       4.50 Mb      -1.00 Mb             1
                                   aten::zeros         0.01%      15.000us         0.24%     706.000us     353.000us       4.50 Mb           0 b             2
                                   aten::clone         0.01%      19.000us         0.11%     328.000us     164.000us       1.00 Mb           0 b             2
                           aten::empty_strided         0.00%      11.000us         0.00%      11.000us       2.200us     512.51 Kb     512.51 Kb             5
                               model_inference         0.18%     550.000us       100.00%     298.329ms     298.329ms     512.00 Kb      -4.00 Mb             1
                                 aten::flatten         0.01%      15.000us         0.07%     200.000us      40.000us     512.00 Kb           0 b             5
                              aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                                      aten::to         0.00%       6.000us         0.01%      31.000us       5.167us         520 b           0 b             6
                                aten::_to_copy         0.00%      14.000us         0.01%      25.000us       6.250us         520 b           0 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 298.329ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  297.443ms
mem usage:  4718592 bytes
```

This code use local buffer that is allocated.
A little bit faster but not a lot.
```
#pragma omp parallel for collapse(2) // Parallelize the two outermost loops
for (int row_tile_idx = 0; row_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; row_tile_idx++) {
    for (int col_tile_idx = 0; col_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; col_tile_idx++) {
        for (int k_tile_idx = 0; k_tile_idx < (d + TILE_SIZE - 1) / TILE_SIZE; k_tile_idx++) {

            // Buffers for tile data
            float Q_tile[TILE_SIZE][TILE_SIZE];
            float K_tile[TILE_SIZE][TILE_SIZE];

            // Preload Q and K tiles into local buffers
            for (int tile_row_idx = 0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                if (row_idx >= N) continue; // Skip out-of-bound rows

                for (int k = 0; k < TILE_SIZE; k++) {
                    int k_idx = k_tile_idx * TILE_SIZE + k;
                    if (k_idx < d) {
                        Q_tile[tile_row_idx][k] = fourDimRead(Q, b, h, row_idx, k_idx, H, N, d);
                    } else {
                        Q_tile[tile_row_idx][k] = 0.0f; // Fill with zero if out-of-bounds
                    }
                }
            }

            for (int tile_col_idx = 0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
                int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                if (col_idx >= N) continue; // Skip out-of-bound columns

                for (int k = 0; k < TILE_SIZE; k++) {
                    int k_idx = k_tile_idx * TILE_SIZE + k;
                    if (k_idx < d) {
                        K_tile[tile_col_idx][k] = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);
                    } else {
                        K_tile[tile_col_idx][k] = 0.0f; // Fill with zero if out-of-bounds
                    }
                }
            }

            // Compute the dot product for the current tile
            for (int tile_row_idx = 0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                if (row_idx >= N) continue; // Skip out-of-bound rows

                for (int tile_col_idx = 0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
                    int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                    if (col_idx >= N) continue; // Skip out-of-bound columns

                    float sum = twoDimRead(QK_t, row_idx, col_idx, N);

                    // Unrolled loop for vectorized dot product
                    for (int k = 0; k < TILE_SIZE; k++) {
                        sum += Q_tile[tile_row_idx][k] * K_tile[tile_col_idx][k];
                    }

                    twoDimWrite(QK_t, row_idx, col_idx, N, sum);
                }
            }
        }
    }
}

```

OUtput:
```
REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  226.667ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:44:02 1342423:1342423 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:44:02 1342423:1342423 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 13:44:02 1342423:1342423 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.2852001190185547

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.00%      13.000us         0.00%      13.000us       1.625us       5.00 Mb       5.00 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.52%     283.895ms        99.68%     284.350ms     284.350ms       4.50 Mb      -1.00 Mb             1
                                   aten::zeros         0.01%      17.000us         0.05%     131.000us      65.500us       4.50 Mb           0 b             2
                                   aten::clone         0.01%      23.000us         0.11%     300.000us     150.000us       1.00 Mb           0 b             2
                           aten::empty_strided         0.00%      12.000us         0.00%      12.000us       2.400us     512.26 Kb     512.26 Kb             5
                               model_inference         0.20%     565.000us       100.00%     285.254ms     285.254ms     512.00 Kb      -4.00 Mb             1
                                 aten::flatten         0.01%      16.000us         0.06%     159.000us      31.800us     512.00 Kb           0 b             5
                              aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                                      aten::to         0.00%      10.000us         0.01%      31.000us       5.167us         520 b         256 b             6
                                aten::_to_copy         0.01%      16.000us         0.01%      26.000us       6.500us         520 b         256 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 285.254ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  284.35ms
mem usage:  4718592 bytes
```


Code that do block matrix multiplication for both (Q * K_t) and (softmax(QK_t ) * V):
```cpp
#define TILE_SIZE 16
torch::Tensor myUnfusedAttentionBlocked(torch::Tensor QTensor, torch::Tensor KTensor, torch::Tensor VTensor, torch::Tensor QK_tTensor,
                int B, int H, int N, int d){
  
  // Q, K, V are passed in with Shape: (B, H, N, d)
  //QK^t Intermediate Tensor has Shape (N, N)

  //Make O Tensor with Shape (B, H, N, d) 
  at::Tensor OTensor = at::zeros({B, H, N, d}, at::kFloat);

  //Format O, Q, K, and V tensors into 4D vectors
  std::vector<float> O = formatTensor(OTensor);
  std::vector<float> Q = formatTensor(QTensor);
  std::vector<float> K = formatTensor(KTensor);
  std::vector<float> V = formatTensor(VTensor);

  //Format QK_t Tensor into a 2D vector.
  std::vector<float> QK_t = formatTensor(QK_tTensor);

  // -------- YOUR CODE HERE  -------- //
  for(int b=0; b < B; b++) {
    for(int h=0; h < H; h++) {

      //
      // correct
      // for(int row_tile_idx=0; row_tile_idx < (N+TILE_SIZE-1)/TILE_SIZE; row_tile_idx++) {
      //   for(int col_tile_idx=0; col_tile_idx < (N+TILE_SIZE-1)/TILE_SIZE; col_tile_idx++) {
      //     for(int k_tile_idx=0; k_tile_idx < (d+TILE_SIZE-1)/TILE_SIZE; k_tile_idx++  ) {
      //       for(int tile_row_idx=0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
      //         for(int tile_col_idx=0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
      //           int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
      //           int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
      //           if(row_idx >= N || col_idx >= N) {
      //             continue;
      //           }
      //           float sum = twoDimRead(QK_t, row_idx, col_idx, N);

      //           for(int k=0; k < TILE_SIZE; k++) {
      //             int k_idx = k_tile_idx * TILE_SIZE + k;
      //             if(k_idx >= d) break;
      //             float q_val =  fourDimRead(Q,b, h, row_idx, k_idx, H, N, d);
      //             float k_val = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);
      //             sum += q_val * k_val;
      //           }
      //           twoDimWrite(QK_t, row_idx, col_idx, N, sum);
      //         }

      //       }
      //     }
      //   }
      // }

      // correct with local buffer
for (int row_tile_idx = 0; row_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; row_tile_idx++) {
    for (int col_tile_idx = 0; col_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; col_tile_idx++) {
        for (int k_tile_idx = 0; k_tile_idx < (d + TILE_SIZE - 1) / TILE_SIZE; k_tile_idx++) {

            // Buffers for tile data
            float Q_tile[TILE_SIZE][TILE_SIZE];
            float K_tile[TILE_SIZE][TILE_SIZE];

            // Preload Q and K tiles into local buffers
            for (int tile_row_idx = 0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                if (row_idx >= N) continue; // Skip out-of-bound rows

                for (int k = 0; k < TILE_SIZE; k++) {
                    int k_idx = k_tile_idx * TILE_SIZE + k;
                    if (k_idx < d) {
                        Q_tile[tile_row_idx][k] = fourDimRead(Q, b, h, row_idx, k_idx, H, N, d);
                    } else {
                        Q_tile[tile_row_idx][k] = 0.0f; // Fill with zero if out-of-bounds
                    }
                }
            }

            for (int tile_col_idx = 0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
                int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                if (col_idx >= N) continue; // Skip out-of-bound columns

                for (int k = 0; k < TILE_SIZE; k++) {
                    int k_idx = k_tile_idx * TILE_SIZE + k;
                    if (k_idx < d) {
                        K_tile[tile_col_idx][k] = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);
                    } else {
                        K_tile[tile_col_idx][k] = 0.0f; // Fill with zero if out-of-bounds
                    }
                }
            }

            // Compute the dot product for the current tile
            for (int tile_row_idx = 0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                if (row_idx >= N) continue; // Skip out-of-bound rows

                for (int tile_col_idx = 0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
                    int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                    if (col_idx >= N) continue; // Skip out-of-bound columns

                    float sum = twoDimRead(QK_t, row_idx, col_idx, N);

                    // Unrolled loop for vectorized dot product
                    for (int k = 0; k < TILE_SIZE; k++) {
                        sum += Q_tile[tile_row_idx][k] * K_tile[tile_col_idx][k];
                    }

                    twoDimWrite(QK_t, row_idx, col_idx, N, sum);
                }
            }
        }
    }
}



      // also correct
//       for (int q_row_tile_idx = 0; q_row_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; q_row_tile_idx++) {
//     for (int k_row_tile_idx = 0; k_row_tile_idx < (N + TILE_SIZE - 1) / TILE_SIZE; k_row_tile_idx++) {
//         for (int d_col_tile_idx = 0; d_col_tile_idx < (d + TILE_SIZE - 1) / TILE_SIZE; d_col_tile_idx++) {
//             for (int tile_row_idx = 0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
//                 for (int tile_col_idx = 0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
//                     int q_row_idx = q_row_tile_idx * TILE_SIZE + tile_row_idx;
//                     int k_row_idx = k_row_tile_idx * TILE_SIZE + tile_col_idx; // Fix indexing
//                     for (int d_idx = d_col_tile_idx * TILE_SIZE; d_idx < (d_col_tile_idx + 1) * TILE_SIZE; d_idx++) {
//                         if (q_row_idx < N && k_row_idx < N && d_idx < d) {
//                             float q_tile_val = fourDimRead(Q, b, h, q_row_idx, d_idx, H, N, d);
//                             float k_tile_val = fourDimRead(K, b, h, k_row_idx, d_idx, H, N, d);
//                             float orig_val = twoDimRead(QK_t, q_row_idx, k_row_idx, N);
//                             float val = q_tile_val * k_tile_val + orig_val;
//                             twoDimWrite(QK_t, q_row_idx, k_row_idx, N, val);
//                         }
//                     }
//                 }
//             }
//         }
//     }
// }




      for(int row_idx=0; row_idx < N; row_idx++) {
        std::vector<double> tmp_row_res(N, 0.0);
        double row_sum = 0.0;
        for(int cold_idx=0; cold_idx < N ;cold_idx++) {
           float val = twoDimRead(QK_t, row_idx, cold_idx, N);
          double exp_val = std::exp(val);
          row_sum += exp_val;
          tmp_row_res[cold_idx] = exp_val;
        }
        for(int cold_idx=0; cold_idx < N ; cold_idx++) {
          float prob = tmp_row_res[cold_idx] / row_sum;
          twoDimWrite(QK_t, row_idx, cold_idx, N, prob);
        }
      }


      
      for(int qkt_row_tile_idx=0; qkt_row_tile_idx < (N+TILE_SIZE-1)/TILE_SIZE; qkt_row_tile_idx++) {
        for(int output_d_tile_idx=0; output_d_tile_idx < (d+TILE_SIZE-1)/TILE_SIZE; output_d_tile_idx++) {

          for(int k_tile_idx=0; k_tile_idx < (N+TILE_SIZE-1)/TILE_SIZE; k_tile_idx++) {
            for(int tile_row_idx=0; tile_row_idx < TILE_SIZE; tile_row_idx++) {
                int out_row_idx = qkt_row_tile_idx * TILE_SIZE + tile_row_idx;
              if(out_row_idx >= N) continue;
              for(int tile_col_idx=0; tile_col_idx < TILE_SIZE; tile_col_idx++) {
                int out_col_idx = output_d_tile_idx * TILE_SIZE + tile_col_idx;
                if( out_col_idx >= d) continue;

                float sum = fourDimRead(O, b, h, out_row_idx, out_col_idx, H, N, d );
                for(int k=0; k < TILE_SIZE; k++) {
                  int k_idx = k_tile_idx * TILE_SIZE + k;
                  float qkt_val = twoDimRead(QK_t, out_row_idx, k_idx, N);
                  float v_val = fourDimRead(V, b, h, k_idx, out_col_idx, H, N, d);
                  sum += qkt_val * v_val; 
                }
                fourDimWrite(O, b, h, out_row_idx, out_col_idx, H, N, d, sum);
              }
            }
          }
        }
      }


    }

  }


  // DO NOT EDIT THIS RETURN STATEMENT //
  // It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //
  return torch::from_blob(O.data(), {B, H, N, d}, torch::TensorOptions().dtype(torch::kFloat32)).clone();
}

```

Output:

Achieve the same cpu time as ref solution.
```
Self CPU time total: 215.420ms

REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  214.494ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 20:06:22 3461555:3461555 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 20:06:23 3461555:3461555 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 20:06:23 3461555:3461555 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.2183218002319336

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.01%      12.000us         0.01%      12.000us       1.500us       5.00 Mb       5.00 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.38%     217.015ms        99.61%     217.504ms     217.504ms       4.50 Mb      -1.00 Mb             1
                                   aten::zeros         0.01%      17.000us         0.07%     142.000us      71.000us       4.50 Mb           0 b             2
                                   aten::clone         0.01%      19.000us         0.14%     313.000us     156.500us       1.00 Mb           0 b             2
                           aten::empty_strided         0.00%       9.000us         0.00%       9.000us       1.800us     512.25 Kb     512.25 Kb             5
                               model_inference         0.24%     532.000us       100.00%     218.359ms     218.359ms     512.00 Kb      -4.00 Mb             1
                                 aten::flatten         0.01%      25.000us         0.09%     189.000us      37.800us     512.00 Kb           0 b             5
                              aten::empty_like         0.00%       4.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                                      aten::to         0.00%       9.000us         0.01%      30.000us       5.000us         520 b           4 b             6
                                aten::_to_copy         0.01%      14.000us         0.01%      25.000us       6.250us         520 b         260 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 218.359ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  217.504ms
mem usage:  4718592 bytes
```
