
## LLM inference optimization framework
- vLLM [Berkeley]

- TensorRT-LLM [Nvidia]

- Silicon flow [Startup in China, Not opensourced yet]

- DeepSpeed-MII [Microsoft]



## Performance comparison
Throughput


