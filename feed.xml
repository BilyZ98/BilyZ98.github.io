<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bilyz98.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bilyz98.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-14T13:35:22+00:00</updated><id>https://bilyz98.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Speed up matrix multiplication</title><link href="https://bilyz98.github.io/blog/2024/matrix-multiplication/" rel="alternate" type="text/html" title="Speed up matrix multiplication"/><published>2024-07-12T11:59:00+00:00</published><updated>2024-07-12T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/matrix-multiplication</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/matrix-multiplication/"><![CDATA[<p>https://gist.github.com/chris124567/c45d46fdf4d922389641cc9f591ae577</p> <h3 id="naive-matrix-multiplication">Naive matrix multiplication</h3> <p>Code</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">matmul_byhand</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">weight</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">input_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">input_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">input_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_col_idx</span> <span class="o">=</span><span class="mi">0</span> <span class="p">;</span> <span class="n">output_col_idx</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">output_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">m_idx</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">m_idx</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">m_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">input_idx</span> <span class="o">=</span> <span class="n">input_row_idx</span> <span class="o">*</span> <span class="n">M</span> <span class="o">+</span> <span class="n">m_idx</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">weight_idx</span> <span class="o">=</span> <span class="n">m_idx</span> <span class="o">*</span>  <span class="n">K</span>  <span class="o">+</span> <span class="n">output_col_idx</span><span class="p">;</span>
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">input</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">weight</span><span class="p">[</span><span class="n">weight_idx</span><span class="p">];</span>
      <span class="p">}</span>

      <span class="kt">int</span> <span class="n">out_idx</span> <span class="o">=</span> <span class="n">input_row_idx</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">output_col_idx</span><span class="p">;</span>
      <span class="n">out</span><span class="p">[</span><span class="n">out_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
	<span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">M</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">K</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span> <span class="o">*</span> <span class="n">M</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">M</span> <span class="o">*</span> <span class="n">K</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span> <span class="o">*</span> <span class="n">K</span><span class="p">];</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">*</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">"%f "</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">j</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="n">matmul_byhand</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">"%f "</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">j</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span>
</code></pre></div></div> <p>Explanation: The most outer loop iterates over the rows of the input matrix.</p> <p>The second outer loop iterates over the columns of the output matrix. The inner loop iterates over the columns of the input matrix and the rows of the weight matrix. The inner loop calculates the dot product of the input row and the weight column and stores the result in the output matrix.</p> <p>Please note that how index of input, weight and output matrix are calculated. <img src="https://github.com/user-attachments/assets/dc21b2cc-20f6-4720-8fbb-5adc7855d4f0" alt="matmul3 drawio"/></p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>20.000000 23.000000 26.000000 29.000000
56.000000 68.000000 80.000000 92.000000
92.000000 113.000000 134.000000 155.000000
</code></pre></div></div>]]></content><author><name></name></author><category term="ml"/><category term="al"/><category term="ml"/><summary type="html"><![CDATA[https://gist.github.com/chris124567/c45d46fdf4d922389641cc9f591ae577]]></summary></entry><entry><title type="html">How to write research paper</title><link href="https://bilyz98.github.io/blog/2024/how-to-write-research-paper/" rel="alternate" type="text/html" title="How to write research paper"/><published>2024-07-07T11:59:00+00:00</published><updated>2024-07-07T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/how-to-write-research-paper</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/how-to-write-research-paper/"><![CDATA[<p>I first found this slide from a wechat offical account.</p> <p>Then I found the original link through google. <a href="https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/">https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/</a></p> <p>What I learn that refreshes my mind is that this talk mentions that</p> <p>one should do this</p> <p>Coming up with research idea -&gt; Writing paper -&gt; Doing research work</p> <p>instead of this</p> <p>Coming up with research idea -&gt; Doing research work -&gt; Writing paper</p> <p>I felt mind blowing when I first read this.</p> <p>This make so much sense.</p> <p>The talk mentions that an idea is meaningless if you just keep it to yourself.</p> <p>So what you should do is to write them down and spread your idea to other people as early as possbile.</p>]]></content><author><name></name></author><category term="research"/><category term="research"/><summary type="html"><![CDATA[How to write research paper]]></summary></entry><entry><title type="html">Fast nano-gpt training</title><link href="https://bilyz98.github.io/blog/2024/gpt-fast/" rel="alternate" type="text/html" title="Fast nano-gpt training"/><published>2024-07-03T11:59:00+00:00</published><updated>2024-07-03T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/gpt-fast</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/gpt-fast/"><![CDATA[<p>In this blog I will document my code and experiment results following karparthy’s latest gpt-2 training tutorial video.</p> <p>The base code on which I build is from this post <a href="./2024-06-25-transformer.md">./2024-06-25-transformer.md</a></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/l8pRSuU81PU?si=2sEmtmn56XBMPTbU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>[https://youtu.be/l8pRSuU81PU?si=2sEmtmn56XBMPTbU][https://youtu.be/l8pRSuU81PU?si=2sEmtmn56XBMPTbU]</p> <p>Code repo url: <a href="https://github.com/BilyZ98/nano-gpt">https://github.com/BilyZ98/nano-gpt</a></p> <p>What is TFLOPs ? Tera floating point operations per second.</p> <h3 id="weight-sharing">Weight sharing</h3> <p>Share weight of <code class="language-plaintext highlighter-rouge">lm_head</code> and <code class="language-plaintext highlighter-rouge">weight_token_embedding</code> This weight share helps to save memory. This is huge amount of memory. It’s <code class="language-plaintext highlighter-rouge">n_embed * vocab_size</code></p> <h3 id="lower-precision">Lower precision</h3> <p>Default tensor precision:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model device cuda:0
m device cuda:0
torch.float32
</code></pre></div></div> <p>Without TF32: Memory usage:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 1.0s: nvidia-smi                                                                                                               Sat Jul  6 16:51:57 2024

Sat Jul  6 16:51:57 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   71C    P0   309W / 300W |   5359MiB / 81920MiB |    100%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   29C    P0    46W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1116, val loss 1.1166
step 4500, loss: 1.1580688953399658, dt: 113.35ms, tok/sec: 144546.60
Time taken: 739.9126691818237 seconds
Total parameters: 10921049
Trainable parameters: 10921049
        ex is consists of centuries and want-wetlife science method ankners, from the ways wheredco by Benrys, where you're rate ar harm browling preservation in musicians. In Athletes have continuies to munically, and effects create cro-intricate jaz towantical navigating vail respectives to diseariety.

**Befor Hera's Players**

Prannit**
Infaming the game's early which commercial quirtual health, and mobile exammatring sound in football in pain this visualization involvement, is home to shapel key th

</code></pre></div></div> <p>TF32: Memory usage:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 0.1s: nvidia-smi                                                                                                                          Sat Jul  6 17:11:38 2024

Sat Jul  6 17:11:38 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   64C    P0   309W / 300W |   5359MiB / 81920MiB |     99%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   31C    P0    47W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 4998, loss: 1.1807901859283447, dt: 30.66ms, tok/sec: 534324.52
step 4999, loss: 1.1486560106277466, dt: 30.19ms, tok/sec: 542632.14
Time taken: 315.7020351886749 seconds
Total parameters: 10921049
Trainable parameters: 10921049


* Nuriety pattern to Play blogge: The misssiol strategies and transfolk to wo halley focused our work's own forecative ar unique planning. This articiples in the Laine Golden

The Host Potential's top init's experienterment and its particular type and regional warveillance. In this article, I'll shedd a fascinat words contributies and other athlete's early what draw from genre, col up jands, lim body eparthory, there ancient work of pairitization criticallysis.

**Addition Enthusiast Players**
~
</code></pre></div></div> <p>Memory usage does not change much. But the training speed is much faster.</p> <p>With BF16 Memory usage</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 0.1s: nvidia-smi                                                                                                                          Sat Jul  6 17:19:50 2024

Sat Jul  6 17:19:50 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   61C    P0   280W / 300W |   5493MiB / 81920MiB |     98%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   32C    P0    47W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1083, val loss 1.1138
step 4500, loss: 1.149060606956482, dt: 46.36ms, tok/sec: 353409.81
Time taken: 323.1115171909332 seconds
Total parameters: 10921049
Trainable parameters: 10921049


* Thre-known variety is Mobile time forishing and a Piece**

Lerious la trend ammach focusion solus, the ways can date on how theselves presents, and cleaning ung time. While the Long-Anti Central to mitigate efforts, referred time

1. The Ritowastic Qrench Literian ass make-had a numberous unfounded women's explore a court of texture or the engage's earching try of from generating secrets. This benefit for a deterrmining movie excebrations, which its form its applyestriants. By regulating the

</code></pre></div></div> <p>It’s not faster. Why is that?</p> <h3 id="-trochcompile">+ troch.compile()</h3> <p>Got this error duing runtime. So I downgrade the python version to 3.11</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 236, in &lt;module&gt;
    model = torch.compile(model)
            ^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.12/site-packages/torch/__init__.py", line 1868, in compile
    raise RuntimeError("Dynamo is not supported on Python 3.12+")
RuntimeError: Dynamo is not supported on Python 3.12+
</code></pre></div></div> <p>Got another error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/tmp/tmpd5t0oroc/main.c: In function ‘list_to_cuuint64_array’:
/tmp/tmpd5t0oroc/main.c:354:3: error: ‘for’ loop initial declarations are only allowed in C99 mode
   for (Py_ssize_t i = 0; i &lt; len; i++) {
   ^
/tmp/tmpd5t0oroc/main.c:354:3: note: use option -std=c99 or -std=gnu99 to compile your code
/tmp/tmpd5t0oroc/main.c: In function ‘list_to_cuuint32_array’:
/tmp/tmpd5t0oroc/main.c:365:3: error: ‘for’ loop initial declarations are only allowed in C99 mode
   for (Py_ssize_t i = 0; i &lt; len; i++) {
   ^
/tmp/tmp0td94o5b/main.c: In function ‘list_to_cuuint64_array’:
/tmp/tmp0td94o5b/main.c:354:3: error: ‘for’ loop initial declarations are only allowed in C99 mode
   for (Py_ssize_t i = 0; i &lt; len; i++) {
   ^
</code></pre></div></div> <p>I install latest gcc version . Old version is 4.8.5 Fix error above.</p> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1037, val loss 1.1087
step 4500, loss: 1.151484727859497, dt: 23.18ms, tok/sec: 706728.75
Time taken: 285.365624666214 seconds
Total parameters: 10921049
Trainable parameters: 10921049
         emphasizing the parming of a spirit on the development conouches tended from the factor, such as jailust power traffice, and ixo, these can serve the alternatively takes part in processed.

**4. Inform 4applications Movements**

1. **Mush Secrets:** Students in Parleers (airl Warrioring change: Bhern Keshedmn) world, as initial do other basebar from leagues, to employ create messaters and their immpact on our camido opportunities. Its gain political windows, proteins and resoling affian languag
~
</code></pre></div></div> <h3 id="-flash-attention">+ Flash attention</h3> <p>Karpathy gives a brief introduction to flash attention in his video. Check out this video to know more . <a href="https://youtu.be/l8pRSuU81PU?t=7521">https://youtu.be/l8pRSuU81PU?t=7521</a></p> <p>Code:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">One head of self-attention</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">tril</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>
        <span class="c1"># wei = q @ k.transpose(-2, -1) * C **-0.5
</span>        <span class="c1"># wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)
</span>        <span class="c1"># wei = F.softmax(wei, dim=-1) # (B, T, T)
</span>        <span class="c1"># wei = self.dropout(wei)
</span>        <span class="c1"># out = wei @ v #(B,T,T) @ ( B, T, C) -&gt; (B, T, C)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

</code></pre></div></div> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.3369, val loss 1.3376
step 4500, loss: 1.3740415573120117, dt: 22.38ms, tok/sec: 731945.94
Time taken: 288.25680232048035 seconds
Total parameters: 10921049
Trainable parameters: 10921049
        facilitation slowing technique it.

In conclusion, is jilitary Ketins and scale, coreting the programphs of cocentries, many only soidhts:

1. **Ingluentalle performances**: Epchilitations and dinactions has functively cardolles experience witting, significantlyqtually.
3. **Pobitice, SStédio micrositiona Ristice**: The Presed full fame leintarians foreices, day exemisions and community to provide a more endaging of his play byautices. The New clear Ara Case gor as mains, Aheragement textures th
</code></pre></div></div> <p>Throughput increases 4\% compared to the previous version which is not a lot. I guess this is because the model is not big enough to benefit from the flash attention.</p> <h3 id="use-distributed-data-parallelddp">Use distributed data parallel(DDP)</h3> <p>Issue: I can only run with 4 GPUs. If I run with 8 GPUs, I got this error when I try to use all 8 gpus:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ddp_world_size 8
[rank4]: Traceback (most recent call last):
[rank4]:   File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 48, in &lt;module&gt;
[rank4]:     torch.cuda.set_device(device)
[rank4]:   File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/cuda/__init__.py", line 399, in set_device
[rank4]:     torch._C._cuda_setDevice(device)
[rank4]: RuntimeError: CUDA error: invalid device ordinal
[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

E0707 18:10:42.318000 47618031484288 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 4 (pid: 52464) of binary: /GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/bin/python                                                                                         Traceback (most recent call last):                                                                                                                    File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/bin/torchrun", line 33, in &lt;module&gt;                                                              sys.exit(load_entry_point('torch==2.3.1', 'console_scripts', 'torchrun')())                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                        File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:

Failures:
[1]:
  time      : 2024-07-07_18:10:42
  host      : gpu72
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 52465)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-07-07_18:10:42
  host      : gpu72
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 52466)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html                                                          [3]:
  time      : 2024-07-07_18:10:42
  host      : gpu72
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 52467)
  error_file: &lt;N/A&gt;                                                                                                                                   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------                                                                                        Root Cause (first observed failure):                                                                                                                [0]:
  time      : 2024-07-07_18:10:42                                                                                                                     host      : gpu72
  rank      : 4 (local_rank: 4)                                                                                                                       exitcode  : 1 (pid: 52464)                                                                                                                          error_file: &lt;N/A&gt;                                                                                                                                   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html                                                          ============================================================

</code></pre></div></div> <p>Actuall my task is assigned only 4 gpus even thought there are 8 gpus on the compute node. I run following code to get number of available cuda device I can use and it outputs 4.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda device count</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">())</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(nano-gpt) [nsccgz_qylin_1@ln102%tianhe2-K gpt-dev]$ yhrun -n 1 -N 1 -p GPU_A800 python test_gpt_count.py
cuda device count 4
</code></pre></div></div> <p>Asked bing and it gives a post that I can set visible cuda devices to all 8 gpus. And then I check this env variable and found that it only outputs <code class="language-plaintext highlighter-rouge">0,1,2,3</code>.</p> <p>So this is the reason why I can not use all 8 gpus.</p> <p>Problem fixed.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span>python test_gpt_count.py
cuda device count 4
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span>
0,1,2,3
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span>^C
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span><span class="nb">export </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="k">${</span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="k">}</span>,4,5,6,7
</code></pre></div></div> <p>Rewrite dataloader.</p> <p>Since now multiple processes are introduced to use multiple gpus, I need to rewrite the dataloader to make it work with multiple processes.</p> <p>Code: <code class="language-plaintext highlighter-rouge">self.process_rank</code> means current process rank. <code class="language-plaintext highlighter-rouge">self.num_process</code> means total number of processes.</p> <p><code class="language-plaintext highlighter-rouge">self.current_idx</code> is the current index of the data that the dataloader is reading. Note that each process will have it own data to train which is different from other processes. So <code class="language-plaintext highlighter-rouge">self.current_idx</code> moves forward by <code class="language-plaintext highlighter-rouge">B * T * self.num_process</code> each time.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DataLoader</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span>  <span class="n">process_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_process</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span>
        <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="n">self</span><span class="p">.</span><span class="n">process_rank</span> <span class="o">=</span> <span class="n">process_rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_process</span> <span class="o">=</span> <span class="n">num_process</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">process_rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">next_batch</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">current_idx</span><span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">buf</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">buf</span><span class="p">[</span><span class="mi">1</span><span class="p">:]).</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">+=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">num_process</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">num_process</span> <span class="o">+</span> <span class="mi">1</span><span class="o">&gt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">):</span>
            <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">process_rank</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>


</code></pre></div></div> <p>It takes twice longer time to finish the job after swtiching to new data loader which is weird. I don’t know why. Is this because of cache miss?</p> <p>I think it’s because of the cache miss. Token throughput after switching to new dataloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim  slurm-2769516.out
m device cuda:0
step &lt;built-in function iter&gt;: train loss4.6247, val loss 4.6222
step 0, loss: 6.304970741271973, dt: 253517.79ms, tok/sec: 64.63
step &lt;built-in function iter&gt;: train loss2.7204, val loss 2.7159
step 500, loss: 2.7232675552368164, dt: 22.78ms, tok/sec: 719357.23
step &lt;built-in function iter&gt;: train loss2.6621, val loss 2.6616
</code></pre></div></div> <p>Token throughput before switching to new dataloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim  slurm-2769041.out
step &lt;built-in function iter&gt;: train loss4.6102, val loss 4.6073
step 0, loss: 6.303163528442383, dt: 105310.21ms, tok/sec: 155.58
step &lt;built-in function iter&gt;: train loss2.6170, val loss 2.6169
step 500, loss: 2.6428515911102295, dt: 22.64ms, tok/sec: 723576.18
</code></pre></div></div> <p>It takes even longer after I move <code class="language-plaintext highlighter-rouge">to(device)</code> out of dataloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
tep &lt;built-in function iter&gt;: train loss2.2721, val loss 2.2711
step 4000, loss: 2.2670536041259766, dt: 22.41ms, tok/sec: 730972.72
step &lt;built-in function iter&gt;: train loss1.9989, val loss 1.9980
step 4500, loss: 2.078113079071045, dt: 22.69ms, tok/sec: 722123.90
Time taken: 651.469042301178 seconds
Total parameters: 10921049
Trainable parameters: 10921049                                                                                                                               the melorme slocites apctature to bayysic impessare to to obaytening ridence's, comuties the prayer'res of conngibuidiktle tooly soudiestinabo, creation, lov expertance entifuchitule, stluch pronuctingmat arous on vismok dailioout ateries tipl,, aphige sthatection of ecpppppivaclliency, powanct transps, and the owen, freper. Coulw fame inidtifinge for falstadry exertwing to socinigys the pecos, reame peound to its daman lawabyang ammbots'res coltual ra casergod asem, yucial gor mann textions mu
</code></pre></div></div> <h4 id="4-gpus-for-ddp">4 GPUs for ddp</h4> <p>Memory usage</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) [nsccgz_qylin_1@gpu72%tianhe2-K ~]$ nvidia-smi
Wed Jul 10 17:18:59 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   45C    P0   145W / 300W |   3975MiB / 81920MiB |     44%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   48C    P0   133W / 300W |   3979MiB / 81920MiB |     45%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  On   | 00000000:53:00.0 Off |                    0 |
| N/A   47C    P0   160W / 300W |   3979MiB / 81920MiB |     46%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  On   | 00000000:57:00.0 Off |                    0 |
| N/A   49C    P0   134W / 300W |   3959MiB / 81920MiB |     44%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 1.0s: nvidia-smi                                                                                                                                                                    Wed Jul 10 17:37:07 2024

Wed Jul 10 17:37:07 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   32C    P0    67W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   35C    P0    69W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  On   | 00000000:53:00.0 Off |                    0 |
| N/A   34C    P0    66W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  On   | 00000000:57:00.0 Off |                    0 |
| N/A   36C    P0    69W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Memory usage gradually increases. Why is this?</p> <p>Issue: Getting error that nccl report heartbeat error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[rank0]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 0] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=1
[rank0]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 0] ProcessGroupNCCL preparing to dump debug info.
[rank0]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 0] [PG 0 Rank 0] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 1
E0710 20:29:48.177000 47209663704448 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: -6) local_rank: 0 (pid: 1882) of binary: /GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/bin/python
Traceback (most recent call last):
</code></pre></div></div> <p>I think this is because I did not add this line of code</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):
</code></pre></div></div> <p>This code does not fix the problem.</p> <p>I am now trying to first call <code class="language-plaintext highlighter-rouge">destroy_process_group</code> and then calling tokens generation to fix issue above.</p> <p>I should decrease count of training iteration so that I can get error as early as possible.</p> <p>It takes 16 mins to finish model training which is not normal. I wonder why does it take so long after switching to new dataloader.</p> <p>Can this be a research problem?</p> <p>Why does it take so long to generate tokens after training is finished?.</p> <p>Is this because of decode part or is this because of the model inference part?</p> <p>Issue: Get this error saying that expected all tensors to be on the same device.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 292, in generate_tokens
    logits, loss = model(idx_cond) # (B,T,vocab_size)
                   ^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 255, in forward
    tok_emb = self.token_embedding_table(idx) #(B,T,C)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/functional.py", line 2264, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
</code></pre></div></div> <p>Don’t know which tensor is not on cuda. So I print device of <code class="language-plaintext highlighter-rouge">idx</code> to see if it’s on cuda.</p> <p>Issue above is sovled after I update the code like this</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span> <span class="p">)</span>
</code></pre></div></div> <p>cuda:0 do use lots of memory while other gpus have freed up memory.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   36C    P0    73W / 300W |   4867MiB / 81920MiB |    100%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   32C    P0    47W / 300W |     27MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  On   | 00000000:53:00.0 Off |                    0 |
| N/A   32C    P0    44W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  On   | 00000000:57:00.0 Off |                    0 |
| N/A   34C    P0    47W / 300W |      7MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Still can not generate tokens.</p> <p>I think this is because that I did not call model forward for all processes.</p> <p>The code runs successfully after I call model forward for all processes for generating text.</p> <p>It does not work even I set <code class="language-plaintext highlighter-rouge">model.eval()</code> only for master process. Why is that?</p> <p>Here’s the code that can finish successfully without process hanging</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_iters</span><span class="p">):</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="c1"># xb, yb = get_batch('train')
</span>    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">data_loader</span><span class="p">.</span><span class="nf">next_batch</span><span class="p">()</span>
    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">xb</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">yb</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">xb</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># with torch.autocast(device_type=device_type, dtype=torch.bfloat16):
</span>    <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

    <span class="c1"># if ddp:
</span>    <span class="c1">#     model.require_backward_grad_sync = True
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">ddp</span><span class="p">:</span>
        <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">AVG</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span> <span class="c1"># wait for the GPU to finish work
</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">token_processed</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">ddp_world_size</span>
    <span class="c1">#if step % eval_interval == 0 :
</span>    <span class="c1">#    losses = estimate_loss()
</span>    <span class="c1">#    print(f"step {iter}: train loss{losses['train']:.4f}, val loss {losses['val']:.4f}")
</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="p">(</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span> <span class="c1"># milli sec
</span>    <span class="n">token_per_sec</span> <span class="o">=</span> <span class="n">token_processed</span><span class="o">/</span> <span class="p">(</span><span class="n">t1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span>
    <span class="nf">call_generate_tokens</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>                                                                                                  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, dt: </span><span class="si">{</span><span class="n">dt</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">ms, tok/sec: </span><span class="si">{</span><span class="n">token_per_sec</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">master_process</span><span class="p">:</span>
</code></pre></div></div> <p>It looks like from this discuss post that I have to call forward and backwad for all processes. <a href="https://discuss.pytorch.org/t/multiple-forward-functions-in-dp-and-ddp/135029/5">https://discuss.pytorch.org/t/multiple-forward-functions-in-dp-and-ddp/135029/5</a></p> <p>No DDP with 1 gpu + new data loader + tf32, no torch.compile</p> <p>It’s pretty fast though.</p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 4998, loss: 1.5543745756149292, dt: 44.11ms, tok/sec: 371476.71
idx device: cuda:0
rank: 0,         broving and mited rocal hand alchiqe, Ond Wastestern have alter numpre, and society.

**Conclusion**

When 103, hith htreate analyzing into talent of the nets encerlated indituted to degensive sports and projote About to  trace producial stratege. Os a care a coved legelad interacting harswe her in the intricacies, with history and winnlowedge in 140s has bhe Bavill –as cit) Éanta.

**Upproach Inper To Adaptation**

The commmal tOxerium boodls to dimerstry of pinemanical the industry to interes
step 4999, loss: 1.5449110269546509, dt: 44.11ms, tok/sec: 371424.51
Time taken: 301.4059376716614 seconds
Total parameters: 10921049
Trainable parameters: 10921049
</code></pre></div></div> <p>DDP with 4 gpus , no torch.compile, tf32</p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 4998, loss: 1.2392717599868774, dt: 85.30ms, tok/sec: 768330.46
idx device:idx device:idx device:idx device:    cuda:3cuda:0cuda:1cuda:2



rank: 1,         Choroes, and formats. In this a
rank: 3,         Choroes, and formats. In this a
rank: 2,         Choroes, and formats. In this a
rank: 0,         Choroes, and formats. In this a
step 4999, loss: 1.2737213373184204, dt: 86.10ms, tok/sec: 761164.76
Time taken: 442.67850971221924 seconds
Total parameters: 10921049
Trainable parameters: 10921049
</code></pre></div></div> <p>I don’t know how FSDP( Fully sharded data parallel ) works yet.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="llm"/><summary type="html"><![CDATA[llm]]></summary></entry><entry><title type="html">System for machine learning papers</title><link href="https://bilyz98.github.io/blog/2024/sysml-papers/" rel="alternate" type="text/html" title="System for machine learning papers"/><published>2024-07-02T11:59:00+00:00</published><updated>2024-07-02T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/sysml-papers</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/sysml-papers/"><![CDATA[<p>For each paper I will give a brief summary and my thoughts on it. I will link each paper to other related papers if possible. Besides, I might add code and related resources if I have time.</p> <p>For most papers I will first read titles and abstract to decide whether I should read the rest part of the paper. This helps me to quickly filter out papers that are interesting to me.</p> <p>So for majority of papers I will make the summary short. For others that are interesting to me, I will write a longer summary.</p> <h4 id="characterization-of-large-language-model-development-in-the-datacenter">Characterization of Large Language Model Development in the Datacenter</h4> <p><a href="https://www.usenix.org/system/files/nsdi24-hu.pdf">https://www.usenix.org/system/files/nsdi24-hu.pdf</a></p> <p>Summary: This papers studies llm traning job workload in datacenter. It releases job traces in datacenter for training llm. It mentions that gpu take 65% of power usage.</p> <p>I didn’t find any other interesting contribution from this paper other than the job traces. So I won’t spend more time reading this paper.</p> <p>Does this paper mention llm serving ?</p> <h4 id="parcae-proactive-liveput-optimized-dnn-training-on-preemptible-instances">Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible Instances</h4> <p>Preemptive scheduling and checkpointing? <a href="https://www.usenix.org/conference/nsdi24/presentation/duan">https://www.usenix.org/conference/nsdi24/presentation/duan</a></p>]]></content><author><name></name></author><category term="ml"/><category term="mlsys"/><summary type="html"><![CDATA[sysml papers]]></summary></entry><entry><title type="html">nano-gpt and Transformer</title><link href="https://bilyz98.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="nano-gpt and Transformer"/><published>2024-06-29T11:59:00+00:00</published><updated>2024-06-29T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/transformer</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/transformer/"><![CDATA[<p>Code repo url: <a href="https://github.com/BilyZ98/nano-gpt">https://github.com/BilyZ98/nano-gpt</a></p> <h3 id="vanilla-bigram-model-without-self-attention">Vanilla bigram model without self attention</h3> <p>As mentioned in the youtube video <a href="https://youtu.be/kCc8FmEb1nY?t=2509">https://youtu.be/kCc8FmEb1nY?t=2509</a>, this code builds a bigram model without self attention. We can use this as baseline to compare with self attention code .</p> <p>Issue: can not use torch cuda module even though I have gpu and install cuda pytorch Solution: Tried to install pytorch cuda again Get this error when I tried to install pytroch-cuda:12.1</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ClobberError: This transaction has incompatible packages due to a shared path.
  packages: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64::jpeg-9e-h5eee18b_1, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'share/man/man1/rdjpgcom.1'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64::jpeg-9e-h5eee18b_1, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'share/man/man1/wrjpgcom.1'


</code></pre></div></div> <p>Solution: Switch to new environment and reinstall pytorch with cuda <a href="https://pytorch.org/get-started/locally/#windows-anaconda">https://pytorch.org/get-started/locally/#windows-anaconda</a></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda clean --all
conda clean -p
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
</code></pre></div></div> <p>Now I am able to see cuda available in pytorch</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># 0 corresponds to the first GPU
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano-gpt
True
NVIDIA A800 80GB PCIe
</code></pre></div></div> <p>Comparison of cpu and gpu for bigram model CPU:</p> <p>Time taken: 14.610548734664917 seconds</p> <p>GPU:</p> <p>Time taken: 18.080146074295044 seconds</p> <p>It takes longer time for gpu to finish. I think it’s because training iteration is not large enough to see the benefit of gpu.</p> <h2 id="transformer-architecture">Transformer architecture</h2> <p>Transformer is a type of neural network architecture that is designed to handle sequential data more effectively than traditional RNNs and LSTMs. It was introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, and has since become a popular choice for natural language processing tasks.</p> <p>As mentioned in this video transformer is like neural network version of map-reduce which also comes from google. The reduce process is the self attention process in transformer. The map process is the feed forward neural network and mutl-head in transformer.</p> <p>Why does transformer have Feedforward and Linear at the same time ? These two looks like the same. Andrej karparthy gives answer to this question at the time point in vide.</p> <p>Feedforward is used to think on the tensor/information the self-attention has produced. <br/> And this feedforward/computation is done in parallel which is pretty fast.</p> <p>The final linear layer is used to output token probabilities.</p> <p><img src="https://github.com/BilyZ98/BilyZ98.github.io/assets/26542149/4ce5458c-3c90-4607-803c-01631327ad0f" width="500" height="500"/></p> <h3 id="bigram-model-with-cpu">bigram model with cpu</h3> <p>Code link: <a href="https://github.com/BilyZ98/nano-gpt/blob/5cae2e1635dc560dc75dc92897ee5add43fa3aed/bigram.py">https://github.com/BilyZ98/nano-gpt/blob/5cae2e1635dc560dc75dc92897ee5add43fa3aed/bigram.py</a></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.5979, val loss 2.5999
Time taken: 38.3419029712677 seconds

SOd HADghe uio choaxlondorerl cy,m thamaw pes$!
LUANIV:
R:
RYo,
W:'th thiveCond wnff tod ghind.
Thathis:Ved esVI!
RUSape ms, yonyail lomustthend? thed of sofatiatherves f het m ssprerh fon,cke d pr&amp;lR.
-IUq-bind p'y w; deland walois WBy ethu l'd t y montircPEMPlanas y dslly?sthan coor ccoust d limald ped il f frs th.
ce our ntLE:

YCEqusI,
K:
wnea!vengjF, 'd
GGe cltLTod.
RSwoppiQYe haland dSt tXESacedDUCORGRENETof hos, sooumouloo meRTooe,
The cke;
ONGu he tpalapy an:
NScheracancoj

HAnend
ANUARK
</code></pre></div></div> <h3 id="bigram-model-with-gpu-with-positional-embedding-and-language-model-head">bigram model with gpu with positional embedding and language model head</h3> <p>No self attention for this version</p> <p>Issue: Get this error while running on gpu</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Indexing.cu:1236: indexSelectSmallIndex: block: [0,0,0], thread: [1,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.
</code></pre></div></div> <p>Searched google and it tells that the problem comes from incorrect indexing while using nn.Embedding.</p> <p>Solution: Check nn.Embedding code and fix it. Fix issue above by adding following code to <code class="language-plaintext highlighter-rouge">generate</code> function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>
  <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
      <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">block_size</span><span class="p">:]</span> <span class="c1"># This line of code fix the issue
</span>      <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># becomes (B, C)
</span>      <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, C)
</span>      <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(B,1)
</span>      <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(B, T+1)
</span>    <span class="k">return</span> <span class="n">idx</span>
</code></pre></div></div> <p>The reason is that <code class="language-plaintext highlighter-rouge">self.token_embedding_table</code> and <code class="language-plaintext highlighter-rouge">self.position_embedding_table</code> shares different input dimensions.</p> <p>If we don’t crop the <code class="language-plaintext highlighter-rouge">idx</code> to <code class="language-plaintext highlighter-rouge">idx_cond</code>, the <code class="language-plaintext highlighter-rouge">pos_emb</code> will take <code class="language-plaintext highlighter-rouge">T</code> that is larger than <code class="language-plaintext highlighter-rouge">block_size</code> which will cause the error.</p> <p>Please check this time in the video <a href="https://youtu.be/kCc8FmEb1nY?t=4854">https://youtu.be/kCc8FmEb1nY?t=4854</a></p> <p>code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>

  <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
      <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">block_size</span><span class="p">:]</span>
      <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
      <span class="c1">#print('shape of logits', logits.shape)
</span>      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># becomes (B, C)
</span>      <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, C)
</span>      <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(B,1)
</span>      <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(B, T+1)
</span>    <span class="k">return</span> <span class="n">idx</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model device cuda:0
m device cuda:0
step &lt;built-in function iter&gt;: train loss2.4926, val loss 2.5021
Time taken: 28.21234440803528 seconds



CEThik bridcowindakis by ble

Hiset bobe d e.
S:
O:
ISM:


Thanss:
Wanthar u qur, vet?
F dilasoate awice my.

Hnstarom oroup
Yowhthetof isth ble mil ndilll,

W:

Yeesengcin lat Heriliov ts, and Win nghire yombousel lind pe llllishe ce hiry:
Supr aisspllw y.
Hllin's noroopetelaves
Momy ll, d mothakeeo W-ndo whthCeiibyo touth dourive weeshieed t so mower; te

AN ad nterupt f s ar iris! m:
</code></pre></div></div> <h3 id="self-attention-with-single-head-on-gpu-with-positional-embedding-and-language-model-head">Self attention with single head on gpu with positional embedding and language model head</h3> <p>According to Andrej karparthy’s video, Self attention has three parts: key, query and value. These three parts are all tensors comming out from Linear layer with input tensor.</p> <p>query means what we are looking for each position in T.</p> <p>key means what we have for each input tensor in (T,C) format.<br/> T means context length in time and C means the number of channels or features.</p> <p>query dot product key to get weight matrix that specify importance of each time position in T.</p> <p>value means the information we get from Linear layer for each input tensor in (T,C) format.</p> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">One head of self-attention</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">tril</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">wei</span> <span class="o">=</span> <span class="n">wei</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">tril</span><span class="p">[:</span><span class="n">T</span><span class="p">,</span> <span class="p">:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span> <span class="c1"># (B, T, T)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">wei</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, T, T)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">wei</span> <span class="o">@</span> <span class="n">v</span> <span class="c1">#(B,T,T) @ ( B, T, C) -&gt; (B, T, C)
</span>        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">Head</span><span class="p">(</span><span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1">#(B,T, C)
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.4015, val loss 2.4166
Time taken: 56.774349212646484 seconds

Whent ik bry cowilen is by bth

Hiset bobe ale.
S:
O:
IS:
Falilauss ar btharu wearthe.
War dilasoate awice my.

HDER:
ANGo oug
Yowhavetof is he ot mil; dill, aes iree sen cie lat Herid ovets, and Win ngar ilerabous lelind peal.
-hull onchiry ptugr aiss hew ye wllinde norod atelaves
Momy ll, dl othake ont---o whth eiiby we ati dourive wee, ired thoouso er; th
To kad nteruptef so;
ARID Wam:
ENGCI inleront ffaf Pre?
</code></pre></div></div> <p>It does not improve a lot.</p> <h3 id="multi-head-self-attention-on-gpu-with-positional-embedding-and-language-model-head">Multi-head self attention on gpu with positional embedding and language model head</h3> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Multiple heads of self-attention in parallel</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">Head</span><span class="p">(</span><span class="n">head_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="c1"># self.sa = Head(n_embd)
</span>    <span class="n">self</span><span class="p">.</span><span class="n">sa_heads</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">//</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="c1"># x = self.sa(x)  #(B,T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa_heads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.2748, val loss 2.2858
Time taken: 90.66784453392029 seconds

Whent if bridcowd, whis byer that set bobe toe anthr-and mealleands:
Warth foulque, vet?
Wedtlay anes wice my.

HDY'n om oroug
Yowns, tof is heir thil; dill, aes isee sen cin lat Hetilrov the and Win now onderabousel.

SFAUS:
Shenser cechiry prugh aissthe, ye wing, u not
To thig I whomeny wod mothake ont---An hat evibys wietit, stile weeshirecs poor gier; to
To k danteref If sor; igre! mef thre inledo the af Pre?

WISo myay I sup!
Atied is:
Sadsal the E'd st hoin couk aar tey Iry to I frouf voul
</code></pre></div></div> <p>It looks better and the loss continues to decrease. But it takes longer to finish. Why is that ?</p> <h3 id="multi-head-self-attention-with-feed-forward-neural-network-on-gpu-with-positional-embedding-and-language-model-head">Multi-head self attention with feed forward neural network on gpu with positional embedding and language model head</h3> <p>Why don’t we add positional embedding again between blocks ?<br/> I think this can help transformer to keep track of the position of the tokens.</p> <p>I think we don’t need to add positional embedding again and again between blocks once we use residual connection.</p> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">a simple linear layer followed by a non-linearity</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="c1"># self.sa = Head(n_embd)
</span>    <span class="n">self</span><span class="p">.</span><span class="n">sa_heads</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">//</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="c1"># x = self.sa(x)  #(B,T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa_heads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <p>output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.2288, val loss 2.2414
Time taken: 102.24195170402527 seconds

And the Ror
Thow and is and thrad thom of oule.
Sthr-' my dall ands:
Warth fou qurord.
War dilth ane aw crup and not, ut onour
Yowns, tof it he cove lend lincath is ees, hain lat Het dulvets, and to poman is wables lill dite ullliser cecrivy prupthaiss hew youn's and knamopetell lownomthy wod moth keacal---A wher eiicks to thour rive cees, meds pood of he thu the hanterth po so;; igis! my to thy ale ontat af Pried my of.
WHINY ICHARD:
Poid:
Ardsal the Eget to uin cour ay andy Rry to chan the!
An
</code></pre></div></div> <p>Loss continue s to decrease but not decreases a lot</p> <h3 id="blocks-of-multi-head-self-attention-on-gpu">Blocks of multi-head self attention on gpu</h3> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Transfomer block: communication followed by computation</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span> <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="c1"># self.sa = Head(n_embd)
</span>    <span class="c1"># self.sa_heads = MultiHeadAttention(4, n_embd//4)
</span>    <span class="c1"># self.ffw = FeedForward(n_embd)
</span>    <span class="n">self</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="c1"># x = self.sa(x)  #(B,T, C)
</span>    <span class="c1"># x = self.sa_heads(x) #(B, T, C)
</span>    <span class="c1"># x = self.ffw(x) #(B, T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.3255, val loss 2.3400
Time taken: 141.53945779800415 seconds

And thif bry cowh, har on, ber
waiset bobe to tavegr-d my dalceauss:
Want he us he hentbardethas ane awche my.

HDEE:
Ay neou waowns
Moof is he me mil; dill, aes ireees, hain latiser drovets, and the nor ond wabousel lind thau.
Hhult cncriby: thartaiss hew you lome.
I yof petelgolg's my yow demeth kleonW nou when eiibas wouth dotrive weeshime sto-oche eroure
Thak danterurt fou ar irist muf thin inle oft to fearr?

KISomerry youu
Hartied is:
Aadsalce.

EIDLHY:
Iin couk aaraney Iry the han yo vely
</code></pre></div></div> <p>Does not improve</p> <h3 id="blocks-of-multi-head-self-attention-with-residule-connection-on-gpu">Blocks of multi-head self attention with residule connection on gpu</h3> <p>No projecttions for residual connection: Code:</p> <p>I only show the code for <code class="language-plaintext highlighter-rouge">Block</code> class because this is the only change in the code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Transfomer block: communication followed by computation</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span> <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.1115, val loss 2.1749
Time taken: 155.8028919696808 seconds

And they bridcown,
The layest madisen bube to tamaght' my daliea
My:
Waith foulqorth
but ceetlay ane awice my.

HEER:
An onour
Yount
Moofuing come mill dill, at miree seng, wilatist in ove the Bent longht is wais welll no me litles;
So chirs: ther aiss haw youn's mause roodeter'd swer:
Ill o' meacke
Ao Windo wht Ceiiby we ath do rive wees ire sto-of of he the the danterty po so;
Ang hink:
'Elt yould ontates
Mare?

KING ENCHENNL:
Hartied is wards beaces and thisin cour ay and
Hire the have fove y
</code></pre></div></div> <p>With projecttions for residual connection:</p> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Multiple heads of self-attention in parallel</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">Head</span><span class="p">(</span><span class="n">head_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">a simple linear layer followed by a non-linearity</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Transfomer block: communication followed by computation</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span> <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.0052, val loss 2.1047
Time taken: 128.87973642349243 seconds


KER:
Dy be will and is by be madised bube to take Our my dagalanss:
Wact me us crome. Wardethas anes wick, you, not to zoknow
Yourselvef is heart milled,
What grive, send, will is therevers, and the now on you me, lord dime littishe courmby pruperais'll woy. Hurmake norfore blaves home.
Who my thake of in on her eis as the most rive cenchimed the come, for unter hands thime son; if hink:
Edway male of wefife
Where, Som.
What suk!
Kered is wards.
Wice Efees bidin couses.
Wher, reath chan the wel
</code></pre></div></div> <p>It’s a little bit better compared to no-projection.</p> <p>What is the difference between these two ? I think projection is used to project output tensor from self attention to the same space with input tensor so that we can add them together.</p> <p>But from the result I can see the not projecting is also fine.</p> <h3 id="residual-blocks-of-self-attention-with-layernorm">Residual blocks of self attention with layernorm</h3> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Transfomer block: communication followed by computation</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span> <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">n_embd</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">n_embd</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">n_embd</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.9904, val loss 2.0918
Time taken: 165.2521140575409 seconds

And they bridle.

SOROR To beer a seek obe don.
Sagrad my dagalans!
You that us hear buble dilt
Hate away, my fears'd of of my
Yoursert foitie bettlit now
Whimes if ensen cim;
Stistaid ove the the me now on that thell in a wall thus would by pruppiness hiw you:
That I mandpeter'd gond:
Is would that
To Winson her eis all'd they srive will ime strow more-fore
To knom thrupt for trear. Wame monge inlee,
Thef firse?

KISTINUS:
If be!

GRESNY:

Sadave the Edwall?

GRAKE Masceave
Hir-bromence you! My
</code></pre></div></div> <p>loss drops a little bit more compared to no layernorm.</p> <h3 id="residual-blocks-of-self-attention-with-layernorm--dropout-full-transformer">Residual blocks of self attention with layernorm + dropout (Full transformer)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">One head of self-attention</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">tril</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">C</span> <span class="o">**-</span><span class="mf">0.5</span>
        <span class="n">wei</span> <span class="o">=</span> <span class="n">wei</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">tril</span><span class="p">[:</span><span class="n">T</span><span class="p">,</span> <span class="p">:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span> <span class="c1"># (B, T, T)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">wei</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, T, T)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">wei</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">wei</span> <span class="o">@</span> <span class="n">v</span> <span class="c1">#(B,T,T) @ ( B, T, C) -&gt; (B, T, C)
</span>        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Multiple heads of self-attention in parallel</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">Head</span><span class="p">(</span><span class="n">head_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">a simple linear layer followed by a non-linearity</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.1027, val loss 2.1580
Time taken: 155.4465034008026 seconds


LONIBENT:
Be a most unre maim, gruch, thath pcourdith my it frilf oblis then, nrie?

Mor hould bace                                                                                                                                                          Ce in apitng, anoth you his to cowll
By mbre wand grarist let fead as be meest, Jo afore by shalve my my sade make ta gior mony ow norane;                                                                   Hould-wrind awnAndead notooth. WARKEIY:                                                                                                                                                                                                                                                                                                         Conear gy?                                                                                                                                                              Srom ands, his gahpe with gowis slined fue no lot all wopmeseond in he tha dee knoth quail hen, slyold aus mawers, slosssig, yat but, hery,                             Ond you hom is oalt in, shealve of dRulet my bafker's deforth the sh
</code></pre></div></div> <p>It’s not getting better. I think this dropout will help when we scale up number of parameters.</p> <h3 id="full-transfomer-with-more-parameters">Full transfomer with more parameters</h3> <p>Previous parameters</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># what is the maximum context length for predictions
</span><span class="n">max_iters</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="c1"># device = 'cpu'
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">torch cuda available</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">n_embd</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">head_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
</code></pre></div></div> <p>Cur param:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">256</span> <span class="c1"># what is the maximum context length for predictions
</span><span class="n">max_iters</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="c1"># device = 'cpu'
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">torch cuda available</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">n_embd</span> <span class="o">=</span> <span class="mi">384</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">n_layer</span> <span class="o">=</span> <span class="mi">6</span>
<span class="c1"># head_size = 16
</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.8683, val loss 2.0060
Time taken: 221.6930215358734 seconds

Bale herse?

LAURENNIUS:
When regoubjon theme as be boeet chal speak treatuls a faulse heave ticeso too out boons in of, privy; char dese in paindaur meet is your both talies so are furnt ereworry,
Besse do you grait see fiee in vile should but roonouth
Than I k ount crow on
Sint, I eid, doust not will comon't te refore wife, which young so hing me the grow by-treate, witee of sword That the we great his worse a mick's sestit well sue
I inn frie dam west you that,S more think
That yest deart com
</code></pre></div></div> <p>loss does not drop to 1.4 which the output in karparthy’s video and the running time is too short.</p> <p>I see, there is a duplicate definition of <code class="language-plaintext highlighter-rouge">batch_size=4</code> and <code class="language-plaintext highlighter-rouge">block_size=8</code> in code. Let’s try to run again.</p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss0.8475, val loss 1.5921
Time taken: 737.4738621711731 seconds

Had you been such tongues, had love me but my lawful hold;
For I, shaked hand I for all discoversion,
To crook his heirs, to his crack that he.
Caius Marcius Corizelland! Murk'd, I had thou
Start with your charters, Warwick, remoning the
powerful leave.

MONTAGUE:
Once, the teernest thy power!

Second Murderer:
I will do not say, i' wedded with thy name?

Second Murderer:
Viello, thou hast affected the king's. Now thou hast
well a knoss to toe a stuffity thou in followine;
what thou hast no news
</code></pre></div></div> <p>The loss drops to 1.5. There is overfitting. The output from model looks better now.</p> <p>Great.</p> <h3 id="full-transformer-without-positional-embedding">Full transformer without positional embedding</h3> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1735, val loss 1.5787
Time taken: 722.4325501918793 seconds

Had you to 'd?

RTANIO:
With is a truless peach was fall by that says.
You will not sleeposed to hand to do on
Friar that thou should have had not daily in
Should show well Richard to him as the Antiates,
And helps in'd blazy smother with a right.

JULIA:
Then Flather sitter, for grief spirit! ah I must,
Then goes hared, now his king. To true. My hence
Be take upon this court-shalt I know.

CAPULET:
Amen, that I will we stille have nothing
Would dibedit her friend be rife;
And then did stuck dis
</code></pre></div></div> <p>Train loss is higher when not using positional encoding but val loss is similar.</p> <h3 id="load-dataset-from-huggingface-locally">Load dataset from huggingface locally</h3> <p>The <code class="language-plaintext highlighter-rouge">datasets</code> library from Hugging Face allows you to load local dataset files. Here’s how you can do it:</p> <p>If your local file is a CSV or JSON file, you can use the <code class="language-plaintext highlighter-rouge">load_dataset</code> function with the ‘csv’ or ‘json’ parameter, and specify the path to your local file³. Here’s an example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># For a CSV file
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="sh">'</span><span class="s">path/to/your/file.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># For a JSON file
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">json</span><span class="sh">'</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="sh">'</span><span class="s">path/to/your/file.json</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Please replace <code class="language-plaintext highlighter-rouge">'path/to/your/file.csv'</code> and <code class="language-plaintext highlighter-rouge">'path/to/your/file.json'</code> with the actual paths to your files³.</p> <p>If you have a dataset saved locally that was previously processed and saved using the <code class="language-plaintext highlighter-rouge">datasets</code> library’s <code class="language-plaintext highlighter-rouge">save_to_disk</code> method, you can load it using the <code class="language-plaintext highlighter-rouge">load_from_disk</code> function⁴:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_from_disk</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_from_disk</span><span class="p">(</span><span class="sh">'</span><span class="s">path/to/your/dataset</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Please replace <code class="language-plaintext highlighter-rouge">'path/to/your/dataset'</code> with the actual path to your dataset⁴.</p> <p>Remember to handle any errors that might occur when loading the dataset to make your code more robust¹.</p> <h3 id="try-use-another-dataset">Try use another dataset</h3> <p>I use this persona dataset from hugging face.</p> <p><a href="https://huggingface.co/datasets/proj-persona/PersonaHub">https://huggingface.co/datasets/proj-persona/PersonaHub</a></p> <p>Data preprocessing script</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="nb">dir</span> <span class="o">=</span> <span class="sh">'</span><span class="s">/mnt/nvme1n1/zt/persona_dataset/PersonaHub/</span><span class="sh">'</span>

<span class="n">output_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">./</span><span class="sh">'</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="nb">dir</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">file</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.jsonl</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">file_without_suffix</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nb">dir</span><span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">json</span><span class="sh">"</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="n">path</span> <span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="sh">'</span><span class="s">input persona</span><span class="sh">'</span>
        <span class="k">if</span> <span class="sh">'</span><span class="s">input persona</span><span class="sh">'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">continue</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">input persona</span><span class="sh">'</span><span class="p">])</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">synthesized text</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">output_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">file_without_suffix</span> <span class="o">+</span> <span class="sh">'</span><span class="s">.txt</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">])):</span>
                <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">input persona</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
                <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">synthesized text</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p>Everything is the same as before including tokenizer.</p> <p>Code:</p> <p>Small transformer model: Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>m device cuda:0
step &lt;built-in function iter&gt;: train loss6.3584, val loss 6.3542
step &lt;built-in function iter&gt;: train loss2.5460, val loss 2.5392
step &lt;built-in function iter&gt;: train loss2.3751, val loss 2.3770
step &lt;built-in function iter&gt;: train loss2.3018, val loss 2.3027
step &lt;built-in function iter&gt;: train loss2.2458, val loss 2.2467
step &lt;built-in function iter&gt;: train loss2.2214, val loss 2.2201
step &lt;built-in function iter&gt;: train loss2.1890, val loss 2.1726
step &lt;built-in function iter&gt;: train loss2.1662, val loss 2.1680
step &lt;built-in function iter&gt;: train loss2.1399, val loss 2.1323
step &lt;built-in function iter&gt;: train loss2.1246, val loss 2.1263
Time taken: 187.76913928985596 seconds
        That Reprotionsray explewtrale, and seactivity add ders eserts sor skedsaling.

Te ***: Ats, undives, and ceintilies, provarting onroegose, in insope develourate the venta portital ofcreal as coutwival naginaps tochud vellegican chand in da Stral Vorle.
3+ **Subre Spione Figsiples and of oon asterst a ow and The wern**: Hure devaleir a keverst and arine wathe pesing gemtunizing happesor ondins toudes storvititiciects.
3. The talue can-Ed thig SEflichat and lalleD, to a arte's and and expald on c
</code></pre></div></div> <p>Large model: Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss0.9089, val loss 0.9231
Time taken: 759.7053854465485 seconds
Total parameters: 11102681
Trainable parameters: 11102681
        2. Bruck-Mining and the level of the Nethiopy class, with a focus on the below vasle assems.
3. **Jasar's "Dayslettle)**: This subsequent fertilization of humor and time was named from the tournament of this women. The creation of forpireʾle artists across sitell signifying water patterns, romantic beefwere data that gained tasting in family.

**Game and Name Implacement: A Improvemented Shaped Story**

A following student football in this growth, typically family, and stigma's approach landscap
</code></pre></div></div> <p>Looks better</p> <p>Even larger model: Params:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">256</span> <span class="c1"># what is the maximum context length for predictions
</span><span class="n">max_iters</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="c1"># device = 'cpu'
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">torch cuda available</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">n_embd</span> <span class="o">=</span> <span class="mi">384</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">n_layer</span> <span class="o">=</span> <span class="mi">6</span>
<span class="c1"># head_size = 16
</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>

</code></pre></div></div> <p>Spent 6 times longer training time. We do see loss decrease though.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss0.7625, val loss 0.7859
Time taken: 4855.381325721741 seconds
Total parameters: 43635161
Trainable parameters: 43635161
        ll share stories that lessons they continue to have on Florist and the planet.

**Recent Recent Recognition: A Deep Dive into Your Authority**

When you begin to take a complex, it's essential. Our young music and information of florist taga), recognize the significance of "Rent Recognition in Baltin." This taggaent's contributions to fats of art, pushing it uses to engage with their examples with a similar panel that draw upon trade with fragmented flowing and dawn.

* Pay Flowing: Weed With a
</code></pre></div></div> <p>There is gpu memory usage flunctuation during training. Why is that?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch -n 0.1 nvidia-smi
</code></pre></div></div>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="llm"/><summary type="html"><![CDATA[llm]]></summary></entry><entry><title type="html">pytorch tensor.to</title><link href="https://bilyz98.github.io/blog/2024/pytorch/" rel="alternate" type="text/html" title="pytorch tensor.to"/><published>2024-06-28T11:59:00+00:00</published><updated>2024-06-28T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/pytorch</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/pytorch/"><![CDATA[<h2 id="move-tensor-to-cuda-with-different-name-returned">Move tensor to cuda with different name returned</h2> <p>Question: can I still use model to do training when I have following code ? In this code, I assign m to tensor on cuda. But I am not sure whether I can still use model to do back propagation</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">BigramLanguageModel</span><span class="p">()</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

</code></pre></div></div> <p>Yes, you can still use the <code class="language-plaintext highlighter-rouge">model</code> for training and backpropagation. When you do <code class="language-plaintext highlighter-rouge">m = model.to(device)</code>, it doesn’t create a new model. Instead, it moves the <code class="language-plaintext highlighter-rouge">model</code> to the specified <code class="language-plaintext highlighter-rouge">device</code> (which is presumably a CUDA device in your case) and returns a reference to the same model. So, <code class="language-plaintext highlighter-rouge">m</code> and <code class="language-plaintext highlighter-rouge">model</code> are actually the same model, just accessed through different names.</p> <p>Here’s an example of how you might use it:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">BigramLanguageModel</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move model to device
</span>
<span class="c1"># define loss function and optimizer
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="c1"># training loop
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">targets</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># forward pass
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="c1"># backward pass and optimization
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, <code class="language-plaintext highlighter-rouge">model</code> is used for both the forward pass and the backward pass, and it works correctly because <code class="language-plaintext highlighter-rouge">model</code> and <code class="language-plaintext highlighter-rouge">m</code> are the same model. The key point is that the model parameters need to be on the same device as the data and targets when you perform the forward and backward passes. That’s why <code class="language-plaintext highlighter-rouge">inputs</code> and <code class="language-plaintext highlighter-rouge">targets</code> are also moved to <code class="language-plaintext highlighter-rouge">device</code> before the forward pass.</p> <p>So, in your case, you can continue to use <code class="language-plaintext highlighter-rouge">model</code> in your code for training and backpropagation, even after calling <code class="language-plaintext highlighter-rouge">model.to(device)</code>. Just make sure to move your inputs and targets to the same device before you use them with your model. If you want to use <code class="language-plaintext highlighter-rouge">m</code> instead of <code class="language-plaintext highlighter-rouge">model</code> after the <code class="language-plaintext highlighter-rouge">to(device)</code> call, that’s fine too. It’s really just a matter of which name you prefer to use. They both refer to the same underlying model.</p> <h3 id="difference-between-modeltodevice-and-modelcuda">Difference between <code class="language-plaintext highlighter-rouge">model.to(device)</code> and <code class="language-plaintext highlighter-rouge">model.cuda()</code></h3> <p>The <code class="language-plaintext highlighter-rouge">.to()</code> method and <code class="language-plaintext highlighter-rouge">.cuda()</code> are both used to move PyTorch tensors to a specific device (CPU or GPU), but they have some differences:</p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">.to(device)</code></strong>: <ul> <li>The <code class="language-plaintext highlighter-rouge">.to()</code> method is more general and versatile. It allows you to move a tensor to any specified device (CPU or a specific GPU).</li> <li>You can pass either a string (e.g., <code class="language-plaintext highlighter-rouge">"cuda"</code> or <code class="language-plaintext highlighter-rouge">"cpu"</code>) or a specific device (e.g., <code class="language-plaintext highlighter-rouge">torch.device("cuda:0")</code>) as an argument.</li> <li>Example: <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Move tensor to the first available GPU (if available)
</span><span class="n">tensor</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda:1</span><span class="sh">"</span><span class="p">))</span>  <span class="c1"># Move tensor to the second GPU (if available)
</span></code></pre></div> </div> </li> </ul> </li> <li><strong><code class="language-plaintext highlighter-rouge">.cuda()</code></strong>: <ul> <li>The <code class="language-plaintext highlighter-rouge">.cuda()</code> method is specifically for moving tensors to a GPU (CUDA device).</li> <li>It is a shorthand for <code class="language-plaintext highlighter-rouge">.to("cuda")</code>.</li> <li>If you call <code class="language-plaintext highlighter-rouge">.cuda()</code> without any arguments, it will move the tensor to the first available GPU (if any). If no GPUs are available, it will raise an error.</li> <li>Example: <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>  <span class="c1"># Move tensor to the first available GPU (if available)
</span></code></pre></div> </div> </li> </ul> </li> </ol>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="cuda"/><summary type="html"><![CDATA[pytorch]]></summary></entry><entry><title type="html">Install neovim with old glibc</title><link href="https://bilyz98.github.io/blog/2024/install-neovim/" rel="alternate" type="text/html" title="Install neovim with old glibc"/><published>2024-06-25T11:59:00+00:00</published><updated>2024-06-25T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/install-neovim</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/install-neovim/"><![CDATA[<p>Issue: neovim can not load libc.so.6 which requires glibc_2.28 which is higher than installed glibc version.</p> <p>After asking bing chat and find out that I need to install latest glibc version with sudo which I can’t I decided to clone neovim and compile locally.</p> <p>glibc is a system level package.</p> <p>Previsou release that requires higher glibc version</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln101%tianhe2-K nvim-linux64]$ ./bin/nvim --version
./bin/nvim: /lib64/libc.so.6: version `GLIBC_2.28' not found (required by ./bin/nvim)                                                                                   ./bin/nvim: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by ./bin/nvim)                                                                                   (llmc) [nsccgz_qylin_1@ln101%tianhe2-K nvim-linux64]$ ldd ./bin/nvim
./bin/nvim: /lib64/libc.so.6: version `GLIBC_2.28' not found (required by ./bin/nvim)                                                                                   ./bin/nvim: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by ./bin/nvim)                                                                                           linux-vdso.so.1 =&gt;  (0x00007ffc5e491000)
        libm.so.6 =&gt; /lib64/libm.so.6 (0x00002ae00f451000)
        libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00002ae00f753000)
        libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00002ae00f957000)
        libgcc_s.so.1 =&gt; /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/libgcc_s.so.1 (0x00002ae00eb83000)                                                                      libc.so.6 =&gt; /lib64/libc.so.6 (0x00002ae00fb73000)
        /lib64/ld-linux-x86-64.so.2 (0x00002ae00eb4a000)
        libutil.so.1 =&gt; /lib64/libutil.so.1 (0x00002ae00ff40000)
</code></pre></div></div> <p>I installed this release which does not require glibc.2.31 and now I can run it successfully.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>https://github.com/neovim/neovim-releases/releases
</code></pre></div></div> <p>current one</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln101%tianhe2-K nvim-linux64]$ ldd ./bin/nvim
        linux-vdso.so.1 =&gt;  (0x00007ffedf36c000)
        libm.so.6 =&gt; /lib64/libm.so.6 (0x00002ba2ec6ee000)                                                                                                                      libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00002ba2ec9f0000)
        librt.so.1 =&gt; /lib64/librt.so.1 (0x00002ba2ecbf4000)
        libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00002ba2ecdfc000)
        libgcc_s.so.1 =&gt; /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/libgcc_s.so.1 (0x00002ba2ebc2a000)
        libc.so.6 =&gt; /lib64/libc.so.6 (0x00002ba2ed018000)                                                                                                                      /lib64/ld-linux-x86-64.so.2 (0x00002ba2ebbf1000)
        libutil.so.1 =&gt; /lib64/libutil.so.1 (0x00002ba2ed3e5000)
</code></pre></div></div> <p>It’s too time consuming to build the neovim from scratch.</p> <h2 id="config-path-for-neovim-to-start-with-different-config">.config path for neovim to start with different config</h2> <p>by default, Neovim looks for configuration files in the <code class="language-plaintext highlighter-rouge">~/.config/nvim</code> directory. However, when you use the <code class="language-plaintext highlighter-rouge">NVIM_APPNAME</code> environment variable, Neovim will look for the configuration in the <code class="language-plaintext highlighter-rouge">~/.config/{NVIM_APPNAME}</code> directory instead.</p> <p>So, if you set <code class="language-plaintext highlighter-rouge">NVIM_APPNAME=lunarvim</code>, Neovim will look for the configuration files in the <code class="language-plaintext highlighter-rouge">~/.config/lunarvim</code> directory. Similarly, if <code class="language-plaintext highlighter-rouge">NVIM_APPNAME=nvchad</code>, it will look in the <code class="language-plaintext highlighter-rouge">~/.config/nvchad</code> directory.</p> <p>This allows you to have multiple separate configurations that you can switch between just by changing the <code class="language-plaintext highlighter-rouge">NVIM_APPNAME</code> environment variable. It’s a powerful feature for managing multiple Neovim configurations.</p> <p>Remember to clone or place your desired configurations into the respective directories under <code class="language-plaintext highlighter-rouge">~/.config/</code>. For example, if you’re using LunarVim and NvChad, you should have <code class="language-plaintext highlighter-rouge">~/.config/lunarvim</code> and <code class="language-plaintext highlighter-rouge">~/.config/nvchad</code> directories, each containing the respective configuration files.</p> <p>If you want to use the default configuration, you can just run <code class="language-plaintext highlighter-rouge">nvim</code> without setting the <code class="language-plaintext highlighter-rouge">NVIM_APPNAME</code> environment variable, and it will look for the configuration files in the default <code class="language-plaintext highlighter-rouge">~/.config/nvim</code> directory.</p> <p>Please note that the <code class="language-plaintext highlighter-rouge">~/.config</code> directory is a standard for user-specific application configuration files on Unix-like operating systems. It’s defined by the <a href="https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html">XDG Base Directory Specification</a>. If you want to use a different directory, you’ll need to change the <code class="language-plaintext highlighter-rouge">XDG_CONFIG_HOME</code> environment variable, which defaults to <code class="language-plaintext highlighter-rouge">~/.config</code>.</p>]]></content><author><name></name></author><category term="vim"/><category term="vim"/><summary type="html"><![CDATA[vim]]></summary></entry><entry><title type="html">llm.c</title><link href="https://bilyz98.github.io/blog/2024/llm-c/" rel="alternate" type="text/html" title="llm.c"/><published>2024-06-22T11:59:00+00:00</published><updated>2024-06-22T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/llm-c</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/llm-c/"><![CDATA[<p>roadmap</p> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Running llm.c</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Running llm.c with cuda</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Multiple gpus</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Inference with fp16</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Inference with vllm</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>try other inference acceleartion tech</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Submit pr to check existence of openmpi by specifying openmpi path. ( can use conda as example)</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Submit pr to check curl result when proxy server returns 503 error.</li> </ul> <h2 id="running-llmc">Running llm.c</h2> <p><a href="https://github.com/karpathy/llm.c">https://github.com/karpathy/llm.c</a></p> <h3 id="gpu">GPU</h3> <p>Had issue running gpu There is only cuda 11.2 on my machine but torch 2.1.0 is installed which requires cuda 12.0</p> <p>Solution: Manually specify torch==1.3.1</p> <p>Get error</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yhrun <span class="nt">-n</span> 4 <span class="nt">-p</span> gpu_v100 python train_gpt2.py
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
yhrun: error: gpu55: tasks 0-3: Exited with exit code 1
</code></pre></div></div> <p>The issue is that nullcontext is introduced in python &gt;=3.7 So I need to upgrade python version</p> <p>Still can not solve problem above because I can’t not import new module to existing module list.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Currently Loaded Modulefiles:
 1) proxy/1.0   2) CUDA/10.0   3) cudnn/7.6.4-CUDA10.0   4) PyTorch/1.2.0-CUDA10.0-py3.6

 $ yhrun -n 4 -p gpu_v100 python train_gpt2.py
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
</code></pre></div></div> <p>My friend told me that I can just use conda to create new namespace and then I can ssh to the compute node and activate the conda environment. And then I can run training process.</p> <p>This means that compute node shares the same file system with login node. But the operating system is different. Because each node has its own hostname.</p> <p>Learn new thing every day.</p> <p>Here’s all available nodes I have.</p> <p>Karpathy has updated gpt2 parameter download script so now I can download parameter via shell script</p> <p>Issue: Can not connect to huggingface todownload pretrained model via proxy</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln102%tianhe2-K llm.c]$ curl -v https://huggingface.co
* About to connect() to proxy 10.20.18.21 port 3128 (#0)
*   Trying 10.20.18.21...
* Connected to 10.20.18.21 (10.20.18.21) port 3128 (#0)
* Establish HTTP proxy tunnel to huggingface.co:443
&gt; CONNECT huggingface.co:443 HTTP/1.1
&gt; Host: huggingface.co:443
&gt; User-Agent: curl/7.29.0
&gt; Proxy-Connection: Keep-Alive
&gt;
&lt; HTTP/1.1 503 Service Unavailable
&lt; Proxy-Agent: gost/2.11.1
&lt; Content-Length: 0
&lt;
* Received HTTP code 503 from proxy after CONNECT
* Connection #0 to host 10.20.18.21 left intact
curl: (56) Received HTTP code 503 from proxy after CONNECT
</code></pre></div></div> <p>Solution: I decide to download on my local laptop and then upload these model parameter files to gpu nodes.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod </span>u+x ./dev/download_starter_pack.sh
./dev/download_starter_pack.sh
make train_gpt2fp32cu
./train_gpt2fp32cu
</code></pre></div></div> <p>cuda env:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Currently Loaded Modulefiles:
 1) proxy/1.0   2) python/3.6.7_anaconda3   3) CUDA/11.2   4) gmp/4.2.4   5) mpfr/2.4.2   6) mpc/0.8.1   7) gcc/9.2.0
</code></pre></div></div> <p>Output :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step   61/74: train loss 3.213066 (312.014672 ms, 13127 tok/s)
step   62/74: train loss 3.450736 (314.262273 ms, 13033 tok/s)
step   63/74: train loss 3.370245 (315.130342 ms, 12997 tok/s)
step   64/74: train loss 3.407992 (316.778140 ms, 12930 tok/s)
step   65/74: train loss 3.580323 (315.324538 ms, 12989 tok/s)
step   66/74: train loss 3.029552 (317.274858 ms, 12909 tok/s)
step   67/74: train loss 3.296448 (317.588671 ms, 12897 tok/s)
step   68/74: train loss 3.675703 (314.929981 ms, 13006 tok/s)
step   69/74: train loss 3.297087 (313.282229 ms, 13074 tok/s)
step   70/74: train loss 3.646337 (315.271277 ms, 12991 tok/s)
step   71/74: train loss 3.566427 (316.123225 ms, 12956 tok/s)
step   72/74: train loss 3.732521 (315.446478 ms, 12984 tok/s)
step   73/74: train loss 3.825229 (318.325142 ms, 12867 tok/s)
step   74/74: train loss 3.380326 (318.066751 ms, 12877 tok/s)
val loss 3.491223
generating:
---
BUCKINGHAM:
But of my penitent ambition
Rome Slicom against Reimy, justice about him!
In case the witness should speak with joy:
Shall now that by these dwelling House,
Suspicions are declaim'd of the Albanian king.
Go
---
total average iteration time: 312.354733 ms
</code></pre></div></div> <h3 id="multiple-gpus">Multiple GPUs</h3> <p>Run with MPI. Don’t know mpi works internally but I will just start using it to train model.</p> <p>I will learn the internals later.</p> <p>Now I just login to gpu node and run the following command</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make train_gpt2cu
mpirun <span class="nt">-np</span> &lt;number of GPUs&gt; ./train_gpt2cu
</code></pre></div></div> <p>Issue: failed to compile with openmpi I used hpc cluster which has openmpi library installed in directory that is different from standard directory.</p> <p>Here’s Makefile in llm.c</p> <div class="language-makefile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">ifeq</span> <span class="nv">($(NO_MULTI_GPU), 1)</span>
  <span class="err">$(info</span> <span class="err">→</span> <span class="err">Multi-GPU</span> <span class="err">(OpenMPI</span> <span class="err">+</span> <span class="err">NCCL)</span> <span class="err">is</span> <span class="err">manually</span> <span class="err">disabled)</span>
<span class="k">else</span>
  <span class="k">ifneq</span> <span class="nv">($(OS), Windows_NT)</span>
    <span class="c"># Detect if running on macOS or Linux
</span>    <span class="k">ifeq</span> <span class="nv">($(SHELL_UNAME), Darwin)</span>
      <span class="err">$(info</span> <span class="err">✗</span> <span class="err">Multi-GPU</span> <span class="err">on</span> <span class="err">CUDA</span> <span class="err">on</span> <span class="err">Darwin</span> <span class="err">is</span> <span class="err">not</span> <span class="err">supported,</span> <span class="err">skipping</span> <span class="err">OpenMPI</span> <span class="err">+</span> <span class="err">NCCL</span> <span class="err">support)</span>
    <span class="err">else</span> <span class="k">ifeq</span> <span class="nv">($(shell [ -d /usr/lib/x86_64-linux-gnu/openmpi/lib/ ] &amp;&amp; [ -d /usr/lib/x86_64-linux-gnu/openmpi/include/ ] &amp;&amp; echo "exists"), exists)</span>
      <span class="err">$(info</span> <span class="err">✓</span> <span class="err">OpenMPI</span> <span class="err">found,</span> <span class="err">OK</span> <span class="err">to</span> <span class="err">train</span> <span class="err">with</span> <span class="err">multiple</span> <span class="err">GPUs)</span>
      <span class="nv">NVCC_INCLUDES</span> <span class="o">+=</span> <span class="nt">-I</span>/usr/lib/x86_64-linux-gnu/openmpi/include
      <span class="nv">NVCC_LDFLAGS</span> <span class="o">+=</span> <span class="nt">-L</span>/usr/lib/x86_64-linux-gnu/openmpi/lib/
      <span class="nv">NVCC_LDLIBS</span> <span class="o">+=</span> <span class="nt">-lmpi</span> <span class="nt">-lnccl</span>
      <span class="nv">NVCC_FLAGS</span> <span class="o">+=</span> <span class="nt">-DMULTI_GPU</span>
    <span class="k">else</span>
      <span class="err">$(info</span> <span class="err">✗</span> <span class="err">OpenMPI</span> <span class="err">is</span> <span class="err">not</span> <span class="err">found,</span> <span class="err">disabling</span> <span class="err">multi-GPU</span> <span class="err">support)</span>
      <span class="err">$(info</span> <span class="err">---&gt;</span> <span class="err">On</span> <span class="err">Linux</span> <span class="err">you</span> <span class="err">can</span> <span class="err">try</span> <span class="err">install</span> <span class="err">OpenMPI</span> <span class="err">with</span> <span class="err">`sudo</span> <span class="err">apt</span> <span class="err">install</span> <span class="err">openmpi-bin</span> <span class="err">openmpi-doc</span> <span class="err">libopenmpi-dev`)</span>
    <span class="k">endif</span>
  <span class="k">endif</span>
<span class="k">endif</span>
</code></pre></div></div> <p>It checks existence of openmpi library in <code class="language-plaintext highlighter-rouge">/usr/lib/x86_64-linux-gnu/openmpi/lib/</code> Openmpi library is at ` ~/local/lib/` in my hpc cluster. Should I raise a pr?</p> <p>Issue： Get compilation error when linking nccl</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/GPUFS/app_GPU/compiler/CUDA/11.2.0/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 -DMULTI_GPU -DENABLE_BF16 train_gpt2.cu -lcublas -lcublasLt -L~/local/lib/  -I~/local/include/   -lmpi -lnccl -o train_gpt2cu
llmc/zero.cuh(28): error: identifier "ncclBfloat16" is undefined

llmc/zero.cuh(209): error: identifier "ncclAvg" is undefined

llmc/zero.cuh(219): error: identifier "ncclAvg" is undefined

3 errors detected in the compilation of "train_gpt2.cu".
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>I have load nccl module but I still get this error and I don’t know how to fix it. Try to compile train_gpt2fp32cu</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load  CUDA/11.2
module load  gcc/9.2.0
<span class="c">#module load  openmpi/1.10.2-pgi-17.1</span>
module load openmpi/3.1.4-icc-18.0.1
module load  nccl/2.9.9-1-cuda-11.0
module list
which nvcc
<span class="nb">pushd </span>llm.c
<span class="c">#make train_gpt2cu</span>
make train_gpt2fp32cu
mpirun <span class="nt">-np</span> 2 ./train_gpt2fp32cu
<span class="nb">popd</span>

</code></pre></div></div> <p>Get out of memory error when running</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 74                                                 |
| val_num_batches       | 8                                                  |
+-----------------------+----------------------------------------------------+
allocated 474 MiB for model parameters
| train_num_batches     | 74                                                 |
| val_num_batches       | 8                                                  |
+-----------------------+----------------------------------------------------+
allocated 474 MiB for model parameters
allocated 5706 MiB for activations
allocated 5706 MiB for activations
val loss 4.513921
val loss 4.513921
allocated 474 MiB for parameter gradients
allocated 252 MiB for activation gradients
allocated 474 MiB for AdamW optimizer state m
allocated 474 MiB for AdamW optimizer state v
allocated 474 MiB for parameter gradients
allocated 252 MiB for activation gradients
[CUDA ERROR] at file train_gpt2_fp32.cu:1443:
out of memory
</code></pre></div></div> <p>I am not famaliar with how cuda can work with multiple gpus when doing training.</p> <p>Should I learn a little bit more about how can I use multiple gpus to do computation when working with cuda?</p> <p>Why do we have to use mpi to run with multiple gpus?</p> <p>Let’s check whether single gpu code actually uses single gpu or not.</p> <p>There’s only one gpu running when training with single gpu. And it works pretty well. So I want to know how to use multiple gpus to train model.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) [nsccgz_qylin_1@gpu29%tianhe2-K zt]$ nvidia-smi
Fri Jun 21 12:17:35 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:8A:00.0 Off |                    0 |
| N/A   62C    P0   276W / 300W |   8354MiB / 16160MiB |     98%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   32C    P0    38W / 300W |      3MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   31C    P0    37W / 300W |      2MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   31C    P0    37W / 300W |      3MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A    153170      C   ./train_gpt2fp32cu               8351MiB |
+-----------------------------------------------------------------------------+
</code></pre></div></div> <p>Here’s the code that initializes multi gpu training config in train_gpt2.cu</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MultiGpuConfig</span> <span class="nf">multi_gpu_config_init</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">***</span><span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
<span class="cp">#ifdef MULTI_GPU
</span>    <span class="c1">// Initialize MPI.</span>
    <span class="n">MultiGpuConfig</span> <span class="n">result</span><span class="p">;</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Init</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">));</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">process_rank</span><span class="p">));</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">num_processes</span><span class="p">));</span>
    <span class="n">result</span><span class="p">.</span><span class="n">local_device_idx</span> <span class="o">=</span> <span class="n">multi_gpu_get_local_device_idx</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">process_rank</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">num_processes</span><span class="p">);</span>
    <span class="n">cudaCheck</span><span class="p">(</span><span class="n">cudaSetDevice</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">local_device_idx</span><span class="p">));</span>
    <span class="n">ncclUniqueId</span> <span class="n">nccl_id</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">process_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">ncclCheck</span><span class="p">(</span><span class="n">ncclGetUniqueId</span><span class="p">(</span><span class="o">&amp;</span><span class="n">nccl_id</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Bcast</span><span class="p">((</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">nccl_id</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">nccl_id</span><span class="p">),</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">));</span>
    <span class="n">ncclCheck</span><span class="p">(</span><span class="n">ncclCommInitRank</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">nccl_comm</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">num_processes</span><span class="p">,</span> <span class="n">nccl_id</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">process_rank</span><span class="p">));</span>
    <span class="n">cudaCheck</span><span class="p">(</span><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">nccl_stream</span><span class="p">));</span>
    <span class="c1">// event without timing for maximum performance</span>
    <span class="n">cudaCheck</span><span class="p">(</span><span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">compute_nccl_sync</span><span class="p">,</span> <span class="n">cudaEventDisableTiming</span><span class="p">));</span>
    <span class="n">nvtxNameCudaStreamA</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">nccl_stream</span><span class="p">,</span> <span class="s">"nccl stream"</span><span class="p">);</span>
    <span class="n">nvtxNameCudaEventA</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">compute_nccl_sync</span><span class="p">,</span> <span class="s">"nccl compute sync"</span><span class="p">);</span>

</code></pre></div></div> <p>Ask bingchat to explain this code This code is for initializing a multi-GPU configuration using MPI (Message Passing Interface), CUDA, and NCCL (NVIDIA Collective Communications Library). Here’s a breakdown of what the code does:</p> <ol> <li> <p><code class="language-plaintext highlighter-rouge">MultiGpuConfig result;</code>: This line initializes a <code class="language-plaintext highlighter-rouge">MultiGpuConfig</code> object named <code class="language-plaintext highlighter-rouge">result</code>.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Init(argc, argv));</code>: This line initializes the MPI environment.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Comm_rank(MPI_COMM_WORLD, &amp;result.process_rank));</code>: This line determines the rank of the calling process in the communicator.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Comm_size(MPI_COMM_WORLD, &amp;result.num_processes));</code>: This line determines the number of processes in the communicator.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">result.local_device_idx = multi_gpu_get_local_device_idx(result.process_rank, result.num_processes);</code>: This line gets the local device index based on the process rank and the number of processes.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cudaCheck(cudaSetDevice(result.local_device_idx));</code>: This line sets the device to be used for GPU executions.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">ncclUniqueId nccl_id;</code>: This line declares a unique NCCL ID.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">if (result.process_rank == 0) { ncclCheck(ncclGetUniqueId(&amp;nccl_id)); }</code>: This line generates a new NCCL unique ID and assigns it to <code class="language-plaintext highlighter-rouge">nccl_id</code> if the process rank is 0.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Bcast((void *)&amp;nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD));</code>: This line broadcasts the NCCL unique ID from the root process (process with rank 0) to all other processes in the communicator.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">ncclCheck(ncclCommInitRank(&amp;result.nccl_comm, result.num_processes, nccl_id, result.process_rank));</code>: This line initializes the NCCL communicator according to the rank, the number of processes, and the unique NCCL ID.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cudaCheck(cudaStreamCreate(&amp;result.nccl_stream));</code>: This line creates a new asynchronous stream.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cudaCheck(cudaEventCreate(&amp;result.compute_nccl_sync, cudaEventDisableTiming));</code>: This line creates an event with no timing data. This is used for synchronization purposes.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nvtxNameCudaStreamA(result.nccl_stream, "nccl stream");</code>: This line names the CUDA stream “nccl stream” for profiling and debugging purposes.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nvtxNameCudaEventA(result.compute_nccl_sync, "nccl compute sync");</code>: This line names the CUDA event “nccl compute sync” for profiling and debugging purposes.</p> </li> </ol> <p>This function is designed to be used in a multi-GPU environment where multiple processes are running in parallel, each possibly on a different GPU.</p> <p>The use of MPI allows these processes to communicate with each other, and the use of NCCL allows them to collectively perform operations such as reductions and broadcasts.</p> <p>The CUDA streams and events are used for managing and synchronizing the GPU computations.</p> <p>The NVTX calls are used for profiling and debugging the GPU code.</p> <p>Note that this code assumes that the <code class="language-plaintext highlighter-rouge">MULTI_GPU</code> macro is defined; if it is not, this function will not do anything.</p> <p>Is there any way to run with multi-gpus without callling like this ?</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make train_gpt2cu
mpirun <span class="nt">-np</span> &lt;number of GPUs&gt; ./train_gpt2cu
</code></pre></div></div> <p>So now I will try to run very simple mpi and cuda program to test cuda and mpi env on my hpc cluster.</p> <pre><code class="language-cu">#include &lt;stdio.h&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;mpi.h&gt;

__global__ void helloFromGPU(void) {
    printf("Hello World from GPU %d!\n", blockIdx.x);
}

int main(int argc, char** argv) {
    // Initialize the MPI environment
    MPI_Init(&amp;argc, &amp;argv);

    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);

    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);

    // Set the device to the rank of the current MPI process
    cudaSetDevice(world_rank);

    printf("Hello World from CPU of MPI process %d!\n", world_rank);

    // Launch the kernel on the GPU
    helloFromGPU&lt;&lt;&lt;world_size, 1&gt;&gt;&gt;();

    // Wait for GPU to finish before accessing on host
    cudaDeviceSynchronize();

    // Finalize the MPI environment.
    MPI_Finalize();
}

~                                                                                                                                                                                   ~                                                                                                                                                                                   ~
</code></pre> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) $ nvcc -I/GPUFS/nsccgz_qylin_1/local/include -L/GPUFS/nsccgz_qylin_1/local/lib  -lmpi hello_mpi_cuda.cu -o hello_mpi_cuda
(base) $ ls
hello.cu  hello_cuda  hello_mpi  hello_mpi.c  hello_mpi_cuda  hello_mpi_cuda.cu
(base) $ mpirun -np 2 ./hello_mpi_cuda
--------------------------------------------------------------------------
WARNING: No preset parameters were found for the device that Open MPI
detected:

  Local host:            gpu29
  Device name:           mlx5_9
  Device vendor ID:      0x02c9
  Device vendor part ID: 4116

Default device parameters will be used, which may result in lower
performance.  You can edit any of the files specified by the
btl_openib_device_param_files MCA parameter to set values for your
device.

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_no_device_params_found to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   gpu29
  Local device: mlx5_9
--------------------------------------------------------------------------
Hello World from CPU of MPI process 0!
Hello World from CPU of MPI process 1!
Hello World from GPU 0!
Hello World from GPU 1!
Hello World from GPU 0!
Hello World from GPU 1!
[gpu29:229661] 1 more process has sent help message help-mpi-btl-openib.txt / no device params found
[gpu29:229661] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[gpu29:229661] 1 more process has sent help message help-mpi-btl-openib.txt / error in device init
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">nvcc</code> is just like <code class="language-plaintext highlighter-rouge">gcc</code> which is used to compile cuda code.</p> <p>What is <code class="language-plaintext highlighter-rouge">nccl</code>? Basically <code class="language-plaintext highlighter-rouge">nccl</code> is just like <code class="language-plaintext highlighter-rouge">mpi</code>. It’s used to do collective communication between gpus.</p> <p>This struct is used as config to maintain as information about each gpu.</p> <pre><code class="language-cu">// Parameters specific to training on multiple GPUs.
typedef struct {
  int process_rank;      // Rank of this process among all MPI processes on all hosts. 0 if no multi-GPU.
  int num_processes;     // Total number of processes on all hosts. 1 if no multi-GPU.
  int local_device_idx;  // This process GPU index on current machine. 0 if no multi-GPU.
  ncclComm_t nccl_comm;  // NCCL communication primitive, used for collective mutli-GPU work.
} MultiGpuConfig;

// Determine which GPU this process should use.
// Processes on the same machines use different GPU indicies. Processes on other machines don't.
// Copied from NCCL examples: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/examples.html#example-2-one-device-per-process-or-thread
int multi_gpu_get_local_device_idx(int process_rank, int num_processes) {
  char hostname[1024];
  hostname[1023] = '\0';
  // All processes on the same machine will share the same hostname.
  gethostname(hostname, 1023);
  for (int i=0; i &lt; 1024; i++) {
    if (hostname[i] == '.') {
        hostname[i] = '\0';
        break;
    }
  }
  uint64_t hostname_hash = 5381;
  for (int c = 0; hostname[c] != '\0'; c++){ hostname_hash = ((hostname_hash &lt;&lt; 5) + hostname_hash) ^ hostname[c]; }

  // Distribute all hostname hashes to all processes.
  uint64_t* all_hostsname_hashes = (uint64_t*)malloc(num_processes * sizeof(uint64_t));
  all_hostsname_hashes[process_rank] = hostname_hash;
  mpiCheck(MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, all_hostsname_hashes, sizeof(uint64_t), MPI_BYTE, MPI_COMM_WORLD));

  // Identify which GPU we need to use.
  int local_device_idx = 0;
  for (int current_process = 0; current_process &lt; num_processes; ++current_process) {
     if (current_process == process_rank) {
      // Found my gpu, local_device_idx now has my target GPU index.
      break;
     }
     if (all_hostsname_hashes[current_process] == all_hostsname_hashes[process_rank]) {
      // This process ID runs on the same machine, but it's not me, skip this GPU
      local_device_idx++;
     }
  }

  free(all_hostsname_hashes);
  return local_device_idx;
}
</code></pre> <p>Get this error.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>error: identifier "ncclAvg" is undefined
</code></pre></div></div> <p>Check nccl commit message and found that ncclAvg is added in version 2.10.3 I have only <code class="language-plaintext highlighter-rouge">nccl/2.9.9-1-cuda-11.0</code>.</p> <p>What should I do?</p> <p>Just learned that I can use <code class="language-plaintext highlighter-rouge">conda</code> to install cuda and nccl. Let’s try it.</p> <p>So what is conda and how does it work? I think conda is just a package manager like <code class="language-plaintext highlighter-rouge">apt</code> in ubuntu. <code class="language-plaintext highlighter-rouge">conda</code> helps with environment management and package installation.</p> <p><code class="language-plaintext highlighter-rouge">apt</code> helps with package installation and update.</p> <p><code class="language-plaintext highlighter-rouge">spack</code> is another package manager that is used in hpc cluster.</p> <p><a href="https://stackoverflow.com/a/78227826/14600569">https://stackoverflow.com/questions/77873047/what-are-the-key-differences-between-spack-and-conda-package-managers</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda <span class="nb">install </span>nvidia::cuda-toolkit
</code></pre></div></div> <p>Get another compilation error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/gcc/x86_64-conda-linux-gnu/12.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib//libcublas.so: undefined reference to `memcpy@GLIBC_2.14'
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/gcc/x86_64-conda-linux-gnu/12.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/libpmix.so.2: undefined reference to `clock_gettime@GLIBC_2.17'
collect2: error: ld returned 1 exit status
make: *** [train_gpt2cu] Error 255
(llmc) [nsccgz_qylin_1@gpu29%tianhe2-K llm.c]$ gcc --version
gcc (conda-forge gcc 12.3.0-11) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre></div></div> <p>check glibc version.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu29%tianhe2-K llm.c]$ ldd --version
ldd (GNU libc) 2.17
Copyright (C) 2012 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
Written by Roland McGrath and Ulrich Drepper.
(llmc) [nsccgz_qylin_1@gpu29%tianhe2-K llm.c]$ which ldd
/usr/bin/ldd
</code></pre></div></div> <p>I think I need to use ldd in conda env to check glibc version. How can I do this? I can not install specified glibc version with conda.</p> <p>Remove conda env and create a new one. Install nccl first</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge
    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge
    cuda-version-12.5          |       hd4f0392_3          21 KB  conda-forge
    libgcc-ng-13.2.0           |      h77fa898_11         777 KB  conda-forge
    libgomp-13.2.0             |      h77fa898_11         434 KB  conda-forge
    libstdcxx-ng-13.2.0        |      hc0a3c3a_11         3.7 MB  conda-forge
    nccl-2.22.3.1              |       hbc370b7_0       107.3 MB  conda-forge
    ------------------------------------------------------------
                                           Total:       112.2 MB

The following NEW packages will be INSTALLED:

  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge
  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu
  cuda-version       conda-forge/noarch::cuda-version-12.5-hd4f0392_3
  libgcc-ng          conda-forge/linux-64::libgcc-ng-13.2.0-h77fa898_11
  libgomp            conda-forge/linux-64::libgomp-13.2.0-h77fa898_11
  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-13.2.0-hc0a3c3a_11
  nccl               conda-forge/linux-64::nccl-2.22.3.1-hbc370b7_0
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span>nvidia/label/cuda-12.5.0::cuda-toolkit
</code></pre></div></div> <p>conda adjust channels priority</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda config --describe channel_priority
conda config --set channel_priority flexible
conda config --prepend channels conda-forge
conda config --prepend channels nvidia
conda config --show channels
</code></pre></div></div> <p>Error no nvtx3 and no openmpi</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable
✓ OpenMP found
✗ OpenMPI is not found, disabling multi-GPU support
---&gt; On Linux you can try install OpenMPI with `sudo apt install openmpi-bin openmpi-doc libopenmpi-dev`
✓ nvcc found, including GPU/CUDA support
---------------------------------------------
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmcc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt   -o train_gpt2cu
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>I remove channel conda-forge and try to install cuda from main anaconda channel.</p> <p>channel config is stored in <code class="language-plaintext highlighter-rouge">~/.condarc</code></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda info
conda remove <span class="nt">--name</span> myenv <span class="nt">--all</span>
</code></pre></div></div> <p>Fix openmpi issue after installing cuda from main channel</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln101 llm.c]$ PRECISION=FP32 make train_gpt2cu
__nvcc_device_query failed to call cudaLoader::cuInit(0) with error 0x64 (CUDA_ERROR_NO_DEVICE)
---------------------------------------------
→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable
✓ OpenMP found
✓ OpenMPI found, OK to train with multiple GPUs
✓ nvcc found, including GPU/CUDA support
---------------------------------------------
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
nvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>Forget that I should ssh to compute node. Let’s try again</p> <p>Still get no nvtx3 error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ PRECISION=FP32 make train_gpt2cu
---------------------------------------------
→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable
✓ OpenMP found
✓ OpenMPI found, OK to train with multiple GPUs
✓ nvcc found, including GPU/CUDA support
---------------------------------------------
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>Get installment error again</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ conda install cuda-nvtx -c nvidia
Channels:
 - nvidia
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: failed

InvalidSpec: The package "nvidia/linux-64::cuda==12.5.0=0" is not available for the specified platform

(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ exit
</code></pre></div></div> <p>https://github.com/NVIDIA/NVTX I will manually pull header files to node if I can’t now solve this issue this time by install 12.0.0 version of cuda</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install nvidia/label/cuda-12.0.0::cuda-toolkit
</code></pre></div></div> <p>Fix nvtx header file after copying to system path</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In file included from train_gpt2.cu:65:
llmc/zero.cuh:16:10: fatal error: nccl.h: No such file or directory
   16 | #include &lt;nccl.h&gt;
      |          ^~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:65:
llmc/zero.cuh:16:10: fatal error: nccl.h: No such file or directory
   16 | #include &lt;nccl.h&gt;
      |          ^~~~~~~~
compilation terminated.
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K nccl_2.22.3-1+cuda12.4_x86_64]$ cp -r include/* /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/
(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K nccl_2.22.3-1+cuda12.4_x86_64]$ cp -r lib/* /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /GPUFS/nsccgz_qylin_1/local/lib/libopen-pal.so.40: undefined reference to `pci_device_cfg_read'
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/</code> is not included in <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> when doing <code class="language-plaintext highlighter-rouge">ld</code> command. So I need to include that in <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> before running <code class="language-plaintext highlighter-rouge">ld</code> command.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ echo $LD_LIBRARY_PATH
/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/t/spack/opt/spack/linux-centos7-haswell/gcc-4.8.5/libevent-2.1.12-oysfi7miuhw62ginwjrr2uy6yldr2oav/lib:/GPUFS/app_GPU/application/anaconda3/5.3.1/envs/python-3.6/lib:/GPUFS/nsccgz_qylin_1/ryz/MARL-test/util/lib:/GPUFS/nsccgz_qylin_1/ryz/icf_test/build_GPTL/gptl_gcc/lib::/GPUFS/nsccgz_qylin_1/software/spack/opt/spack/linux-centos7-skylake_avx512/
</code></pre></div></div> <p>Should I switch to spack next time?</p> <p>Finally I am able to compile this train_gpt2cu after several days of failing and trying. I put conda lib at the first of <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> and it works.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ export  LD_LIBRARY_PATH=/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:$
LD_LIBRARY_PATH
(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ echo $LD_LIBRARY_PATH
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/t/spack/opt/spack/linux-centos7-haswell/gcc-4.8.5/libevent-2.1.12-oysfi7miuhw62ginwjrr2uy6yldr2oav/lib:/GPUFS/app_GPU/application/anaconda3/5.3.1/envs/python-3.6/lib:/GPUFS/nsccgz_qylin_1/ryz/MARL-test/util/lib:/GPUFS/nsccgz_qylin_1/ryz/icf_test/build_GPTL/gptl_gcc/lib::/GPUFS/nsccgz_qylin_1/software/spack/opt/spack/linux-centos7-skylake_avx512/:/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/
</code></pre></div></div> <p>Issue: cuda driver version is not compatible with cuda runtime version. What should I do?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
gpu 30
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:8A:00.0 Off |                    0 |
| N/A   39C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   34C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   33C    P0    37W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   36C    P0    51W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre></div></div> <p>Guess I have to install cuda 11.2 to match the driver version. Have to download nccl with cuda 11.</p> <p>However, I don’t find cuda:toolkit that is compatible with cuda 11.2 in conda.</p> <p>I think one solution is to compile nccl manually on this compute node with old cuda driver.</p> <p>But now I just switch use A800 which comes with cuda 12.0 driver. Let’s go. I have two A800 available so I can test cross node training and multi gpu card training. This is great.</p> <p>Issue: Seems that program is loading a .bin file with _bf16</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
</code></pre></div></div> <p>Finally I think I successfully run model training on two gpus on A800. Here’s compile and run script.</p> <p>compile.sh</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">echo</span> <span class="nv">$LD_LIBRARY_PATH</span>
yhrun <span class="nt">-n1</span> <span class="nt">-p</span> GPU_A800 make train_gpt2cu
</code></pre></div></div> <p>run.sh</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
conda activate llmc
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">echo</span> <span class="nv">$LD_LIBRARY_PATH</span>
yhrun <span class="nt">-n2</span> <span class="nt">-p</span> GPU_A800 /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/mpirun <span class="nt">-np</span> 2 train_gpt2cu
</code></pre></div></div> <p>output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| weight init method    | OpenAI's GPT-2 checkpoint                          |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| weight init method    | OpenAI's GPT-2 checkpoint                          |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 37                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 37                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 2                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
HellaSwag eval not found at dev/data/hellaswag/hellaswag_val.bin, skipping its evaluation
You can run `python dev/data/hellaswag.py` to export and use it with `-h 1`.
num_parameters: 124475904 =&gt; bytes: 248951808
allocated 237 MiB for model parameters
batch_size B=4 * seq_len T=1024 * num_processes=2 and total_batch_size=8192

step   15/37 | train loss 3.640566 | norm 2.2486 | lr 3.00e-04 | 268.99 ms | -100.0% bf16 MFU | 63701 tok/s
step   14/37 | train loss 3.381429 | norm 2.5808 | lr 3.00e-04 | 287.48 ms | -100.0% bf16 MFU | 57996 tok/s
step   16/37 | train loss 3.413064 | norm 2.0632 | lr 3.00e-04 | 174.35 ms | -100.0% bf16 MFU | 62144 tok/s
step   15/37 | train loss 3.640008 | norm 2.3061 | lr 3.00e-04 | 147.72 ms | -100.0% bf16 MFU | 57748 tok/s
step   17/37 | train loss 3.584605 | norm 2.0251 | lr 3.00e-04 | 158.54 ms | -100.0% bf16 MFU | 61209 tok/s
step   16/37 | train loss 3.412876 | norm 2.0334 | lr 3.00e-04 | 135.09 ms | -100.0% bf16 MFU | 58018 tok/s
step   18/37 | train loss 3.486408 | norm 1.6377 | lr 3.00e-04 | 115.45 ms | -100.0% bf16 MFU | 62047 tok/s
step   17/37 | train loss 3.584733 | norm 2.0324 | lr 3.00e-04 | 141.30 ms | -100.0% bf16 MFU | 58014 tok/s
step   19/37 | train loss 3.450470 | norm 2.1760 | lr 3.00e-04 | 120.89 ms | -100.0% bf16 MFU | 62521 tok/s
step   18/37 | train loss 3.487690 | norm 1.6469 | lr 3.00e-04 | 128.45 ms | -100.0% bf16 MFU | 58510 tok/s
step   20/37 | train loss 3.542054 | norm 2.2739 | lr 3.00e-04 | 66.91 ms | -100.0% bf16 MFU | 67331 tok/s
step   19/37 | train loss 3.451924 | norm 2.1979 | lr 3.00e-04 | 118.83 ms | -100.0% bf16 MFU | 59375 tok/s
step   20/37 | train loss 3.544027 | norm 2.2397 | lr 3.00e-04 | 108.95 ms | -100.0% bf16 MFU | 60644 tok/s
</code></pre></div></div> <p>6x throughput compared to v100.</p> <p>However, I am not sure if this trully accelerate training. How can I verify that multiple gpu training accelerate training process ?</p> <h3 id="cpu">CPU</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
python dev/data/tinyshakespeare.py
python train_gpt2.py
make train_gpt2
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>8 ./train_gpt2
</code></pre></div></div> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 20: train loss 4.527330 (took 2636.617334 ms)
step 21: train loss 4.065797 (took 2701.692621 ms)
step 22: train loss 3.965316 (took 2681.297241 ms)
step 23: train loss 3.449409 (took 2650.111416 ms)
step 24: train loss 4.490954 (took 2637.116332 ms)
step 25: train loss 4.035361 (took 2659.843151 ms)
step 26: train loss 3.445302 (took 2652.557792 ms)
step 27: train loss 3.993789 (took 2649.868369 ms)
step 28: train loss 4.199468 (took 2638.095098 ms)
step 29: train loss 4.538460 (took 2669.385015 ms)
val loss 4.350866
step 30: train loss 4.306292 (took 2658.306411 ms)
step 31: train loss 4.851407 (took 2634.616368 ms)
step 32: train loss 4.577479 (took 2670.470130 ms)
step 33: train loss 4.124943 (took 2660.545565 ms)
step 34: train loss 4.330319 (took 2669.532886 ms)
step 35: train loss 3.399416 (took 2639.378693 ms)
step 36: train loss 3.661207 (took 2632.377219 ms)
step 37: train loss 3.330453 (took 2637.114896 ms)
step 38: train loss 3.567853 (took 2645.744510 ms)
step 39: train loss 3.902004 (took 2635.939546 ms)
val loss 4.319361
generating:
---
EditBOOK IX:
Under the boasted sute of Georges:
So lordly is the prize had sin is high;
Hell is the way to God: frankish friends from blessed daughters
To Bermuda have heard the saying,
Then how to place the artscape.
Strong should a bellow
---
step 40: train loss 3.952987 (took 2665.948189 ms)
</code></pre></div></div> <p>Some questions? How many low end gpus are there in the market? I am thinking about utilizing low end gpus to train model, large or small model.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="cuda"/><summary type="html"><![CDATA[llm minikune]]></summary></entry><entry><title type="html">Basic digital electronic</title><link href="https://bilyz98.github.io/blog/2024/digital-electronic/" rel="alternate" type="text/html" title="Basic digital electronic"/><published>2024-06-18T11:59:00+00:00</published><updated>2024-06-18T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/digital-electronic</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/digital-electronic/"><![CDATA[<h2 id="basic-digital">Basic digital</h2> <p>Transistor to ALU <a href="https://youtu.be/HjneAhCy2N4?si=OpouvGQDJhw4sSKE">https://youtu.be/HjneAhCy2N4?si=OpouvGQDJhw4sSKE</a></p> <p>Transistor to memory <a href="https://youtu.be/rM9BjciBLmg?si=TQe2Wijej4iezyzV">https://youtu.be/rM9BjciBLmg?si=TQe2Wijej4iezyzV</a></p> <p>I learn from this video that memory is made of several gates to remember bit information and controls whether to enable write.</p> <p>Another interesting thing I learn is that 2 dimensions of gates are used to reduce number of wires to store same amount of information.</p>]]></content><author><name></name></author><category term="ml-fundamental"/><category term="transistor"/><summary type="html"><![CDATA[transistor]]></summary></entry><entry><title type="html">K8s Advance</title><link href="https://bilyz98.github.io/blog/2024/k8s-advance/" rel="alternate" type="text/html" title="K8s Advance"/><published>2024-06-16T11:59:00+00:00</published><updated>2024-06-16T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/k8s-advance</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/k8s-advance/"><![CDATA[<h2 id="kubectl-inspect-volume-content"><code class="language-plaintext highlighter-rouge">kubectl</code> inspect volume content</h2> <p><a href="https://stackoverflow.com/questions/49529005/how-to-inspect-the-content-of-persistent-volume-by-kubernetes-on-azure-cloud-ser">https://stackoverflow.com/questions/49529005/how-to-inspect-the-content-of-persistent-volume-by-kubernetes-on-azure-cloud-ser</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: pvc-inspector
spec:
  containers:
  - image: busybox
    name: pvc-inspector
    command: ["tail"]
    args: ["-f", "/dev/null"]
    volumeMounts:
    - mountPath: /pvc
      name: pvc-mount
  volumes:
  - name: pvc-mount
    persistentVolumeClaim:
      claimName: YOUR_CLAIM_NAME_HERE
</span><span class="no">EOF
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl exec -it pvc-inspector -- sh
$ ls /pvc
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl delete pod pvc-inspector
</code></pre></div></div>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="k8s"/><summary type="html"><![CDATA[k8s minikune]]></summary></entry></feed>