<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bilyz98.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bilyz98.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-19T01:02:06+00:00</updated><id>https://bilyz98.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Python Package Path</title><link href="https://bilyz98.github.io/blog/2024/python-package-path/" rel="alternate" type="text/html" title="Python Package Path"/><published>2024-08-10T00:00:00+00:00</published><updated>2024-08-10T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/python-package-path</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/python-package-path/"><![CDATA[<p>I have micrograd2 under micrograd folder but this error shows when I run test.py. how to fix it ?</p> <p>File “/home/zt/bilyz_website_new/code/micrograd/test/test.py”, line 2, in <module> from micrograd2.engine import Value ModuleNotFoundError: No module named 'micrograd2'</module></p> <p>It looks like Python is unable to find the <code class="language-plaintext highlighter-rouge">micrograd2</code> module. Here are a few steps you can try to resolve this issue:</p> <ol> <li> <p><strong>Check the Directory Structure</strong>: Ensure that the <code class="language-plaintext highlighter-rouge">micrograd2</code> folder is correctly placed within the <code class="language-plaintext highlighter-rouge">micrograd</code> directory and that it contains an <code class="language-plaintext highlighter-rouge">__init__.py</code> file. This file is necessary for Python to recognize the directory as a package.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> micrograd/
 ├── micrograd2/
 │   ├── __init__.py
 │   └── engine.py
 └── test/
     └── test.py
</code></pre></div> </div> </li> <li><strong>Update the Import Statement</strong>: If <code class="language-plaintext highlighter-rouge">micrograd2</code> is a subdirectory of <code class="language-plaintext highlighter-rouge">micrograd</code>, you might need to adjust your import statement in <code class="language-plaintext highlighter-rouge">test.py</code> to reflect the correct path. Try changing: <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">from</span> <span class="n">micrograd2.engine</span> <span class="kn">import</span> <span class="n">Value</span>
</code></pre></div> </div> <p>to:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">from</span> <span class="n">micrograd.micrograd2.engine</span> <span class="kn">import</span> <span class="n">Value</span>
</code></pre></div> </div> </li> <li><strong>Check the PYTHONPATH</strong>: Ensure that the <code class="language-plaintext highlighter-rouge">micrograd</code> directory is included in your <code class="language-plaintext highlighter-rouge">PYTHONPATH</code>. You can temporarily add it by running: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">export </span><span class="nv">PYTHONPATH</span><span class="o">=</span>/home/zt/bilyz_website_new/code/micrograd:<span class="nv">$PYTHONPATH</span>
</code></pre></div> </div> </li> <li><strong>Run the Script from the Correct Directory</strong>: Make sure you are running <code class="language-plaintext highlighter-rouge">test.py</code> from the <code class="language-plaintext highlighter-rouge">micrograd</code> directory or adjust your working directory accordingly.</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">cpp async</title><link href="https://bilyz98.github.io/blog/2024/cpp-async/" rel="alternate" type="text/html" title="cpp async"/><published>2024-08-05T11:59:00+00:00</published><updated>2024-08-05T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cpp-async</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cpp-async/"><![CDATA[<p><a href="https://github.com/muluoleiguo/interview/blob/master/%E9%9D%A2%E8%AF%95/c%2B%2B%E5%B9%B6%E5%8F%91/%E5%A4%9A%E7%BA%BF%E7%A8%8B/C%2B%2B%E5%A4%9A%E7%BA%BF%E7%A8%8B.md">async</a></p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thread&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;mutex&gt;</span><span class="cp">
#include</span><span class="cpf">&lt;future&gt;</span><span class="cp">
#include</span><span class="cpf">&lt;Windows.h&gt;</span><span class="cp">
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>
<span class="kt">double</span> <span class="nf">t1</span><span class="p">(</span><span class="k">const</span> <span class="kt">double</span> <span class="n">a</span><span class="p">,</span> <span class="k">const</span> <span class="kt">double</span> <span class="n">b</span><span class="p">)</span>
<span class="p">{</span>
 <span class="kt">double</span> <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span>
 <span class="n">Sleep</span><span class="p">(</span><span class="mi">3000</span><span class="p">);</span><span class="c1">//假设t1函数是个复杂的计算过程，需要消耗3秒</span>
 <span class="k">return</span> <span class="n">c</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> 
<span class="p">{</span>
 <span class="kt">double</span> <span class="n">a</span> <span class="o">=</span> <span class="mf">2.3</span><span class="p">;</span>
 <span class="kt">double</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">6.7</span><span class="p">;</span>
 <span class="n">future</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">fu</span> <span class="o">=</span> <span class="n">async</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">);</span><span class="c1">//创建异步线程线程，并将线程的执行结果用fu占位；</span>
 <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"正在进行计算"</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
 <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"计算结果马上就准备好，请您耐心等待"</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
 <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"计算结果："</span> <span class="o">&lt;&lt;</span> <span class="n">fu</span><span class="p">.</span><span class="n">get</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span><span class="c1">//阻塞主线程，直至异步线程return</span>
        <span class="c1">//cout &lt;&lt; "计算结果：" &lt;&lt; fu.get() &lt;&lt; endl;//取消该语句注释后运行会报错，因为future对象的get()方法只能调用一次。</span>
 <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>正在进行计算
计算结果马上就准备好，请您耐心等待
计算结果：9
</code></pre></div></div>]]></content><author><name></name></author><category term="cpp"/><category term="cpp"/><summary type="html"><![CDATA[async]]></summary></entry><entry><title type="html">cpp thread local</title><link href="https://bilyz98.github.io/blog/2024/cpp-thread-local/" rel="alternate" type="text/html" title="cpp thread local"/><published>2024-08-05T11:59:00+00:00</published><updated>2024-08-05T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cpp-thread-local</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cpp-thread-local/"><![CDATA[<p><a href="https://github.com/muluoleiguo/interview/blob/master/%E9%9D%A2%E8%AF%95/c%2B%2B%E5%B9%B6%E5%8F%91/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E7%BA%BF%E7%A8%8B%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8.md">thread-local</a></p> <p>When to use thread local?</p> <p>When you want to store data that is unique to each thread, you can use thread local storage. This is useful when you want to store data that is global to a thread, but not global to the entire program. For example, you might want to store a counter that is unique to each thread, or a pointer to a resource that is unique to each thread.</p> <p>Usually each thread uses thread local when there are multiple function calls in each thread and each function call needs to access the same data.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thread&gt;</span><span class="cp">
</span>
<span class="kt">void</span> <span class="nf">add</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">thread_local</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1">// static thread_local int count = 0; // 两种写法等价！</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="n">n</span><span class="p">;</span>
    <span class="c1">// 休眠n秒，防止输出时数据交错（Mac会出现）</span>
    <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">seconds</span><span class="p">(</span><span class="n">n</span><span class="p">));</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">get_id</span><span class="p">()</span><span class="o">&lt;&lt;</span><span class="s">":"</span><span class="o">&lt;&lt;</span><span class="n">count</span><span class="o">&lt;&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">td</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">td</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">td</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">join</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>23456242050624:1
23456239949376:2
</code></pre></div></div> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thread&gt;</span><span class="cp">
</span>
<span class="k">class</span> <span class="nc">A</span> <span class="p">{</span>
<span class="nl">public:</span>
    <span class="kt">void</span> <span class="n">dump</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="n">id</span><span class="o">&lt;&lt;</span><span class="s">":"</span><span class="o">&lt;&lt;</span><span class="n">count</span><span class="o">&lt;&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="o">::</span><span class="n">id</span> <span class="n">id</span><span class="p">;</span>
    <span class="k">static</span> <span class="k">thread_local</span> <span class="kt">int</span> <span class="n">count</span><span class="p">;</span>
<span class="p">};</span>
<span class="k">thread_local</span> <span class="kt">int</span> <span class="n">A</span><span class="o">::</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

<span class="kt">void</span> <span class="nf">add</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">A</span> <span class="n">a</span><span class="p">;</span>
    <span class="n">a</span><span class="p">.</span><span class="n">id</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">get_id</span><span class="p">();</span>
    <span class="n">a</span><span class="p">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">n</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">seconds</span><span class="p">(</span><span class="n">n</span><span class="p">));</span>
    <span class="n">a</span><span class="p">.</span><span class="n">dump</span><span class="p">();</span>
    <span class="n">A</span> <span class="n">aa</span><span class="p">;</span>
    <span class="n">aa</span><span class="p">.</span><span class="n">dump</span><span class="p">();</span> <span class="c1">// aa 和 a 中的count在同一个线程内相同。</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">td</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">td</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">td</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">join</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>23456242050624:1
thread::id of a non-executing thread:1
23456239949376:2
thread::id of a non-executing thread:2
</code></pre></div></div>]]></content><author><name></name></author><category term="cpp"/><category term="cpp"/><summary type="html"><![CDATA[thread-local]]></summary></entry><entry><title type="html">micrograd</title><link href="https://bilyz98.github.io/blog/2024/micrograd/" rel="alternate" type="text/html" title="micrograd"/><published>2024-08-05T11:59:00+00:00</published><updated>2024-08-05T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/micrograd</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/micrograd/"><![CDATA[<p>What is micrograd?</p> <p>micrograd is a simple implementation of pytorch-like autograd engine built by karparthy.</p> <h2 id="test-code">Test code</h2> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">micrograd.engine</span> <span class="kn">import</span> <span class="n">Value</span>

<span class="n">a</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">data</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">c</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">b</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>The output is quite straightforward:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>12.0000
3.0000
4.0000
1.0000
</code></pre></div></div> <p>However, the grad will accumulate in the value if it’s not set to zero.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">micrograd.engine</span> <span class="kn">import</span> <span class="n">Value</span>

<span class="n">a</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">data</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">c</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">b</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>


<span class="c1"># a = Value(4.0)
# b = Value(3.0)
</span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">d</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">b</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>


</code></pre></div></div> <p>OUtput:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>12.0000
3.0000
4.0000
1.0000
4.0000
5.0000
</code></pre></div></div> <h3 id="internal-implementation">Internal implementation</h3> <p>Please check code in this notebook <a href="https://colab.research.google.com/drive/1KF6houJ-X_uLIgZ5BaSV24-GnYpTQdeh?usp=sharing">micrograd note book</a></p> <p>Some vector based tensor implementation based on micrograd <a href="https://github.com/rohit-krish/Deeplex/tree/main">deeplex</a> <a href="https://github.com/conscell/ugrad">ugrad</a></p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="pytorch"/><summary type="html"><![CDATA[What is micrograd?]]></summary></entry><entry><title type="html">python dataframe drop row</title><link href="https://bilyz98.github.io/blog/2024/df-drop-row/" rel="alternate" type="text/html" title="python dataframe drop row"/><published>2024-08-04T11:59:00+00:00</published><updated>2024-08-04T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/df-drop-row</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/df-drop-row/"><![CDATA[<p>To drop rows from a Pandas DataFrame where a key only appears once, you can use the <code class="language-plaintext highlighter-rouge">groupby</code> and <code class="language-plaintext highlighter-rouge">transform</code> methods. Here’s a simple example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Sample DataFrame
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">ID</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">Value</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">60</span><span class="p">]}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Drop rows where 'ID' appears only once
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">ID</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">ID</span><span class="sh">'</span><span class="p">].</span><span class="nf">transform</span><span class="p">(</span><span class="sh">'</span><span class="s">count</span><span class="sh">'</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div> <p>This code will keep only the rows where the ‘ID’ appears more than once¹².</p> <p>Would you like more details or help with a different aspect of your DataFrame?</p>]]></content><author><name></name></author><category term="python"/><category term="python"/><summary type="html"><![CDATA[To drop rows from a Pandas DataFrame where a key only appears once, you can use the groupby and transform methods. Here’s a simple example:]]></summary></entry><entry><title type="html">Git merge file from another branch</title><link href="https://bilyz98.github.io/blog/2024/git-merge-file-from-another-branch/" rel="alternate" type="text/html" title="Git merge file from another branch"/><published>2024-07-29T11:59:00+00:00</published><updated>2024-07-29T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/git-merge-file-from-another-branch</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/git-merge-file-from-another-branch/"><![CDATA[<ol> <li> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout --patch
</code></pre></div> </div> </li> </ol> <p>Use case: when you want to check out specific part in a file.</p> <p>2. Direct checkout Use case: replace entire file from another branch</p> <ol> <li> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git cherry-pick &lt;commit-hash&gt;
</code></pre></div> </div> </li> </ol> <p>Use case: apply enire commit from another branch to your current branch.</p>]]></content><author><name></name></author><category term="git"/><category term="git"/><summary type="html"><![CDATA[git checkout --patch]]></summary></entry><entry><title type="html">Efficiency tips</title><link href="https://bilyz98.github.io/blog/2024/efficiency-tips/" rel="alternate" type="text/html" title="Efficiency tips"/><published>2024-07-19T11:59:00+00:00</published><updated>2024-07-19T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/efficiency-tips</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/efficiency-tips/"><![CDATA[<ul> <li>Do important and urgent things first.</li> <li>Try to avoid urgent and important thing, delegate them if possible.</li> <li>Motivation succeds action , not precedes it. Start doing asap.</li> <li>Try 5mins rule. Just try new things with 5 mins. This helps to ease the mind burden</li> <li>Make decision in advance. <ul> <li>Think about all postive and negative outcomes and come up with solutions.</li> </ul> </li> <li>Difference between research and enginnering <ul> <li>Research is about fail fast and and find the right problem and solution</li> <li>Engineering is about doing execution.</li> </ul> </li> </ul> <p><a href="https://www.youtube.com/live/b33vqX74EcA?si=fC-vlyOsjak_EXtH">https://www.youtube.com/live/b33vqX74EcA?si=fC-vlyOsjak_EXtH</a></p>]]></content><author><name></name></author><category term="tips"/><category term="efficiency"/><category term="tip"/><summary type="html"><![CDATA[Do important and urgent things first. Try to avoid urgent and important thing, delegate them if possible. Motivation succeds action , not precedes it. Start doing asap. Try 5mins rule. Just try new things with 5 mins. This helps to ease the mind burden Make decision in advance. Think about all postive and negative outcomes and come up with solutions. Difference between research and enginnering Research is about fail fast and and find the right problem and solution Engineering is about doing execution.]]></summary></entry><entry><title type="html">Speed up matrix multiplication</title><link href="https://bilyz98.github.io/blog/2024/matrix-multiplication/" rel="alternate" type="text/html" title="Speed up matrix multiplication"/><published>2024-07-12T11:59:00+00:00</published><updated>2024-07-12T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/matrix-multiplication</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/matrix-multiplication/"><![CDATA[<p>https://gist.github.com/chris124567/c45d46fdf4d922389641cc9f591ae577</p> <h3 id="naive-matrix-multiplication">Naive matrix multiplication</h3> <p>Code</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">matmul_byhand</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">weight</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">input_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">input_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">input_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_col_idx</span> <span class="o">=</span><span class="mi">0</span> <span class="p">;</span> <span class="n">output_col_idx</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">output_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">m_idx</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">m_idx</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">m_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">input_idx</span> <span class="o">=</span> <span class="n">input_row_idx</span> <span class="o">*</span> <span class="n">M</span> <span class="o">+</span> <span class="n">m_idx</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">weight_idx</span> <span class="o">=</span> <span class="n">m_idx</span> <span class="o">*</span>  <span class="n">K</span>  <span class="o">+</span> <span class="n">output_col_idx</span><span class="p">;</span>
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">input</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">weight</span><span class="p">[</span><span class="n">weight_idx</span><span class="p">];</span>
      <span class="p">}</span>

      <span class="kt">int</span> <span class="n">out_idx</span> <span class="o">=</span> <span class="n">input_row_idx</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">output_col_idx</span><span class="p">;</span>
      <span class="n">out</span><span class="p">[</span><span class="n">out_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
	<span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">M</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">K</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span> <span class="o">*</span> <span class="n">M</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">M</span> <span class="o">*</span> <span class="n">K</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span> <span class="o">*</span> <span class="n">K</span><span class="p">];</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">*</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">"%f "</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">j</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="n">matmul_byhand</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">"%f "</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">j</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span>
</code></pre></div></div> <p>Explanation: The most outer loop iterates over the rows of the input matrix.</p> <p>The second outer loop iterates over the columns of the output matrix. The inner loop iterates over the columns of the input matrix and the rows of the weight matrix. The inner loop calculates the dot product of the input row and the weight column and stores the result in the output matrix.</p> <p>Please note that how index of input, weight and output matrix are calculated. <img src="https://github.com/user-attachments/assets/dc21b2cc-20f6-4720-8fbb-5adc7855d4f0" alt="matmul3 drawio"/></p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>20.000000 23.000000 26.000000 29.000000
56.000000 68.000000 80.000000 92.000000
92.000000 113.000000 134.000000 155.000000
</code></pre></div></div> <p>Issue: Failed to install cuda 12.1.0</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano-gpt) [nsccgz_qylin_1@ln101%tianhe2-K matmul]$ conda install nvidia/label/cuda-12.1.0::cuda-toolkit
Channels:
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
 - defaults
 - nvidia/label/cuda-12.1.0
 - conda-forge
 - nvidia
 - pytorch
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: failed

InvalidSpec: The package "nvidia/linux-64::cuda-compiler==12.5.1=0" is not available for the specified platform
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-y</span> <span class="nt">-c</span> nvidia <span class="nv">cuda</span><span class="o">=</span>12.1.0 cuda-tools<span class="o">=</span>12.1.0 cuda-toolkit<span class="o">=</span>12.1.0 cuda-version<span class="o">=</span>12.1 cuda-command-line-tools<span class="o">=</span>12.1.0 cuda-compiler<span class="o">=</span>12.1.0 cuda-runtime<span class="o">=</span>12.1.0
</code></pre></div></div>]]></content><author><name></name></author><category term="ml"/><category term="al"/><category term="ml"/><summary type="html"><![CDATA[https://gist.github.com/chris124567/c45d46fdf4d922389641cc9f591ae577]]></summary></entry><entry><title type="html">How to write research paper</title><link href="https://bilyz98.github.io/blog/2024/how-to-write-research-paper/" rel="alternate" type="text/html" title="How to write research paper"/><published>2024-07-07T11:59:00+00:00</published><updated>2024-07-07T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/how-to-write-research-paper</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/how-to-write-research-paper/"><![CDATA[<p>I first found this slide from a wechat offical account.</p> <p>Then I found the original link through google. <a href="https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/">https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/</a></p> <p>What I learn that refreshes my mind is that this talk mentions that</p> <p>one should do this</p> <p>Coming up with research idea -&gt; Writing paper -&gt; Doing research work</p> <p>instead of this</p> <p>Coming up with research idea -&gt; Doing research work -&gt; Writing paper</p> <p>I felt mind blowing when I first read this.</p> <p>This make so much sense.</p> <p>The talk mentions that an idea is meaningless if you just keep it to yourself.</p> <p>So what you should do is to write them down and spread your idea to other people as early as possbile.</p>]]></content><author><name></name></author><category term="research"/><category term="research"/><summary type="html"><![CDATA[How to write research paper]]></summary></entry><entry><title type="html">Fast nano-gpt training</title><link href="https://bilyz98.github.io/blog/2024/gpt-fast/" rel="alternate" type="text/html" title="Fast nano-gpt training"/><published>2024-07-03T11:59:00+00:00</published><updated>2024-07-03T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/gpt-fast</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/gpt-fast/"><![CDATA[<p>In this blog I will document my code and experiment results following karparthy’s latest gpt-2 training tutorial video.</p> <iframe width="560" height="315" src="https://www.youtube.com/embed/l8pRSuU81PU?si=2sEmtmn56XBMPTbU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>[https://youtu.be/l8pRSuU81PU?si=2sEmtmn56XBMPTbU][https://youtu.be/l8pRSuU81PU?si=2sEmtmn56XBMPTbU]</p> <p>Code repo url: <a href="https://github.com/BilyZ98/nano-gpt">https://github.com/BilyZ98/nano-gpt</a></p> <p>What is TFLOPs ? Tera floating point operations per second.</p> <h3 id="weight-sharing">Weight sharing</h3> <p>Share weight of <code class="language-plaintext highlighter-rouge">lm_head</code> and <code class="language-plaintext highlighter-rouge">weight_token_embedding</code> This weight share helps to save memory. This is huge amount of memory. It’s <code class="language-plaintext highlighter-rouge">n_embed * vocab_size</code></p> <h3 id="lower-precision">Lower precision</h3> <p>Default tensor precision:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model device cuda:0
m device cuda:0
torch.float32
</code></pre></div></div> <p>Without TF32: Memory usage:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 1.0s: nvidia-smi                                                                                                               Sat Jul  6 16:51:57 2024

Sat Jul  6 16:51:57 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   71C    P0   309W / 300W |   5359MiB / 81920MiB |    100%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   29C    P0    46W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1116, val loss 1.1166
step 4500, loss: 1.1580688953399658, dt: 113.35ms, tok/sec: 144546.60
Time taken: 739.9126691818237 seconds
Total parameters: 10921049
Trainable parameters: 10921049
        ex is consists of centuries and want-wetlife science method ankners, from the ways wheredco by Benrys, where you're rate ar harm browling preservation in musicians. In Athletes have continuies to munically, and effects create cro-intricate jaz towantical navigating vail respectives to diseariety.

**Befor Hera's Players**

Prannit**
Infaming the game's early which commercial quirtual health, and mobile exammatring sound in football in pain this visualization involvement, is home to shapel key th

</code></pre></div></div> <p>TF32: Memory usage:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 0.1s: nvidia-smi                                                                                                                          Sat Jul  6 17:11:38 2024

Sat Jul  6 17:11:38 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   64C    P0   309W / 300W |   5359MiB / 81920MiB |     99%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   31C    P0    47W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 4998, loss: 1.1807901859283447, dt: 30.66ms, tok/sec: 534324.52
step 4999, loss: 1.1486560106277466, dt: 30.19ms, tok/sec: 542632.14
Time taken: 315.7020351886749 seconds
Total parameters: 10921049
Trainable parameters: 10921049


* Nuriety pattern to Play blogge: The misssiol strategies and transfolk to wo halley focused our work's own forecative ar unique planning. This articiples in the Laine Golden

The Host Potential's top init's experienterment and its particular type and regional warveillance. In this article, I'll shedd a fascinat words contributies and other athlete's early what draw from genre, col up jands, lim body eparthory, there ancient work of pairitization criticallysis.

**Addition Enthusiast Players**
~
</code></pre></div></div> <p>Memory usage does not change much. But the training speed is much faster.</p> <p>With BF16 Memory usage</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 0.1s: nvidia-smi                                                                                                                          Sat Jul  6 17:19:50 2024

Sat Jul  6 17:19:50 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   61C    P0   280W / 300W |   5493MiB / 81920MiB |     98%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   32C    P0    47W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1083, val loss 1.1138
step 4500, loss: 1.149060606956482, dt: 46.36ms, tok/sec: 353409.81
Time taken: 323.1115171909332 seconds
Total parameters: 10921049
Trainable parameters: 10921049


* Thre-known variety is Mobile time forishing and a Piece**

Lerious la trend ammach focusion solus, the ways can date on how theselves presents, and cleaning ung time. While the Long-Anti Central to mitigate efforts, referred time

1. The Ritowastic Qrench Literian ass make-had a numberous unfounded women's explore a court of texture or the engage's earching try of from generating secrets. This benefit for a deterrmining movie excebrations, which its form its applyestriants. By regulating the

</code></pre></div></div> <p>It’s not faster. Why is that?</p> <h3 id="-trochcompile">+ troch.compile()</h3> <p>Got this error duing runtime. So I downgrade the python version to 3.11</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 236, in &lt;module&gt;
    model = torch.compile(model)
            ^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.12/site-packages/torch/__init__.py", line 1868, in compile
    raise RuntimeError("Dynamo is not supported on Python 3.12+")
RuntimeError: Dynamo is not supported on Python 3.12+
</code></pre></div></div> <p>Got another error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/tmp/tmpd5t0oroc/main.c: In function ‘list_to_cuuint64_array’:
/tmp/tmpd5t0oroc/main.c:354:3: error: ‘for’ loop initial declarations are only allowed in C99 mode
   for (Py_ssize_t i = 0; i &lt; len; i++) {
   ^
/tmp/tmpd5t0oroc/main.c:354:3: note: use option -std=c99 or -std=gnu99 to compile your code
/tmp/tmpd5t0oroc/main.c: In function ‘list_to_cuuint32_array’:
/tmp/tmpd5t0oroc/main.c:365:3: error: ‘for’ loop initial declarations are only allowed in C99 mode
   for (Py_ssize_t i = 0; i &lt; len; i++) {
   ^
/tmp/tmp0td94o5b/main.c: In function ‘list_to_cuuint64_array’:
/tmp/tmp0td94o5b/main.c:354:3: error: ‘for’ loop initial declarations are only allowed in C99 mode
   for (Py_ssize_t i = 0; i &lt; len; i++) {
   ^
</code></pre></div></div> <p>I install latest gcc version . Old version is 4.8.5 Fix error above.</p> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1037, val loss 1.1087
step 4500, loss: 1.151484727859497, dt: 23.18ms, tok/sec: 706728.75
Time taken: 285.365624666214 seconds
Total parameters: 10921049
Trainable parameters: 10921049
         emphasizing the parming of a spirit on the development conouches tended from the factor, such as jailust power traffice, and ixo, these can serve the alternatively takes part in processed.

**4. Inform 4applications Movements**

1. **Mush Secrets:** Students in Parleers (airl Warrioring change: Bhern Keshedmn) world, as initial do other basebar from leagues, to employ create messaters and their immpact on our camido opportunities. Its gain political windows, proteins and resoling affian languag
~
</code></pre></div></div> <h3 id="-flash-attention">+ Flash attention</h3> <p>Karpathy gives a brief introduction to flash attention in his video. Check out this video to know more . <a href="https://youtu.be/l8pRSuU81PU?t=7521">https://youtu.be/l8pRSuU81PU?t=7521</a></p> <p>Code:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">One head of self-attention</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">tril</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>
        <span class="c1"># wei = q @ k.transpose(-2, -1) * C **-0.5
</span>        <span class="c1"># wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)
</span>        <span class="c1"># wei = F.softmax(wei, dim=-1) # (B, T, T)
</span>        <span class="c1"># wei = self.dropout(wei)
</span>        <span class="c1"># out = wei @ v #(B,T,T) @ ( B, T, C) -&gt; (B, T, C)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

</code></pre></div></div> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.3369, val loss 1.3376
step 4500, loss: 1.3740415573120117, dt: 22.38ms, tok/sec: 731945.94
Time taken: 288.25680232048035 seconds
Total parameters: 10921049
Trainable parameters: 10921049
        facilitation slowing technique it.

In conclusion, is jilitary Ketins and scale, coreting the programphs of cocentries, many only soidhts:

1. **Ingluentalle performances**: Epchilitations and dinactions has functively cardolles experience witting, significantlyqtually.
3. **Pobitice, SStédio micrositiona Ristice**: The Presed full fame leintarians foreices, day exemisions and community to provide a more endaging of his play byautices. The New clear Ara Case gor as mains, Aheragement textures th
</code></pre></div></div> <p>Throughput increases 4\% compared to the previous version which is not a lot. I guess this is because the model is not big enough to benefit from the flash attention.</p> <h3 id="use-distributed-data-parallelddp">Use distributed data parallel(DDP)</h3> <p>Issue: I can only run with 4 GPUs. If I run with 8 GPUs, I got this error when I try to use all 8 gpus:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ddp_world_size 8
[rank4]: Traceback (most recent call last):
[rank4]:   File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 48, in &lt;module&gt;
[rank4]:     torch.cuda.set_device(device)
[rank4]:   File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/cuda/__init__.py", line 399, in set_device
[rank4]:     torch._C._cuda_setDevice(device)
[rank4]: RuntimeError: CUDA error: invalid device ordinal
[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

E0707 18:10:42.318000 47618031484288 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 4 (pid: 52464) of binary: /GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/bin/python                                                                                         Traceback (most recent call last):                                                                                                                    File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/bin/torchrun", line 33, in &lt;module&gt;                                                              sys.exit(load_entry_point('torch==2.3.1', 'console_scripts', 'torchrun')())                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                        File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:

Failures:
[1]:
  time      : 2024-07-07_18:10:42
  host      : gpu72
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 52465)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-07-07_18:10:42
  host      : gpu72
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 52466)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html                                                          [3]:
  time      : 2024-07-07_18:10:42
  host      : gpu72
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 52467)
  error_file: &lt;N/A&gt;                                                                                                                                   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------                                                                                        Root Cause (first observed failure):                                                                                                                [0]:
  time      : 2024-07-07_18:10:42                                                                                                                     host      : gpu72
  rank      : 4 (local_rank: 4)                                                                                                                       exitcode  : 1 (pid: 52464)                                                                                                                          error_file: &lt;N/A&gt;                                                                                                                                   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html                                                          ============================================================

</code></pre></div></div> <p>Actuall my task is assigned only 4 gpus even thought there are 8 gpus on the compute node. I run following code to get number of available cuda device I can use and it outputs 4.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda device count</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">())</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(nano-gpt) [nsccgz_qylin_1@ln102%tianhe2-K gpt-dev]$ yhrun -n 1 -N 1 -p GPU_A800 python test_gpt_count.py
cuda device count 4
</code></pre></div></div> <p>Asked bing and it gives a post that I can set visible cuda devices to all 8 gpus. And then I check this env variable and found that it only outputs <code class="language-plaintext highlighter-rouge">0,1,2,3</code>.</p> <p>So this is the reason why I can not use all 8 gpus.</p> <p>Problem fixed.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span>python test_gpt_count.py
cuda device count 4
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span>
0,1,2,3
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span>^C
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span><span class="nb">export </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="k">${</span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="k">}</span>,4,5,6,7
</code></pre></div></div> <p>Rewrite dataloader.</p> <p>Since now multiple processes are introduced to use multiple gpus, I need to rewrite the dataloader to make it work with multiple processes.</p> <p>Code: <code class="language-plaintext highlighter-rouge">self.process_rank</code> means current process rank. <code class="language-plaintext highlighter-rouge">self.num_process</code> means total number of processes.</p> <p><code class="language-plaintext highlighter-rouge">self.current_idx</code> is the current index of the data that the dataloader is reading. Note that each process will have it own data to train which is different from other processes. So <code class="language-plaintext highlighter-rouge">self.current_idx</code> moves forward by <code class="language-plaintext highlighter-rouge">B * T * self.num_process</code> each time.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DataLoader</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span>  <span class="n">process_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_process</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span>
        <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="n">self</span><span class="p">.</span><span class="n">process_rank</span> <span class="o">=</span> <span class="n">process_rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_process</span> <span class="o">=</span> <span class="n">num_process</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">process_rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">next_batch</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">current_idx</span><span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">buf</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">buf</span><span class="p">[</span><span class="mi">1</span><span class="p">:]).</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">+=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">num_process</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">num_process</span> <span class="o">+</span> <span class="mi">1</span><span class="o">&gt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">):</span>
            <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">process_rank</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>


</code></pre></div></div> <p>It takes twice longer time to finish the job after swtiching to new data loader which is weird. I don’t know why. Is this because of cache miss?</p> <p>I think it’s because of the cache miss. Token throughput after switching to new dataloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim  slurm-2769516.out
m device cuda:0
step &lt;built-in function iter&gt;: train loss4.6247, val loss 4.6222
step 0, loss: 6.304970741271973, dt: 253517.79ms, tok/sec: 64.63
step &lt;built-in function iter&gt;: train loss2.7204, val loss 2.7159
step 500, loss: 2.7232675552368164, dt: 22.78ms, tok/sec: 719357.23
step &lt;built-in function iter&gt;: train loss2.6621, val loss 2.6616
</code></pre></div></div> <p>Token throughput before switching to new dataloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim  slurm-2769041.out
step &lt;built-in function iter&gt;: train loss4.6102, val loss 4.6073
step 0, loss: 6.303163528442383, dt: 105310.21ms, tok/sec: 155.58
step &lt;built-in function iter&gt;: train loss2.6170, val loss 2.6169
step 500, loss: 2.6428515911102295, dt: 22.64ms, tok/sec: 723576.18
</code></pre></div></div> <p>It takes even longer after I move <code class="language-plaintext highlighter-rouge">to(device)</code> out of dataloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
tep &lt;built-in function iter&gt;: train loss2.2721, val loss 2.2711
step 4000, loss: 2.2670536041259766, dt: 22.41ms, tok/sec: 730972.72
step &lt;built-in function iter&gt;: train loss1.9989, val loss 1.9980
step 4500, loss: 2.078113079071045, dt: 22.69ms, tok/sec: 722123.90
Time taken: 651.469042301178 seconds
Total parameters: 10921049
Trainable parameters: 10921049                                                                                                                               the melorme slocites apctature to bayysic impessare to to obaytening ridence's, comuties the prayer'res of conngibuidiktle tooly soudiestinabo, creation, lov expertance entifuchitule, stluch pronuctingmat arous on vismok dailioout ateries tipl,, aphige sthatection of ecpppppivaclliency, powanct transps, and the owen, freper. Coulw fame inidtifinge for falstadry exertwing to socinigys the pecos, reame peound to its daman lawabyang ammbots'res coltual ra casergod asem, yucial gor mann textions mu
</code></pre></div></div> <h4 id="4-gpus-for-ddp">4 GPUs for ddp</h4> <p>Memory usage</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) [nsccgz_qylin_1@gpu72%tianhe2-K ~]$ nvidia-smi
Wed Jul 10 17:18:59 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   45C    P0   145W / 300W |   3975MiB / 81920MiB |     44%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   48C    P0   133W / 300W |   3979MiB / 81920MiB |     45%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  On   | 00000000:53:00.0 Off |                    0 |
| N/A   47C    P0   160W / 300W |   3979MiB / 81920MiB |     46%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  On   | 00000000:57:00.0 Off |                    0 |
| N/A   49C    P0   134W / 300W |   3959MiB / 81920MiB |     44%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 1.0s: nvidia-smi                                                                                                                                                                    Wed Jul 10 17:37:07 2024

Wed Jul 10 17:37:07 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   32C    P0    67W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   35C    P0    69W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  On   | 00000000:53:00.0 Off |                    0 |
| N/A   34C    P0    66W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  On   | 00000000:57:00.0 Off |                    0 |
| N/A   36C    P0    69W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Memory usage gradually increases. Why is this?</p> <p>Issue: Getting error that nccl report heartbeat error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[rank0]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 0] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=1
[rank0]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 0] ProcessGroupNCCL preparing to dump debug info.
[rank0]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 0] [PG 0 Rank 0] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 1
E0710 20:29:48.177000 47209663704448 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: -6) local_rank: 0 (pid: 1882) of binary: /GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/bin/python
Traceback (most recent call last):
</code></pre></div></div> <p>I think this is because I did not add this line of code</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):
</code></pre></div></div> <p>This code does not fix the problem.</p> <p>I am now trying to first call <code class="language-plaintext highlighter-rouge">destroy_process_group</code> and then calling tokens generation to fix issue above.</p> <p>I should decrease count of training iteration so that I can get error as early as possible.</p> <p>It takes 16 mins to finish model training which is not normal. I wonder why does it take so long after switching to new dataloader.</p> <p>Can this be a research problem?</p> <p>Why does it take so long to generate tokens after training is finished?.</p> <p>Is this because of decode part or is this because of the model inference part?</p> <p>Issue: Get this error saying that expected all tensors to be on the same device.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 292, in generate_tokens
    logits, loss = model(idx_cond) # (B,T,vocab_size)
                   ^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 255, in forward
    tok_emb = self.token_embedding_table(idx) #(B,T,C)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/functional.py", line 2264, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
</code></pre></div></div> <p>Don’t know which tensor is not on cuda. So I print device of <code class="language-plaintext highlighter-rouge">idx</code> to see if it’s on cuda.</p> <p>Issue above is sovled after I update the code like this</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span> <span class="p">)</span>
</code></pre></div></div> <p>cuda:0 do use lots of memory while other gpus have freed up memory.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   36C    P0    73W / 300W |   4867MiB / 81920MiB |    100%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   32C    P0    47W / 300W |     27MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  On   | 00000000:53:00.0 Off |                    0 |
| N/A   32C    P0    44W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  On   | 00000000:57:00.0 Off |                    0 |
| N/A   34C    P0    47W / 300W |      7MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Still can not generate tokens.</p> <p>I think this is because that I did not call model forward for all processes.</p> <p>The code runs successfully after I call model forward for all processes for generating text.</p> <p>It does not work even I set <code class="language-plaintext highlighter-rouge">model.eval()</code> only for master process. Why is that?</p> <p>Here’s the code that can finish successfully without process hanging</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_iters</span><span class="p">):</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="c1"># xb, yb = get_batch('train')
</span>    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">data_loader</span><span class="p">.</span><span class="nf">next_batch</span><span class="p">()</span>
    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">xb</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">yb</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">xb</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># with torch.autocast(device_type=device_type, dtype=torch.bfloat16):
</span>    <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

    <span class="c1"># if ddp:
</span>    <span class="c1">#     model.require_backward_grad_sync = True
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">ddp</span><span class="p">:</span>
        <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">AVG</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span> <span class="c1"># wait for the GPU to finish work
</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">token_processed</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">ddp_world_size</span>
    <span class="c1">#if step % eval_interval == 0 :
</span>    <span class="c1">#    losses = estimate_loss()
</span>    <span class="c1">#    print(f"step {iter}: train loss{losses['train']:.4f}, val loss {losses['val']:.4f}")
</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="p">(</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span> <span class="c1"># milli sec
</span>    <span class="n">token_per_sec</span> <span class="o">=</span> <span class="n">token_processed</span><span class="o">/</span> <span class="p">(</span><span class="n">t1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span>
    <span class="nf">call_generate_tokens</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>                                                                                                  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, dt: </span><span class="si">{</span><span class="n">dt</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">ms, tok/sec: </span><span class="si">{</span><span class="n">token_per_sec</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">master_process</span><span class="p">:</span>
</code></pre></div></div> <p>It looks like from this discuss post that I have to call forward and backwad for all processes. <a href="https://discuss.pytorch.org/t/multiple-forward-functions-in-dp-and-ddp/135029/5">https://discuss.pytorch.org/t/multiple-forward-functions-in-dp-and-ddp/135029/5</a></p> <p>No DDP with 1 gpu + new data loader + tf32, no torch.compile</p> <p>It’s pretty fast though.</p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 4998, loss: 1.5543745756149292, dt: 44.11ms, tok/sec: 371476.71
idx device: cuda:0
rank: 0,         broving and mited rocal hand alchiqe, Ond Wastestern have alter numpre, and society.

**Conclusion**

When 103, hith htreate analyzing into talent of the nets encerlated indituted to degensive sports and projote About to  trace producial stratege. Os a care a coved legelad interacting harswe her in the intricacies, with history and winnlowedge in 140s has bhe Bavill –as cit) Éanta.

**Upproach Inper To Adaptation**

The commmal tOxerium boodls to dimerstry of pinemanical the industry to interes
step 4999, loss: 1.5449110269546509, dt: 44.11ms, tok/sec: 371424.51
Time taken: 301.4059376716614 seconds
Total parameters: 10921049
Trainable parameters: 10921049
</code></pre></div></div> <p>DDP with 4 gpus , no torch.compile, tf32</p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 4998, loss: 1.2392717599868774, dt: 85.30ms, tok/sec: 768330.46
idx device:idx device:idx device:idx device:    cuda:3cuda:0cuda:1cuda:2



rank: 1,         Choroes, and formats. In this a
rank: 3,         Choroes, and formats. In this a
rank: 2,         Choroes, and formats. In this a
rank: 0,         Choroes, and formats. In this a
step 4999, loss: 1.2737213373184204, dt: 86.10ms, tok/sec: 761164.76
Time taken: 442.67850971221924 seconds
Total parameters: 10921049
Trainable parameters: 10921049
</code></pre></div></div> <p>I don’t know how FSDP( Fully sharded data parallel ) works yet.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="llm"/><summary type="html"><![CDATA[llm]]></summary></entry></feed>