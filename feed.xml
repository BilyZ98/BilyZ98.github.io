<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bilyz98.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bilyz98.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-02T08:30:18+00:00</updated><id>https://bilyz98.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">chibicc C compiler - parser review and expression evaluator</title><link href="https://bilyz98.github.io/blog/2025/chibicc-parser-review-and-calculator/" rel="alternate" type="text/html" title="chibicc C compiler - parser review and expression evaluator"/><published>2025-01-01T11:59:00+00:00</published><updated>2025-01-01T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/chibicc-parser-review-and-calculator</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/chibicc-parser-review-and-calculator/"><![CDATA[<h2 id="lea-and-mov-explanation">lea and mov explanation</h2> <p><code class="language-plaintext highlighter-rouge">lea -4(%rbp), %rax</code> means that memory address that is 4 bytes below current base pointer <code class="language-plaintext highlighter-rouge">rbp</code> is stored in <code class="language-plaintext highlighter-rouge">rax</code> register.</p> <p>For example,</p> <p>assume <code class="language-plaintext highlighter-rouge">%rbp</code> holds value <code class="language-plaintext highlighter-rouge">0x7fffffffddd0</code> offset is 8,</p> <p>then this code</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lea -8(%rbp), %rax
</code></pre></div></div> <p>means</p> <ol> <li>the effective address calculation is <code class="language-plaintext highlighter-rouge">0x7fffffffddd0 -8 = 0x7fffffffdd8</code></li> <li>The value <code class="language-plaintext highlighter-rouge">0x7fffffffdd8</code> is stored in <code class="language-plaintext highlighter-rouge">rax</code> register</li> </ol> <p>I get confused about whether <code class="language-plaintext highlighter-rouge">mov src, dest</code> or <code class="language-plaintext highlighter-rouge">mov dest, src</code> is correct.</p> <p>I learn from this <a href="https://stackoverflow.com/questions/5890724/mov-instruction-in-x86-assembly">stack overflow post about mov in x86 and AT&amp;T</a> that both are valid.</p> <p><code class="language-plaintext highlighter-rouge">mov src, dest</code> is correct in AT&amp;T and <code class="language-plaintext highlighter-rouge">mov dest, src</code> is valid in Intel syntax.</p> <p><a href="https://electronicsreference.com/assembly-language/mov/">Intel x86 mov explanation</a></p> <h3 id="gcc-uses-att-assembly-standard">gcc uses AT&amp;T assembly standard</h3> <p>GCC (GNU Compiler Collection) uses the AT&amp;T assembly syntax by default. This is the standard assembly syntax used in Unix-like systems. However, GCC also supports Intel syntax, and you can switch to it using specific compiler flags if needed.</p> <p>If you’re working on a project and need to use Intel syntax, you can enable it with the <code class="language-plaintext highlighter-rouge">-masm=intel</code> flag. For example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcc <span class="nt">-masm</span><span class="o">=</span>intel <span class="nt">-o</span> myprogram myprogram.c
</code></pre></div></div>]]></content><author><name></name></author><category term="compiler"/><category term="c"/><category term="compiler"/><summary type="html"><![CDATA[lea and mov explanation]]></summary></entry><entry><title type="html">Autodiff implementation - kernel and memory management</title><link href="https://bilyz98.github.io/blog/2024/uw-sysml-assign2/" rel="alternate" type="text/html" title="Autodiff implementation - kernel and memory management"/><published>2024-12-24T11:59:00+00:00</published><updated>2024-12-24T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/uw-sysml-assign2</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/uw-sysml-assign2/"><![CDATA[<p>This is fun to implement</p> <p>Never aware that we can do management in python language like this.</p> <p>I always think that memory management in only done in cpp.</p> <p>Interesting, so interesting.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def memory_plan(self, feed_shapes):
        """Allocates tvm.nd.array for every node except feed_dict nodes.

        Implementation note:
        Easy Option: Alloc a tvm.nd.array per node that persists across run()

        Use self.node_to_arr_map to store node-&gt;tvm.nd.array mapping
        to allow mapping to persist across multiple executor.run().

        Parameters
        ----------
        feed_shapes: node-&gt;shapes mapping for feed_dict nodes.
        """
        """TODO: Your code here"""


</code></pre></div></div> <h2 id="what-is-placeholder-in-tvm-">What is placeholder in tvm ?</h2> <p>In TVM, a <code class="language-plaintext highlighter-rouge">placeholder</code> is used to define an input tensor for a computation. It essentially acts as a symbolic representation of the input data, specifying the shape and optionally the data type. This is useful for defining the structure of the computation graph before actual data is provided.</p> <p>Here’s a simple example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="c1"># Define the shape of the input tensor
</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="sh">"</span><span class="s">float32</span><span class="sh">"</span>

<span class="c1"># Create a placeholder tensor
</span><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Print the placeholder tensor
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Placeholder tensor A:</span><span class="sh">"</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</code></pre></div></div> <p>In this example:</p> <ul> <li><code class="language-plaintext highlighter-rouge">te.placeholder</code> creates a tensor <code class="language-plaintext highlighter-rouge">A</code> with the specified shape <code class="language-plaintext highlighter-rouge">(3, 4)</code> and data type <code class="language-plaintext highlighter-rouge">"float32"</code>.</li> <li>This tensor can then be used in further computations, such as defining operations that use <code class="language-plaintext highlighter-rouge">A</code> as input.</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">placeholder</code> is a fundamental concept in TVM’s Tensor Expression (TE) language, allowing you to build complex computation graphs by defining how data flows through various operations¹².</p> <p>¹: <a href="https://tvm.apache.org/docs/reference/api/python/te.html">TVM Documentation</a></p> <p>²: <a href="https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html">TVM Tutorial on Tensor Expressions</a></p> <p>(1) tvm.te — tvm 0.19.dev0 documentation - The Apache Software Foundation. https://tvm.apache.org/docs/reference/api/python/te.html.</p> <p>(2) TVM 自底向上（三）：TE 的概念和编译原理 - 知乎. https://zhuanlan.zhihu.com/p/534313816.</p> <p>(3) Working with Operators Using Tensor Expression — tvm 0.19.dev0 …. https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html.</p> <p>(4) TVM学习（六）细读前端 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/346514871.</p> <p>(5) External Tensor Functions — tvm 0.19.dev0 documentation. https://tvm.apache.org/docs/how_to/work_with_schedules/extern_op.html.</p> <h2 id="tvm-create-ndarray-with-specified-shape">tvm create nd.array with specified shape</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="c1"># Specify the shape and data type
</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="sh">"</span><span class="s">float32</span><span class="sh">"</span>

<span class="c1"># Create an empty TVM NDArray with the specified shape
</span><span class="n">tvm_array</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Print the TVM NDArray
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TVM NDArray with specified shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tvm_array</span><span class="p">)</span>

</code></pre></div></div> <h2 id="difference-between-tvm-and-ncnn">Difference between tvm and ncnn</h2> <p>tvm is a open source deep learning compiler for cpus, gpus and specialized hardware.</p> <p>ncnn is a neural network inference framework optimized for mobile and embedded devices.</p> <p>We can assume input shape of mat_mul is 2d in this task.</p> <h2 id="matmul-kernel-for-unknown-input-dimension-shape">matmul kernel for unknown input dimension shape</h2> <p>This code is from gpt.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="k">def</span> <span class="nf">make_flexible_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="c1"># Determine the shapes of the input tensors
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">shapeA</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">shapeA</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">shapeA</span>
    
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">shapeB</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">shapeB</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">shapeB</span>
    
    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch</span> <span class="nf">else </span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch</span> <span class="nf">else </span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Create placeholders for the input tensors
</span>    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    
    <span class="c1"># Define the reduction axis
</span>    <span class="n">k_axis</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># Compute the matrix multiplication based on transpose flags and dimensionality
</span>    <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeA</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeA</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
    
    <span class="c1"># Create a schedule for the computation
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    
    <span class="c1"># Apply optimizations: split, reorder, vectorize, parallel
</span>    <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">bo</span><span class="p">,</span> <span class="n">bi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">io</span><span class="p">,</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">jo</span><span class="p">,</span> <span class="n">ji</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">bo</span><span class="p">,</span> <span class="n">io</span><span class="p">,</span> <span class="n">jo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">bi</span><span class="p">,</span> <span class="n">ii</span><span class="p">,</span> <span class="n">ji</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">bo</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">io</span><span class="p">,</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">jo</span><span class="p">,</span> <span class="n">ji</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">io</span><span class="p">,</span> <span class="n">jo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">ii</span><span class="p">,</span> <span class="n">ji</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">io</span><span class="p">)</span>
    
    <span class="c1"># Lower the schedule to generate the IR code
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">tvm</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    
    <span class="c1"># Build the function
</span>    <span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">func</span>

<span class="c1"># Example usage
</span><span class="n">tgt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">tgt_host</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">func_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">flexible_matrix_mul</span><span class="sh">"</span>
<span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># Batch size of 32
</span><span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>  <span class="c1"># Batch size of 32
</span><span class="n">transposeA</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">transposeB</span> <span class="o">=</span> <span class="bp">False</span>
<span class="nf">make_flexible_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">)</span>

</code></pre></div></div> <h2 id="conv2d-tvm-kernel">conv2d tvm kernel</h2> <p>Code is generated by gpt</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span><span class="p">,</span> <span class="n">topi</span>

<span class="k">def</span> <span class="nf">make_conv2d</span><span class="p">(</span><span class="n">shapeX</span><span class="p">,</span> <span class="n">shapeF</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">assert</span><span class="p">(</span><span class="n">shapeX</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeF</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">shapeX</span>
    <span class="n">M</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">shapeF</span>

    <span class="c1"># Create placeholders for the input tensor and filter
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">F</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># Define the reduction axes
</span>    <span class="n">rc</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rc</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">rr</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">R</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rr</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">rs</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rs</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Compute the convolution
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">H</span> <span class="o">-</span> <span class="n">R</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">S</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
        <span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="n">rr</span><span class="p">,</span> <span class="n">w</span> <span class="o">+</span> <span class="n">rs</span><span class="p">]</span> <span class="o">*</span> <span class="n">F</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">]),</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span>
    <span class="p">)</span>

    <span class="c1"># Create a schedule for the computation
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">Y</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># Apply optimizations: split, reorder, vectorize, parallel
</span>    <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
    <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span>
    <span class="n">ho</span><span class="p">,</span> <span class="n">hi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">wo</span><span class="p">,</span> <span class="n">wi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">ho</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">hi</span><span class="p">,</span> <span class="n">wi</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">ho</span><span class="p">)</span>

    <span class="c1"># Lower the schedule to generate the IR code
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">tvm</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="c1"># Build the function
</span>    <span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">func</span>

<span class="c1"># Example usage
</span><span class="n">tgt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">tgt_host</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">func_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">conv2d</span><span class="sh">"</span>
<span class="n">shapeX</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>  <span class="c1"># Example input shape (N, C, H, W)
</span><span class="n">shapeF</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   <span class="c1"># Example filter shape (M, C, R, S)
</span><span class="nf">make_conv2d</span><span class="p">(</span><span class="n">shapeX</span><span class="p">,</span> <span class="n">shapeF</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">)</span>


</code></pre></div></div> <h2 id="install-tvm-by-building-from-source">Install tvm by building from source</h2> <p>Can not install tvm through pip. Have to download source code and build it myself. I don’t know why.</p> <p>Folow steps in this <a href="https://github.com/apache/tvm/issues/13507">issue</a> to compile locally.</p> <p>Need to disable gtest inroder to pass cmake</p> <p><a href="https://tvm.apache.org/docs/install/from_source.html#install-from-source">Offical install document</a></p> <p>Finally finish installing tvm after building locally.</p> <p>Run this command to verify</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>uwsyml<span class="o">)</span> ➜  tvm git:<span class="o">(</span>main<span class="o">)</span> python <span class="nt">-c</span> <span class="s2">"import tvm; print(tvm.__file__)"</span>
/mnt/nvme1n1/zt/tvm/python/tvm/__init__.py
</code></pre></div></div> <p>nosetests does not use python in conda. It uses that in /usr/bin which is not what I want.</p> <p>It reports error that it can not find numpy which I have already installed in conda environment</p> <h2 id="update-code-to-use-latest-function-in-tvm-instead-of-old-function-in-tvm">Update code to use latest function in tvm instead of old function in tvm</h2> <p>New code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="k">def</span> <span class="nf">test_matrix_elementwise_mul</span><span class="p">():</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">arr_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_y</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_z</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">elemwise_mul</span> <span class="o">=</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_mul</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_add</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">elemwise_mul</span><span class="p">(</span><span class="n">arr_x</span><span class="p">,</span> <span class="n">arr_y</span><span class="p">,</span> <span class="n">arr_z</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">arr_z</span><span class="p">.</span><span class="nf">asnumpy</span><span class="p">()</span>
    <span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>


    
<span class="k">def</span> <span class="nf">make_elemwise_mul</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <p>Old code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">test_matrix_elementwise_mul_by_const</span><span class="p">():</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">const_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">arr_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_y</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">elemwise_mul_by_const</span> <span class="o">=</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_mul_by_const</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">const_val</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_mul_by_const</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">elemwise_mul_by_const</span><span class="p">(</span><span class="n">arr_x</span><span class="p">,</span> <span class="n">arr_y</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">arr_y</span><span class="p">.</span><span class="nf">asnumpy</span><span class="p">()</span>
    <span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">const_val</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_elemwise_mul_by_const</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">const_k</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">const_k</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>
</code></pre></div></div> <h3 id="how-node-value-is-stored-in-autodiff-">How node value is stored in autodiff ?</h3> <p>It’s stored in dict <code class="language-plaintext highlighter-rouge">node_to_val_map</code></p> <p>The graph is just computation graph. Each node is an operation node.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">convert_to_numpy_ret_vals</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Parameters
        ----------
        feed_dict: a dictionary of node-&gt;np.ndarray supplied by user.
        convert_to_numpy_ret_vals: whether to convert ret vals to np.array.

        Returns
        -------
        A list of values for nodes in eval_node_list. tvm.nd.array or np.ndarray.
        </span><span class="sh">"""</span>
        <span class="k">def</span> <span class="nf">are_feed_shapes_equal</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="n">sb</span><span class="p">):</span>
            <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sb</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
                <span class="k">return</span> <span class="bp">False</span>
            <span class="n">unmatched_item</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">sa</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span> <span class="o">^</span> <span class="nf">set</span><span class="p">(</span><span class="n">sb</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
            <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">unmatched_item</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="n">node_to_val_map</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">feed_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">ndarray</span><span class="p">.</span><span class="n">NDArray</span><span class="p">),</span>\
                <span class="sh">"</span><span class="s">feed_dict value type not supported</span><span class="sh">"</span>    
            <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

</code></pre></div></div> <h3 id="how-node-computation-is-done-in-graph-">How node computation is done in graph ?</h3> <p>In run function , <code class="language-plaintext highlighter-rouge">compute</code> is called for each operation node.</p> <p>Each opeartion node in computation graph has its own compute function.</p> <p>For example , <code class="language-plaintext highlighter-rouge">AddOp</code> has its own compute function and this compute function calls passed-in compiled_func to do the function call from compiled code.</p> <p>Note that this <code class="language-plaintext highlighter-rouge">compiled_func</code> is built before forward of computation graph.</p> <p>And the return function of <code class="language-plaintext highlighter-rouge">make_elemwise_add</code> is a tvm build function that takes <code class="language-plaintext highlighter-rouge">[A,B,C]</code> three tensor as input instead of parameters in <code class="language-plaintext highlighter-rouge">compute</code> function.</p> <p><code class="language-plaintext highlighter-rouge">tgt</code> and <code class="language-plaintext highlighter-rouge">shape</code> is defined during function compilation in <code class="language-plaintext highlighter-rouge">make_elemwise_add</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">convert_to_numpy_ret_vals</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Parameters
        ----------
        feed_dict: a dictionary of node-&gt;np.ndarray supplied by user.
        convert_to_numpy_ret_vals: whether to convert ret vals to np.array.

        Returns
        -------
        A list of values for nodes in eval_node_list. tvm.nd.array or np.ndarray.
        </span><span class="sh">"""</span>
        <span class="k">def</span> <span class="nf">are_feed_shapes_equal</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="n">sb</span><span class="p">):</span>
            <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sb</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
                <span class="k">return</span> <span class="bp">False</span>
            <span class="n">unmatched_item</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">sa</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span> <span class="o">^</span> <span class="nf">set</span><span class="p">(</span><span class="n">sb</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
            <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">unmatched_item</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="n">node_to_val_map</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">feed_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">ndarray</span><span class="p">.</span><span class="n">NDArray</span><span class="p">),</span>\
                <span class="sh">"</span><span class="s">feed_dict value type not supported</span><span class="sh">"</span>    
            <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># collect shapes for all placeholders
</span>        <span class="n">feed_shapes</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_to_val_map</span><span class="p">:</span>
            <span class="n">feed_shapes</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">].</span><span class="n">shape</span>

        <span class="c1"># infer shape if feed_shapes changed since last run
</span>        <span class="c1"># e.g. call run() on test data after trainng
</span>        <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="nf">are_feed_shapes_equal</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">feed_shapes</span><span class="p">)):</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">infer_shape</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">feed_shapes</span> <span class="o">=</span> <span class="n">feed_shapes</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">memory_plan</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">compile_funcs</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">)</span>

        <span class="c1"># Traverse graph in topo order and compute values for all nodes.
</span>        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">topo_order</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_to_val_map</span><span class="p">:</span>
                <span class="c1"># Skip placeholder nodes. Values already provided by feed_dict.
</span>                <span class="k">continue</span>
            <span class="n">input_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_to_val_map</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">node_val</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">node_to_arr_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
            <span class="c1"># node_val is modified in-place
</span>            <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="n">node</span><span class="p">,</span> <span class="n">input_vals</span><span class="p">,</span> <span class="n">node_val</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">node_to_compiled_func</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
            <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_val</span>
        <span class="c1"># Collect node values.
</span>        <span class="k">if</span> <span class="n">convert_to_numpy_ret_vals</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">node_to_val_map</span><span class="p">[</span><span class="n">n</span><span class="p">].</span><span class="nf">asnumpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">eval_node_list</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">node_to_val_map</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">eval_node_list</span><span class="p">]</span>


</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AddOp</span><span class="p">(</span><span class="n">Op</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node_A</span><span class="p">,</span> <span class="n">node_B</span><span class="p">):</span>
        <span class="n">new_node</span> <span class="o">=</span> <span class="n">Op</span><span class="p">.</span><span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
        <span class="n">new_node</span><span class="p">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_A</span><span class="p">,</span> <span class="n">node_B</span><span class="p">]</span>
        <span class="n">new_node</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">(%s+%s)</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">node_A</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">node_B</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_node</span>

    <span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">input_vals</span><span class="p">,</span> <span class="n">output_val</span><span class="p">,</span> <span class="n">compiled_func</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">input_vals</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">assert</span> <span class="n">input_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span> <span class="o">==</span> <span class="n">input_vals</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">shape</span>
        <span class="nf">compiled_func</span><span class="p">(</span><span class="n">input_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_vals</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_val</span><span class="p">)</span>  

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">output_grad</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">input_shapes</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Need to handle input_vals[0].shape != input_vals[1].shape</span><span class="sh">"""</span>
        <span class="k">return</span> <span class="nf">broadcast_rule</span><span class="p">(</span><span class="n">input_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">compiled_func</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">input_shapes</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_add</span><span class="p">(</span>
            <span class="n">input_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_add</span><span class="sh">"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_elemwise_add</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">B</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="nc">B</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <h3 id="matrix_mul-impl-debug">matrix_mul impl debug</h3> <p>Get this erro while debgging matrix_mul</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>======================================================================
ERROR: test_tvm_op.test_matrix_multiply
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/zt/miniconda3/envs/uwsyml/lib/python3.8/site-packages/nose/case.py", line 197, in runTest
    self.test(*self.arg)
  File "/mnt/nvme1n1/zt/assignment2-2018/tests/test_tvm_op.py", line 94, in test_matrix_multiply
    matrix_mul(arr_x, arr_y, arr_z)
  File "/mnt/nvme1n1/zt/tvm/python/tvm/runtime/module.py", line 201, in __call__
    return self.entry_func(*args)
  File "/mnt/nvme1n1/zt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 245, in __call__
    raise_last_ffi_error()
  File "/mnt/nvme1n1/zt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm._ffi.base.TVMError: Traceback (most recent call last): 0: operator()                                                                                                                                                                                           
at /mnt/nvme1n1/zt/tvm/src/runtime/library_module.cc:82                                                                                                                                   
TVMError: Assert fail: T.Cast("int32", matrix_mul_B_shape[0]) == 500, Argument matrix_mul.B.shape[0] has an unsatisfied constraint: 500 == T.Cast("int32", matrix_mul_B_shape[0])   
</code></pre></div></div> <p>Wrong code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="o">//</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">wrong</span><span class="p">,</span> <span class="n">we</span> <span class="n">should</span> <span class="n">put</span> <span class="n">this</span> <span class="n">code</span> <span class="n">before</span> <span class="k">if</span> <span class="n">transposeA</span>  
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">come here</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a shape </span><span class="sh">'</span><span class="p">,</span> <span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="sh">'</span><span class="s">b shape</span><span class="sh">'</span><span class="p">,</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>



</code></pre></div></div> <p>Correct code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="c1"># print('a shape ', A.shape, 'b shape', B.shape)
</span>        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>

</code></pre></div></div> <p>I asked gpt to find the bug in this code it does not find the bug.</p> <h3 id="softmax-cross-entropy-impl">softmax cross entropy impl</h3> <p>Wrong code</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def make_matrix_softmax_cross_entropy(shape, tgt, tgt_host, func_name,
                                      dtype="float32"):
    """TODO: Your code here"""
    """Hint: output shape should be (1,)"""

    A = te.placeholder(shape, dtype=dtype, name='A')
    A_ = te.placeholder(shape, dtype=dtype, name='A_')

    B = te.compute(
           shape,
            lambda i, j: A_[i,j ] * te.log(A[i, j]),
            name='B'
            )

    row, col = shape
    axis_j = te.reduce_axis((0, col))
    axis_k = te.reduce_axis((0,row))
    C = te.compute(
            (1,),
            lambda :  -te.sum(B[ axis_j, axis_k], axis=[axis_j, axis_k]),
            name='C'
            )

    D = te.compute(
            (1,),
            lambda:  C / (row*col),
            name = 'D'
            )

    s = te.create_schedule(D.op)
    f = tvm.build(s, [A, A_, D], tgt, target_host=tgt_host, name=func_name)
    return f
</code></pre></div></div> <p>Got this error with fowllowing code</p> <p>error:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "/home/zt/miniconda3/envs/uwsyml/lib/python3.8/site-packages/nose/case.py", line 197, in runTest
    self.test(*self.arg)
  File "/mnt/nvme1n1/zt/assignment2-2018/tests/test_tvm_op.py", line 238, in test_softmax_cross_entropy
    matrix_softmax_cross_entropy = tvm_op.make_matrix_softmax_cross_entropy(shape, tgt, tgt_host, "softmax_cross_entropy")
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/tvm_op.py", line 217, in make_matrix_softmax_cross_entropy
    B = te.compute(

    ICHECK(0 == level_) &lt;&lt; "Reductions are only allowed at the top level of compute. "
tvm.error.InternalError: Traceback (most recent call last):
  7: operator()
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:168
  6: tvm::te::ComputeOp::ComputeOp(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tvm::runtime::Map&lt;tvm::runtime::String, tvm::runtime::ObjectRef, void, void&gt;, tvm::runtime::Array&lt;tvm::tir::IterVar, void&gt;, tvm::runtime::Array&lt;tvm::PrimExpr, void&gt;)
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:161
  5: VerifyComputeOp
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:581
  4: Run
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:551
  3: tvm::tir::ExprVisitor::VisitExpr_(tvm::tir::AddNode const*)
        at /mnt/nvme1n1/zt/tvm/src/tir/ir/expr_functor.cc:60
  2: VisitExpr
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:560
  1: VisitExpr_
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:564
  0: VisitExpr_
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:566
  File "/mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc", line 566
InternalError: Check failed: (0 == level_) is false: Reductions are only allowed at the top level of compute. Please create another tensor for further composition.
</code></pre></div></div> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_softmax_cross_entropy</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: output shape should be (1,)</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">A_</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A_</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">))</span>
    <span class="n">axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">row</span><span class="p">))</span>

    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
           <span class="p">(</span><span class="n">row</span><span class="p">,),</span>
           <span class="k">lambda</span> <span class="n">i</span> <span class="p">:</span> <span class="o">-</span><span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span> <span class="p">]</span> <span class="o">*</span> <span class="n">te</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_j</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span>
            <span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">axis_k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_k</span><span class="p">)</span><span class="o">/</span> <span class="n">row</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
        <span class="p">)</span>

</code></pre></div></div> <p>I think the error is saying that we should not use reduction and te.log() together?</p> <p>https://discuss.tvm.apache.org/t/non-top-level-reductions-in-compute-statements/5693</p> <p><a href="https://github.com/wyc-ruiker/CSE-599W-2018/blob/master/assignment2/python/dlsys/tvm_op.py">softmax cross entropy reference impl</a></p> <p>Try another impl code</p> <p>Fix code above after calculating te.log first and then do te.sum</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_softmax_cross_entropy</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: output shape should be (1,)</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">A_</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A_</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">j</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">row</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">log</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="n">sum_cross_entropy</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">row</span><span class="p">,),</span>
            <span class="c1"># lambda i: te.sum(B[i, axis_j], axis=axis_j ),
</span>            <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="o">-</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_j</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">sum_cross_entropy</span><span class="sh">'</span>
            <span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
        <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">sum_cross_entropy</span><span class="p">[</span><span class="n">axis_k</span><span class="p">]</span><span class="o">/</span><span class="n">row</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_k</span> <span class="p">)</span> <span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
        <span class="p">)</span>
</code></pre></div></div> <p>Correct code :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_softmax_cross_entropy</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: output shape should be (1,)</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">A_</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A_</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">softmax_axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax_j</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">softmax_axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax_k</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">max_x</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">),</span> 
           <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">softmax_axis_j</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="n">softmax_axis_j</span><span class="p">),</span> 
           <span class="n">name</span><span class="o">=</span> <span class="sh">'</span><span class="s">max_x</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">e_x</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="p">]</span> <span class="o">-</span> <span class="n">max_x</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">e_x</span><span class="sh">"</span>
            <span class="p">)</span>
    <span class="n">ex_sum</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">),</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">e_x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">softmax_axis_k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">softmax_axis_k</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">ex_sm</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">softmax</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">e_x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">ex_sum</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax_x</span><span class="sh">'</span><span class="p">)</span>
 

    <span class="n">axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">j</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">row</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">log</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">softmax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="n">sum_cross_entropy</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">row</span><span class="p">,),</span>
            <span class="c1"># lambda i: te.sum(B[i, axis_j], axis=axis_j ),
</span>            <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="o">-</span><span class="n">A_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_j</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">sum_cross_entropy</span><span class="sh">'</span>
            <span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
        <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">sum_cross_entropy</span><span class="p">[</span><span class="n">axis_k</span><span class="p">]</span><span class="o">/</span><span class="n">row</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_k</span> <span class="p">)</span> <span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
        <span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">A_</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <h3 id="fix-infer_shape-error">Fix infer_shape error</h3> <p><code class="language-plaintext highlighter-rouge">infer_shape()</code> is called for nodes in feed_dict. I don’t know why this happens.</p> <p>I am fixing it.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>type of node &lt;class 'dlsys.autodiff.Node'&gt; node name X
Traceback (most recent call last):
  File "tests/mnist_dlsys.py", line 373, in &lt;module&gt;
    m(executor_ctx, num_epochs, print_loss_val_each_epoch)
  File "tests/mnist_dlsys.py", line 132, in mnist_logreg
    loss_val, grad_W1_val, grad_b1_val, _ = executor.run(
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/autodiff.py", line 706, in run
    self.infer_shape(feed_shapes)
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/autodiff.py", line 616, in infer_shape
    infer_shape = node.op.infer_shape(node, input_shapes)
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/autodiff.py", line 320, in infer_shape
    assert False, "placeholder %s shape provided by feed_shape" % node.name
AssertionError: placeholder X shape provided by feed_shape
</code></pre></div></div> <p>I fixed this issue by swapping <code class="language-plaintext highlighter-rouge">shape[0]</code> and <code class="language-plaintext highlighter-rouge">shape[1]</code> directly instead of calling <code class="language-plaintext highlighter-rouge">np.transpose</code></p> <h3 id="speed-up-matrix-multilication">Speed up matrix multilication</h3> <p>Original matrix mul code and execution time Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="c1"># print('a shape ', A.shape, 'b shape', B.shape)
</span>        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(uwsyml) ➜  assignment2-2018 git:(master) ✗ python tests/mnist_dlsys.py -l -m mlp
=== Build 3-layer MLP model...
Loading data...
Start training loop...
/mnt/nvme1n1/zt/tvm/python/tvm/driver/build_module.py:280: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.
  warnings.warn(
epoch 0
shape a 1 784 shapeB 0 784
shape a 1 256 shapeB 0 256
shape a 1 100 shapeB 0 100
shape a 1 10 shapeB 0 10
shape a 1 100 shapeB 0 100
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
loss = 0.565684; Time taken this epoch = 39.259721 s
epoch 1
loss = 0.302340; Time taken this epoch = 37.834584 s
epoch 2
loss = 0.227699; Time taken this epoch = 37.836843 s
epoch 3
loss = 0.199743; Time taken this epoch = 37.733063 s
epoch 4
loss = 0.174254; Time taken this epoch = 37.731381 s
epoch 5
loss = 0.189644; Time taken this epoch = 37.791435 s
epoch 6
loss = 0.125607; Time taken this epoch = 37.795841 s
epoch 7
loss = 0.104398; Time taken this epoch = 37.821751 s
epoch 8
loss = 0.088052; Time taken this epoch = 37.845443 s
epoch 9
loss = 0.073229; Time taken this epoch = 37.798183 s
Validation set accuracy = 0.971600
Average Time per Training Epoch = 37.944825 s
</code></pre></div></div> <p>Optmized code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="c1"># print('a shape ', A.shape, 'b shape', B.shape)
</span>        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">xo</span><span class="p">,</span> <span class="n">xi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">yo</span><span class="p">,</span> <span class="n">yi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">xo</span><span class="p">,</span> <span class="n">yo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">yi</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">xo</span><span class="p">)</span>


    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(uwsyml) ➜  assignment2-2018 git:(master) ✗ python tests/mnist_dlsys.py -l -m mlp
=== Build 3-layer MLP model...
Loading data...
Start training loop...
/mnt/nvme1n1/zt/tvm/python/tvm/driver/build_module.py:280: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.
  warnings.warn(
epoch 0
shape a 1 784 shapeB 0 784
shape a 1 256 shapeB 0 256
shape a 1 100 shapeB 0 100
shape a 1 10 shapeB 0 10
shape a 1 100 shapeB 0 100
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
loss = 0.565684; Time taken this epoch = 2.756940 s
epoch 1
loss = 0.302340; Time taken this epoch = 0.805593 s
epoch 2
loss = 0.227699; Time taken this epoch = 0.840440 s
epoch 3
loss = 0.199743; Time taken this epoch = 0.827800 s
epoch 4
loss = 0.174254; Time taken this epoch = 0.829301 s
epoch 5
loss = 0.189644; Time taken this epoch = 0.835602 s
epoch 6
loss = 0.125607; Time taken this epoch = 0.836001 s
epoch 7
loss = 0.104398; Time taken this epoch = 0.831005 s
epoch 8
loss = 0.088052; Time taken this epoch = 0.829629 s
epoch 9
loss = 0.073229; Time taken this epoch = 0.830049 s
Validation set accuracy = 0.971600
Average Time per Training Epoch = 1.022236 s
</code></pre></div></div> <p>Explaination of <code class="language-plaintext highlighter-rouge">k</code> Certainly! In the line <code class="language-plaintext highlighter-rouge">k = s[C].op.reduce_axis[0]</code>, we are accessing the first reduction axis of the operation that computes <code class="language-plaintext highlighter-rouge">C</code>.</p> <p>Here’s a breakdown:</p> <ul> <li><code class="language-plaintext highlighter-rouge">s[C]</code>: This refers to the schedule for the computation of <code class="language-plaintext highlighter-rouge">C</code>.</li> <li><code class="language-plaintext highlighter-rouge">s[C].op</code>: This accesses the operation that computes <code class="language-plaintext highlighter-rouge">C</code>.</li> <li><code class="language-plaintext highlighter-rouge">s[C].op.reduce_axis</code>: This is a list of reduction axes used in the computation of <code class="language-plaintext highlighter-rouge">C</code>. Reduction axes are the dimensions over which we perform a reduction operation, such as summing elements.</li> <li><code class="language-plaintext highlighter-rouge">[0]</code>: This accesses the first reduction axis in the list.</li> </ul> <p>In the context of matrix multiplication, the reduction axis <code class="language-plaintext highlighter-rouge">k</code> represents the dimension over which we sum the products of elements from matrices <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>. For example, if <code class="language-plaintext highlighter-rouge">A</code> is of shape <code class="language-plaintext highlighter-rouge">(m, n)</code> and <code class="language-plaintext highlighter-rouge">B</code> is of shape <code class="language-plaintext highlighter-rouge">(n, p)</code>, the reduction axis <code class="language-plaintext highlighter-rouge">k</code> would range from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">n-1</code>, summing the products of corresponding elements from <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> to compute each element of the resulting matrix <code class="language-plaintext highlighter-rouge">C</code>.</p> <p>Let me know if you need any further clarification!</p> <h4 id="problem-setup">Problem Setup</h4> <p>Suppose <code class="language-plaintext highlighter-rouge">C</code> computes the result of multiplying two matrices ( A ) (size ( 64 \times 64 )) and ( B ) (size ( 64 \times 64 )):</p> <p>[ C[i, j] = \sum_{k=0}^{63} A[i, k] \cdot B[k, j] ]</p> <p>Here:</p> <ul> <li><code class="language-plaintext highlighter-rouge">x</code> corresponds to <code class="language-plaintext highlighter-rouge">i</code> (rows of ( C )),</li> <li><code class="language-plaintext highlighter-rouge">y</code> corresponds to <code class="language-plaintext highlighter-rouge">j</code> (columns of ( C )),</li> <li><code class="language-plaintext highlighter-rouge">k</code> is the reduction axis (over ( k )).</li> </ul> <h4 id="steps-in-context">Steps in Context</h4> <ol> <li><strong>Splitting Axes</strong> <ul> <li><code class="language-plaintext highlighter-rouge">x</code> (rows) and <code class="language-plaintext highlighter-rouge">y</code> (columns) are split into blocks of size <code class="language-plaintext highlighter-rouge">32</code>. This creates a <strong>tile-based computation</strong>: <ul> <li><code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> iterate over ( 2 \times 2 ) blocks (as ( 64 / 32 = 2 )).</li> <li><code class="language-plaintext highlighter-rouge">xi</code> and <code class="language-plaintext highlighter-rouge">yi</code> handle elements within each ( 32 \times 32 ) block.</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">k</code> is split into chunks of size <code class="language-plaintext highlighter-rouge">4</code>: <ul> <li><code class="language-plaintext highlighter-rouge">ko</code> iterates over 16 chunks (as ( 64 / 4 = 16 )).</li> <li><code class="language-plaintext highlighter-rouge">ki</code> handles individual reduction operations within each chunk.</li> </ul> </li> </ul> </li> <li><strong>Reordering</strong><br/> The computation is reordered to maximize: <ul> <li><strong>Data locality</strong>: Processing elements in nearby memory locations together.</li> <li><strong>Parallelism</strong>: Outer loops (<code class="language-plaintext highlighter-rouge">xo</code>, <code class="language-plaintext highlighter-rouge">yo</code>) can often run in parallel.</li> </ul> </li> </ol> <h4 id="execution-order">Execution Order</h4> <p>The reordered iteration could look like this (pseudocode):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">xo</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>         <span class="c1"># Iterate over 32-row blocks
</span>    <span class="k">for</span> <span class="n">yo</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>     <span class="c1"># Iterate over 32-column blocks
</span>        <span class="k">for</span> <span class="n">ko</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span> <span class="c1"># Iterate over reduction chunks (k-axis)
</span>            <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">32</span><span class="p">):</span>  <span class="c1"># Process rows within a block
</span>                <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">32</span><span class="p">):</span>  <span class="c1"># Process columns within a block
</span>                    <span class="k">for</span> <span class="n">ki</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>  <span class="c1"># Perform reduction within the chunk
</span>                        <span class="n">C</span><span class="p">[</span><span class="n">xo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">yi</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">xo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">xi</span><span class="p">,</span> <span class="n">ko</span><span class="o">*</span><span class="mi">4</span> <span class="o">+</span> <span class="n">ki</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">ko</span><span class="o">*</span><span class="mi">4</span> <span class="o">+</span> <span class="n">ki</span><span class="p">,</span> <span class="n">yo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">yi</span><span class="p">]</span>
</code></pre></div></div> <hr/> <h3 id="optimization-insight">Optimization Insight</h3> <p>This approach of splitting and reordering:</p> <ul> <li><strong>Improves memory access patterns</strong>: Data is processed in small blocks, reducing cache misses.</li> <li><strong>Enables parallel execution</strong>: Larger outer loops (<code class="language-plaintext highlighter-rouge">xo</code>, <code class="language-plaintext highlighter-rouge">yo</code>, <code class="language-plaintext highlighter-rouge">ko</code>) can be distributed across threads or cores.</li> <li><strong>Reduces computation overhead</strong>: By carefully controlling inner loops, computation can be streamlined for specific hardware.</li> </ul> <p>This code snippet is an example of how to define a computation schedule in <strong>TVM</strong>, a machine learning compiler framework used for optimizing tensor computations. Let’s break it down step by step:</p> <hr/> <h3 id="1-s--tecreate_schedulecop">1. <strong><code class="language-plaintext highlighter-rouge">s = te.create_schedule(C.op)</code></strong></h3> <ul> <li>This creates a schedule for the operation (<code class="language-plaintext highlighter-rouge">C.op</code>) that needs to be optimized.</li> <li>A schedule defines how the computation will be organized in terms of loops, parallelism, vectorization, etc.</li> </ul> <hr/> <h3 id="2-x-y--scopaxis">2. <strong><code class="language-plaintext highlighter-rouge">x, y = s[C].op.axis</code></strong></h3> <ul> <li><code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> are the primary loop axes of the computation. These are typically the dimensions of the tensor being computed.</li> <li>For instance, if <code class="language-plaintext highlighter-rouge">C</code> is a 2D tensor, <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> might represent its row and column dimensions.</li> </ul> <hr/> <h3 id="3-k--scopreduce_axis0">3. <strong><code class="language-plaintext highlighter-rouge">k = s[C].op.reduce_axis[0]</code></strong></h3> <ul> <li><code class="language-plaintext highlighter-rouge">k</code> is a reduction axis, commonly used in operations like matrix multiplication or summation over an axis.</li> <li>If <code class="language-plaintext highlighter-rouge">C</code> is the result of a matrix multiplication ( A \times B ), then <code class="language-plaintext highlighter-rouge">k</code> would be the summation axis.</li> </ul> <hr/> <h3 id="4-xo-xi--scsplitx-factor32">4. <strong><code class="language-plaintext highlighter-rouge">xo, xi = s[C].split(x, factor=32)</code></strong></h3> <ul> <li>Splits the <code class="language-plaintext highlighter-rouge">x</code> axis into two parts: <ul> <li><code class="language-plaintext highlighter-rouge">xo</code>: Outer loop, handling chunks of 32 iterations.</li> <li><code class="language-plaintext highlighter-rouge">xi</code>: Inner loop, handling the remaining iterations within each chunk.</li> </ul> </li> <li>This is a technique for tiling the computation, which improves cache efficiency.</li> </ul> <hr/> <h3 id="5-yo-yi--scsplity-factor32">5. <strong><code class="language-plaintext highlighter-rouge">yo, yi = s[C].split(y, factor=32)</code></strong></h3> <ul> <li>Similar to <code class="language-plaintext highlighter-rouge">x</code>, this splits the <code class="language-plaintext highlighter-rouge">y</code> axis into outer (<code class="language-plaintext highlighter-rouge">yo</code>) and inner (<code class="language-plaintext highlighter-rouge">yi</code>) loops, with a tile size of 32.</li> </ul> <hr/> <h3 id="6-ko-ki--scsplitk-factor4">6. <strong><code class="language-plaintext highlighter-rouge">ko, ki = s[C].split(k, factor=4)</code></strong></h3> <ul> <li>Splits the reduction axis <code class="language-plaintext highlighter-rouge">k</code> into: <ul> <li><code class="language-plaintext highlighter-rouge">ko</code>: Outer loop for reduction.</li> <li><code class="language-plaintext highlighter-rouge">ki</code>: Inner loop for reduction, with a tile size of 4.</li> </ul> </li> </ul> <hr/> <h3 id="7-screorderxo-yo-ko-xi-yi-ki">7. <strong><code class="language-plaintext highlighter-rouge">s[C].reorder(xo, yo, ko, xi, yi, ki)</code></strong></h3> <ul> <li>Rearranges the loop order to optimize computation.</li> <li>The new order ensures that the outer loops (<code class="language-plaintext highlighter-rouge">xo</code>, <code class="language-plaintext highlighter-rouge">yo</code>, <code class="language-plaintext highlighter-rouge">ko</code>) iterate first, followed by the inner loops (<code class="language-plaintext highlighter-rouge">xi</code>, <code class="language-plaintext highlighter-rouge">yi</code>, <code class="language-plaintext highlighter-rouge">ki</code>).</li> <li>This can improve locality of reference and allow efficient use of the hardware.</li> </ul> <hr/> <h3 id="8-scvectorizeyi">8. <strong><code class="language-plaintext highlighter-rouge">s[C].vectorize(yi)</code></strong></h3> <ul> <li>Applies vectorization to the <code class="language-plaintext highlighter-rouge">yi</code> loop.</li> <li>This instructs the compiler to use SIMD (Single Instruction, Multiple Data) instructions to compute multiple iterations of the <code class="language-plaintext highlighter-rouge">yi</code> loop simultaneously, boosting performance.</li> </ul> <hr/> <h3 id="9-scparallelxo">9. <strong><code class="language-plaintext highlighter-rouge">s[C].parallel(xo)</code></strong></h3> <ul> <li>Parallelizes the <code class="language-plaintext highlighter-rouge">xo</code> loop, enabling multiple threads to compute different chunks of the <code class="language-plaintext highlighter-rouge">x</code> axis concurrently.</li> <li>This is particularly useful on multi-core processors.</li> </ul> <hr/> <h3 id="example"><strong>Example</strong></h3> <p>Suppose <code class="language-plaintext highlighter-rouge">C</code> is the result of a matrix multiplication ( A[m, k] \times B[k, n] ), and we are optimizing the computation.</p> <ul> <li><strong>Matrix Dimensions</strong>: ( A ) is ( 64 \times 128 ), ( B ) is ( 128 \times 64 ).</li> <li>The schedule: <ul> <li>Splits the computation into tiles of ( 32 \times 32 ) for <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> dimensions (row and column).</li> <li>Reduces over <code class="language-plaintext highlighter-rouge">k</code> in chunks of 4.</li> <li>Reorders loops to optimize memory access patterns.</li> <li>Vectorizes operations within a 32-element column tile (<code class="language-plaintext highlighter-rouge">yi</code>).</li> <li>Parallelizes over outermost rows (<code class="language-plaintext highlighter-rouge">xo</code>), enabling multi-threading.</li> </ul> </li> </ul> <p>This structured approach ensures better utilization of the CPU/GPU by leveraging parallelism, vectorization, and efficient memory access.</p> <h2 id="we-can-parallelize-xo-and-yo-at-the-same-time">We can parallelize xo and yo at the same time</h2> <p>Parallelizing <code class="language-plaintext highlighter-rouge">xo</code> distributes the workload of the outer <code class="language-plaintext highlighter-rouge">x</code> axis (rows of tiles) across multiple threads, allowing each thread to compute one or more row tiles simultaneously. This is often chosen because:</p> <ol> <li> <p><strong>Independent Workload</strong>: Each tile along <code class="language-plaintext highlighter-rouge">xo</code> is independent, meaning computations for different tiles don’t depend on each other. This independence is a prerequisite for parallelization.</p> </li> <li> <p><strong>Balanced Workload</strong>: If the number of row tiles (determined by the size of <code class="language-plaintext highlighter-rouge">xo</code>) matches the number of available threads or cores, it results in a balanced workload.</p> </li> <li> <p><strong>Memory Access</strong>: For row-major memory layouts, accessing data row-wise tends to be more cache-friendly. Parallelizing <code class="language-plaintext highlighter-rouge">xo</code> aligns well with this layout.</p> </li> </ol> <hr/> <h3 id="can-we-parallelize-yo-at-the-same-time">Can We Parallelize <code class="language-plaintext highlighter-rouge">yo</code> at the Same Time?</h3> <p>Yes, it’s possible to parallelize <code class="language-plaintext highlighter-rouge">yo</code> as well, but there are important considerations:</p> <ol> <li><strong>Nested Parallelism</strong>: <ul> <li>If <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> are both parallelized, this results in nested parallelism (threads inside threads).</li> <li>This is often inefficient because hardware threads can’t dynamically spawn or manage sub-threads efficiently, and most hardware supports a flat parallel structure.</li> </ul> </li> <li><strong>Overhead</strong>: <ul> <li>Parallelizing both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> can introduce thread management overhead.</li> <li>The system must divide and coordinate work across all threads, which can diminish performance if not carefully managed.</li> </ul> </li> <li><strong>Thread Count</strong>: <ul> <li>Hardware has a limited number of threads or cores. If you parallelize both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code>, you risk oversubscribing threads, causing context-switching overhead.</li> </ul> </li> </ol> <hr/> <h3 id="when-parallelizing-both-could-work"><strong>When Parallelizing Both Could Work</strong></h3> <p>Parallelizing both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> is feasible under specific conditions:</p> <ol> <li><strong>Low Tile Counts in <code class="language-plaintext highlighter-rouge">xo</code> or <code class="language-plaintext highlighter-rouge">yo</code></strong>: <ul> <li>If <code class="language-plaintext highlighter-rouge">xo</code> alone doesn’t provide enough parallelism (e.g., when the matrix is small), you might also parallelize <code class="language-plaintext highlighter-rouge">yo</code>.</li> </ul> </li> <li><strong>Hierarchical Parallelism</strong>: <ul> <li>You can map <code class="language-plaintext highlighter-rouge">xo</code> to thread blocks and <code class="language-plaintext highlighter-rouge">yo</code> to threads within each block on GPUs or similar architectures.</li> </ul> </li> <li><strong>Custom Scheduling</strong>: <ul> <li>For some specialized cases, you can explicitly balance thread allocation across <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> to avoid over-parallelization.</li> </ul> </li> </ol> <hr/> <h3 id="example-scenario">Example Scenario</h3> <p>If you’re working with a 2D matrix computation and your target machine has 16 cores:</p> <ul> <li><strong>Parallelize only <code class="language-plaintext highlighter-rouge">xo</code></strong>: Efficient if the number of row tiles ( \frac{\text{rows}}{\text{tile_size}} ) is ≥ 16.</li> <li><strong>Parallelize both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code></strong>: Useful when row and column tiles combined can provide enough parallelism (e.g., ( 4 \times 4 = 16 ) tiles).</li> </ul> <p>To summarize, while it’s theoretically possible to parallelize both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code>, in practice, parallelizing only one is simpler and avoids excessive thread management overhead on most CPUs. For GPUs, however, hierarchical or hybrid parallelization across <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> is more common.</p>]]></content><author><name></name></author><category term="ml"/><category term="framework"/><category term="ml"/><category term="framework"/><summary type="html"><![CDATA[This is fun to implement]]></summary></entry><entry><title type="html">Imperative programming vs. Declarative programming</title><link href="https://bilyz98.github.io/blog/2024/imperative-programming/" rel="alternate" type="text/html" title="Imperative programming vs. Declarative programming"/><published>2024-12-24T11:59:00+00:00</published><updated>2024-12-24T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/imperative-programming</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/imperative-programming/"><![CDATA[<h2 id="what-is-imperative-programming-vs-declarative-programming">What is imperative programming vs. declarative programming</h2> <p>Imperative programming means that each steps of command/code is specified to get the final computation result.</p> <p>Declarative programming means that only the results that are wanted are specified, no step by step code is provided to get the final computation results.</p> <p>For example,</p> <p>SQL is a declarative programming language</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="k">user</span> 
<span class="k">where</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">'bily'</span>
</code></pre></div></div> <p>c++ is an iimperative programming language</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vector</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">users</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">users</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span><span class="p">(</span><span class="n">users</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="s">"bily"</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">user</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
        <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="advantage-and-disadvantage-of-imperative-programming">Advantage and disadvantage of imperative programming.</h3> <ol> <li> <p>Imperative programming depends on defined instructions to achieve final computation results, so code can be easy to understand and straightforward.</p> </li> <li> <p>Order of performed operations is completely controlled by developer</p> </li> <li> <p>Bugs can be easily traced because the program is assembled from blocks of code that is based on step by step commands.</p> </li> <li> <p>Memory allocation and manipulation is directly linked in imperative programming. Efficient use of machine memory.</p> </li> </ol> <h3 id="reference">Reference</h3> <p><a href="https://zhuanlan.zhihu.com/p/34445114">1</a> <a href="https://www.geeksforgeeks.org/what-is-imperative-programming/">2</a></p>]]></content><author><name></name></author><category term="interview"/><category term="programming"/><summary type="html"><![CDATA[What is imperative programming vs. declarative programming]]></summary></entry><entry><title type="html">Hash in cpp</title><link href="https://bilyz98.github.io/blog/2024/hash-in-cpp/" rel="alternate" type="text/html" title="Hash in cpp"/><published>2024-12-22T11:59:00+00:00</published><updated>2024-12-22T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/hash-in-cpp</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/hash-in-cpp/"><![CDATA[<h2 id="open-hashing-and-close-hashing">Open hashing and close hashing</h2> <p>What is open hashing and close hashing ?</p> <h3 id="open-hashing-separate-chaining">Open hashing (separate chaining)</h3> <p>Method: Each bucket in the table contains a linked list of all elements that hash to the same index</p> <p>Collision handling: the new element is added to the list of the corresponding linked list(bucket)</p> <p>Memory usage: requires additional memory for pointers in linked list</p> <p>Performance: generally good compared to close hashing.</p> <p>Example: <code class="language-plaintext highlighter-rouge">std::unordered_map</code> in cpp</p> <h3 id="close-hashing-open-addressing">Close hashing (open addressing)</h3> <p>Method: All elements are stored in the table itself. When a collision occurs the algorithm searched for next available slot in the table</p> <p>Collision handling: linear probing, quadratic probing, or double hashing.</p> <p>Memory usage: less compared to open hashing</p> <p>Performance: performance can degrade if the table is full, leading to longer search time</p> <p>Example: python uses close hashing.</p> <h2 id="load-factor-in-hash-table">Load factor in hash table</h2> <p>load factor is defined as number of elements to number of buckets in the table \(load factor = \frac{n}{b}\)</p> <p>Example</p> <p>If a hash table has 100 elements and 150 buckets, the load factor is</p> \[load factor = \frac{100}{150} = 0.67\] <h2 id="unordered_map-in-cpp">unordered_map in cpp</h2> <p>This article talks about internal implementation of <code class="language-plaintext highlighter-rouge">unordered_map</code> in cpp.</p> <p>It’s very intuitive.</p> <p><a href="https://jbseg.medium.com/c-unordered-map-under-the-hood-9540cec4553a">Explanation of internal implementation of unordered_map</a></p> <p><a href="https://stackoverflow.com/questions/31112852/how-stdunordered-map-is-implemented">This stackoverflow post talks about why unordered_map in cpp uses open hashing</a></p>]]></content><author><name></name></author><category term="interview"/><category term="algorithm"/><category term="cpp"/><summary type="html"><![CDATA[Open hashing and close hashing]]></summary></entry><entry><title type="html">Computer basics</title><link href="https://bilyz98.github.io/blog/2024/some-basic-computer-knowledge/" rel="alternate" type="text/html" title="Computer basics"/><published>2024-12-21T11:59:00+00:00</published><updated>2024-12-21T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/some-basic-computer-knowledge</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/some-basic-computer-knowledge/"><![CDATA[<p>Had an interview today.</p> <p>I was struggling to answer the computer basics questions smoothly.</p> <p>I write down the questions I got here to help strengthen the knowledge.</p> <h2 id="difference-between-b-tree-b-tree-and-binary-tree">Difference between b+ tree, b tree and binary tree</h2> <ul> <li>b+ tree and btree can be used to disk storage, binary tree is for memory opeartion.</li> </ul> <h3 id="difference-between-btree-and-btree">Difference between b+tree and btree</h3> <ul> <li> <p>Structure and storage</p> <p>btree stores both keys and data pointers in leaf nodes and internal nodes.</p> <p>b+tree only stores data pointers at leaf nodes. Internal nodes are only used to store keys go guide search process</p> </li> <li> <p>Search efficiency</p> <p>Search in btree can be slower because data might be in any level in btree</p> <p>Search in b+tree might be faster because all data is stored in leaf nodes</p> </li> <li> <p>Insertion and deletion</p> <p>Insertion and deletion in b=tree is more simpler because they only affect leaf nodes.</p> </li> <li> <p>Sequential access</p> <p>Sequential access is not possible in a btree because leaf nodes are not linked.</p> <p>It’s possible in b+tree</p> </li> <li> <p>Applications</p> <p>btree used in databases and search engines</p> <p>b+tree used in database indexing and multi-level indexing</p> </li> </ul> <h3 id="where-is-binary-tree-used-in-real-worlod-software">Where is binary tree used in real worlod software</h3> <p>Linux kernel.</p> <ol> <li> <p>I/O scheduler</p> <p>CFQ(completely fair queueing) I/O scheduler to distribute I/O bandwidth among all I/O requests.</p> </li> <li> <p>Filesystem</p> </li> <li> <p>virtual memory area</p> </li> <li> <p>epoll file descriptor</p> </li> <li> <p>network packets</p> </li> </ol> <h2 id="mutex-and-spinlock">mutex and spinlock</h2> <p>mutex.lock() will put thread to sleep if it can not get the lock immediately which is blocking lock acquire</p> <p>spinlock.lock() will not put thread into sleep. It does busy checking all the time until it gets the lock.</p> <p>mutex internal implementation with pthread</p> <p>As we can see it uses conditional variable to do this sleep and wakeup.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">pthread_mutex_lock</span><span class="p">(</span><span class="n">pthread_mutex_t</span> <span class="o">*</span><span class="n">mutex</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Try to acquire the lock using an atomic operation</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">atomic_compare_and_swap</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Successfully acquired the lock</span>
        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Spin for a short period</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">spin_count</span> <span class="o">&lt;</span> <span class="n">MAX_SPIN_COUNT</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">atomic_compare_and_swap</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// Successfully acquired the lock</span>
            <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">spin_count</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// If spinning fails, block the thread</span>
    <span class="n">block_thread</span><span class="p">(</span><span class="n">mutex</span><span class="p">);</span>

    <span class="c1">// When the thread is woken up, try to acquire the lock again</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">atomic_compare_and_swap</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Wait until the lock is available</span>
        <span class="n">wait_for_lock</span><span class="p">(</span><span class="n">mutex</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>


<span class="cp">#include</span> <span class="cpf">&lt;pthread.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;queue&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;atomic&gt;</span><span class="cp">
</span>
<span class="k">struct</span> <span class="nc">Mutex</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span> <span class="n">locked</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">queue</span><span class="o">&lt;</span><span class="n">pthread_t</span><span class="o">&gt;</span> <span class="n">waiting_threads</span><span class="p">;</span>
    <span class="n">pthread_mutex_t</span> <span class="n">internal_mutex</span><span class="p">;</span>
    <span class="n">pthread_cond_t</span> <span class="n">cond</span><span class="p">;</span>
<span class="p">};</span>

<span class="kt">void</span> <span class="nf">block_thread</span><span class="p">(</span><span class="n">Mutex</span><span class="o">*</span> <span class="n">mutex</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">pthread_mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">internal_mutex</span><span class="p">);</span>

    <span class="c1">// Add the current thread to the waiting queue</span>
    <span class="n">pthread_t</span> <span class="n">current_thread</span> <span class="o">=</span> <span class="n">pthread_self</span><span class="p">();</span>
    <span class="n">mutex</span><span class="o">-&gt;</span><span class="n">waiting_threads</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">current_thread</span><span class="p">);</span>

    <span class="c1">// Wait for the condition variable to be signaled</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">locked</span><span class="p">.</span><span class="n">load</span><span class="p">())</span> <span class="p">{</span>
        <span class="n">pthread_cond_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">cond</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">internal_mutex</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="c1">// Remove the current thread from the waiting queue</span>
    <span class="n">mutex</span><span class="o">-&gt;</span><span class="n">waiting_threads</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>

    <span class="n">pthread_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">internal_mutex</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">wake_thread</span><span class="p">(</span><span class="n">Mutex</span><span class="o">*</span> <span class="n">mutex</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">pthread_mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">internal_mutex</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">waiting_threads</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
        <span class="c1">// Signal the condition variable to wake up one waiting thread</span>
        <span class="n">pthread_cond_signal</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">cond</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">pthread_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="o">-&gt;</span><span class="n">internal_mutex</span><span class="p">);</span>
<span class="p">}</span>

</code></pre></div></div>]]></content><author><name></name></author><category term="interview"/><category term="algorithm"/><category term="os"/><category term="linux"/><category term="cpp"/><category term="lock"/><category term="b+tree"/><summary type="html"><![CDATA[Had an interview today.]]></summary></entry><entry><title type="html">Rdma</title><link href="https://bilyz98.github.io/blog/2024/rdma/" rel="alternate" type="text/html" title="Rdma"/><published>2024-12-06T00:00:00+00:00</published><updated>2024-12-06T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/rdma</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/rdma/"><![CDATA[<p>[Article talks about difference between one sided and two sided rdma]https://forums.developer.nvidia.com/t/comparasion-of-one-sided-vs-two-sided-operations/207183</p> <h2 id="what-is-single-sided-rdma">What is single-sided RDMA</h2> <p>Single-sided RDMA (Remote Direct Memory Access) refers to a method where one machine can directly access the memory of another machine without involving the remote CPU. This is achieved through RDMA’s one-sided verbs, which allow for operations like read and write to be performed directly on the remote memory.</p> <h3 id="key-features-of-single-sided-rdma">Key Features of Single-Sided RDMA:</h3> <ul> <li><strong>Low Latency</strong>: Provides ultra-low latency network communication.</li> <li><strong>CPU Bypass</strong>: Bypasses the remote CPU, reducing overhead and improving efficiency.</li> <li><strong>Direct Memory Access</strong>: Enables direct access to remote memory, which is particularly useful in distributed systems and databases.</li> </ul> <h3 id="applications">Applications:</h3> <ul> <li><strong>Distributed Databases</strong>: Used in disaggregated database management systems to improve performance and scalability.</li> <li><strong>Key-Value Stores</strong>: Utilized in high-performance key-value stores to achieve low latency and high throughput.</li> </ul> <p>Single-sided RDMA is a powerful tool for building efficient and scalable distributed systems by leveraging direct memory access and minimizing CPU involvement.</p> <h2 id="what-is-two-sided-rdma">What is two-sided RDMA</h2> <p>Yes, there is a concept known as two-sided RDMA operations. While single-sided RDMA operations involve one machine directly accessing the memory of another without involving the remote CPU, two-sided RDMA operations require both the sender and receiver to participate in the communication.</p> <h3 id="key-features-of-two-sided-rdma">Key Features of Two-Sided RDMA:</h3> <ul> <li><strong>Send/Receive Operations</strong>: In two-sided RDMA, the sender posts a send work request, and the receiver posts a receive work request. The data transfer occurs when the send request matches the receive request.</li> <li><strong>CPU Involvement</strong>: Both the sender’s and receiver’s CPUs are involved in the communication process, which can add some overhead compared to single-sided operations.</li> <li><strong>Use Cases</strong>: Two-sided RDMA is often used in scenarios where synchronization between the sender and receiver is necessary, such as in certain types of distributed transactions and message passing.</li> </ul> <h3 id="comparison-with-single-sided-rdma">Comparison with Single-Sided RDMA:</h3> <ul> <li><strong>Performance</strong>: Single-sided RDMA generally offers lower latency and higher throughput because it bypasses the remote CPU. However, two-sided RDMA can be more suitable for applications requiring tight synchronization and coordination between nodes.</li> <li><strong>Complexity</strong>: Two-sided RDMA operations can be more complex to implement due to the need for both sides to coordinate their actions.</li> </ul> <p>You can find more detailed information and comparisons between one-sided and two-sided RDMA operations <a href="https://forums.developer.nvidia.com/t/comparasion-of-one-sided-vs-two-sided-operations/207183">here</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[[Article talks about difference between one sided and two sided rdma]https://forums.developer.nvidia.com/t/comparasion-of-one-sided-vs-two-sided-operations/207183]]></summary></entry><entry><title type="html">Autodiff implementation</title><link href="https://bilyz98.github.io/blog/2024/uw-sysml-assign/" rel="alternate" type="text/html" title="Autodiff implementation"/><published>2024-11-29T11:59:00+00:00</published><updated>2024-11-29T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/uw-sysml-assign</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/uw-sysml-assign/"><![CDATA[<p><a href="https://github.com/dlsys-course/assignment1-2018">Source assignment</a></p> <p>Wrong code I wrote:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    for node in reverse_topo_order:
        cur_node_grad = node_to_output_grads_list[node]
        sum_grad = np.sum(cur_node_grad, axis=0)
        node_grad = node.op.gradient(node, sum_grad)
        for input in node.inputs:
            input_grads = node.op.gradient(input, sum_grad)
            if input not in node_to_output_grads_list:
                node_to_output_grads_list[input] = []
            node_to_output_grads_list[input].append(input_grads)


</code></pre></div></div> <p><strong>Understanding the Code and Identifying the Issue</strong></p> <p>The provided code implements a backpropagation algorithm to compute gradients of an output node with respect to a list of input nodes in a computational graph. The core idea is to traverse the graph in reverse topological order, calculating gradients at each node and propagating them backward.</p> <p>However, the current implementation has a potential issue:</p> <ol> <li><strong>Incorrect Gradient Calculation:</strong> <ul> <li>The line <code class="language-plaintext highlighter-rouge">node_to_output_grad[node] = sum_grad</code> directly assigns the sum of gradients to the node’s gradient. This is incorrect, as it doesn’t account for the specific gradient contribution of each input to the node’s output.</li> <li>The subsequent gradient calculation for input nodes using <code class="language-plaintext highlighter-rouge">node.op.gradient</code> might also be affected by this incorrect sum.</li> </ul> </li> </ol> <p><strong>Corrected Code</strong></p> <p>Here’s a corrected implementation that addresses the issue:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">output_node</span><span class="p">,</span> <span class="n">node_list</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Take gradient of output node with respect to each node in node_list.

    Parameters
    ----------
    output_node: output node that we are taking derivative of.
    node_list: list of nodes that we are taking derivative wrt.

    Returns
    -------
    A list of gradient values, one for each node in node_list respectively.
    </span><span class="sh">"""</span>

    <span class="n">node_to_output_grads_list</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">node_to_output_grad</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Initialize gradient of output node as oneslike_op(output_node)
</span>    <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">output_node</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nf">oneslike_op</span><span class="p">(</span><span class="n">output_node</span><span class="p">)]</span>

    <span class="n">reverse_topo_order</span> <span class="o">=</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">find_topo_sort</span><span class="p">([</span><span class="n">output_node</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">reverse_topo_order</span><span class="p">:</span>
        <span class="n">cur_node_grads</span> <span class="o">=</span> <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
        <span class="n">sum_grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">cur_node_grads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Sum gradients across different output nodes
</span>
        <span class="c1"># Calculate gradient for the current node
</span>        <span class="n">node_grad</span> <span class="o">=</span> <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">sum_grad</span><span class="p">)</span>
        <span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_grad</span>

        <span class="c1"># Propagate gradients to input nodes
</span>        <span class="k">for</span> <span class="n">input_node</span><span class="p">,</span> <span class="n">input_grad</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">node_grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">input_node</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node_to_output_grads_list</span><span class="p">:</span>
                <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">input_node</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">input_node</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">input_grad</span><span class="p">)</span>

    <span class="c1"># Collect results for gradients requested
</span>    <span class="n">grad_node_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_list</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">grad_node_list</span>
</code></pre></div></div> <p><strong>Explanation of the Correction</strong></p> <ol> <li><strong>Correct Gradient Calculation:</strong> <ul> <li>The <code class="language-plaintext highlighter-rouge">sum_grad</code> is now calculated by summing the gradients across different output nodes, ensuring that the total gradient for the current node is correctly computed.</li> <li>The <code class="language-plaintext highlighter-rouge">node.op.gradient</code> method is used to calculate the gradient of the node with respect to its inputs, considering the current sum of gradients.</li> </ul> </li> <li><strong>Gradient Propagation:</strong> <ul> <li>The gradients are propagated to input nodes using <code class="language-plaintext highlighter-rouge">zip</code> to iterate over input nodes and their corresponding gradients, ensuring correct association.</li> </ul> </li> </ol> <p>By addressing the incorrect gradient calculation, this revised code provides a more accurate and robust implementation of the backpropagation algorithm.</p> <h2 id="nontype-not-iterable-error">nontype not iterable error</h2> <p>Code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def gradients(output_node, node_list):
    """Take gradient of output node with respect to each node in node_list.

    Parameters
    ----------
    output_node: output node that we are taking derivative of.
    node_list: list of nodes that we are taking derivative wrt.

    Returns
    -------
    A list of gradient values, one for each node in node_list respectively.

    """

    # a map from node to a list of gradient contributions from each output node
    node_to_output_grads_list = {}
    # Special note on initializing gradient of output_node as oneslike_op(output_node):
    # We are really taking a derivative of the scalar reduce_sum(output_node)
    # instead of the vector output_node. But this is the common case for loss function.
    node_to_output_grads_list[output_node] = [oneslike_op(output_node)]
    # a map from node to the gradient of that node
    node_to_output_grad = {}
    # Traverse graph in reverse topological order given the output_node that we are taking gradient wrt.
    reverse_topo_order = reversed(find_topo_sort([output_node]))

    """TODO: Your code here"""
    for node in reverse_topo_order:
        cur_node_grad = node_to_output_grads_list[node]
        sum_grad = np.sum(cur_node_grad, axis=0)
        # gradient for each input node with gradient from current node
        node_grad = node.op.gradient(node, sum_grad)
        node_to_output_grad[node] = node_grad

        if node.inputs is not None:
            print('type node.inputs', type(node.inputs), 'node name', node.name,  'type node_grad', type(node_grad))
            for input_node, input_grad in zip(node.inputs, node_grad):
                if input_node not in node_to_output_grads_list:
                    node_to_output_grads_list[input_node] = []
                # This is wrong
                node_to_output_grads_list[input_node] = input_grad

        # node_grad = node.op.gradient(node, sum_grad)
        # for input in node.inputs:
        #     input_grads = node.op.gradient(input, sum_grad)
        #     if input not in node_to_output_grads_list:
        #         node_to_output_grads_list[input] = []
        #     node_to_output_grads_list[input].append(input_grads)

        # for i in range(len(node.inputs)):
        #     input_node = node.inputs[i]
        #     if input_node not in node_to_output_grads_list:
        #         node_to_output_grads_list[input_node] = []
        #     input_node_grad = node_grad[i]
        #     node_to_output_grads_list[input_node].append(input_node_grad)

    # Collect results for gradients requested.
    grad_node_list = [node_to_output_grad[node] for node in node_list]
    return grad_node_list


</code></pre></div></div> <p>Error:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>======================================================================                                               ERROR: autodiff_test.test_add_mul_mix_1                                                                              ----------------------------------------------------------------------                                               Traceback (most recent call last):                                                                                     File "/home/zt/miniconda3/lib/python3.12/site-packages/nose/case.py", line 189, in runTest                             self.test(*self.arg)                                                                                               File "/mnt/nvme1n1/zt/assignment1-2018/autodiff_test.py", line 86, in test_add_mul_mix_1                               grad_x1, grad_x2, grad_x3 = ad.gradients(y, [x1, x2, x3])                                                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                          File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 353, in gradients                                            for input_node, input_grad in zip(node.inputs, node_grad):                                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                        TypeError: 'NoneType' object is not iterable                                                                         -------------------- &gt;&gt; begin captured stdout &lt;&lt; ---------------------
type node.inputs &lt;class 'list'&gt; node name (x1+((x2*x3)*x1)) type node_grad &lt;class 'list'&gt;
type node.inputs &lt;class 'list'&gt; node name ((x2*x3)*x1) type node_grad &lt;class 'list'&gt;
type node.inputs &lt;class 'list'&gt; node name (x2*x3) type node_grad &lt;class 'list'&gt;
type node.inputs &lt;class 'list'&gt; node name x3 type node_grad &lt;class 'NoneType'&gt;
                                                                                                                     --------------------- &gt;&gt; end captured stdout &lt;&lt; ----------------------
-------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------
--------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------

</code></pre></div></div> <p>I think it’s because I don’t add placeholder op to input x value.</p> <p>Get another error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ERROR: autodiff_test.test_add_by_const                                                                               ----------------------------------------------------------------------                                               Traceback (most recent call last):                                                                                     File "/home/zt/miniconda3/lib/python3.12/site-packages/nose/case.py", line 189, in runTest                             self.test(*self.arg)                                                                                               File "/mnt/nvme1n1/zt/assignment1-2018/autodiff_test.py", line 26, in test_add_by_const                                y_val, grad_x2_val= executor.run(feed_dict = {x2 : x2_val})                                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                        File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 306, in run                                                  topo_order = find_topo_sort(self.eval_node_list)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                   File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 392, in find_topo_sort                                       topo_sort_dfs(node, visited, topo_order)                                                                           File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 400, in topo_sort_dfs                                        for n in node.inputs:                                                                                                         ^^^^^^^^^^^                                                                                             AttributeError: 'NoneType' object has no attribute 'inputs'                                                          -------------------- &gt;&gt; begin captured stdout &lt;&lt; ---------------------                                               type node.inputs &lt;class 'list'&gt; node name (x2+5) type node_grad &lt;class 'list'&gt;                                                                                                                                                            --------------------- &gt;&gt; end captured stdout &lt;&lt; ----------------------                                               -------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------                                               --------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------
</code></pre></div></div> <p>Correct code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">output_node</span><span class="p">,</span> <span class="n">node_list</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Take gradient of output node with respect to each node in node_list.

    Parameters
    ----------
    output_node: output node that we are taking derivative of.
    node_list: list of nodes that we are taking derivative wrt.

    Returns
    -------
    A list of gradient values, one for each node in node_list respectively.

    </span><span class="sh">"""</span>

    <span class="c1"># a map from node to a list of gradient contributions from each output node
</span>    <span class="n">node_to_output_grads_list</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Special note on initializing gradient of output_node as oneslike_op(output_node):
</span>    <span class="c1"># We are really taking a derivative of the scalar reduce_sum(output_node)
</span>    <span class="c1"># instead of the vector output_node. But this is the common case for loss function.
</span>    <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">output_node</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nf">oneslike_op</span><span class="p">(</span><span class="n">output_node</span><span class="p">)]</span>
    <span class="c1"># a map from node to the gradient of that node
</span>    <span class="n">node_to_output_grad</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Traverse graph in reverse topological order given the output_node that we are taking gradient wrt.
</span>    <span class="n">reverse_topo_order</span> <span class="o">=</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">find_topo_sort</span><span class="p">([</span><span class="n">output_node</span><span class="p">]))</span>

    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">reverse_topo_order</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_to_output_grads_list</span><span class="p">:</span>
            <span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="nf">sum_node_list</span><span class="p">(</span><span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
        <span class="c1"># cur_node_grad = node_to_output_grads_list[node]
</span>        <span class="c1"># sum_grad = np.sum(cur_node_grad, axis=0)
</span>        <span class="c1"># gradient for each input node with gradient from current node
</span>        <span class="c1"># node_grad = node.op.gradient(node, sum_grad)
</span>        <span class="c1"># node_to_output_grad[node] = node_grad
</span>
        <span class="c1"># if node_grad is not None:
</span>        <span class="c1">#     print('type node.inputs', type(node.inputs), 'node name', node.name,  'type node_grad', type(node_grad))
</span>        <span class="c1">#     for input_node, input_grad in zip(node.inputs, node_grad):
</span>        <span class="c1">#         if input_node not in node_to_output_grads_list:
</span>        <span class="c1">#             node_to_output_grads_list[input_node] = []
</span>        <span class="c1">#         node_to_output_grads_list[input_node] = input_grad
</span>            <span class="k">for</span> <span class="nb">id</span> <span class="p">,</span> <span class="n">in_nodes</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">in_nodes</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node_to_output_grads_list</span><span class="p">:</span>
                    <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">in_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">in_nodes</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="nb">id</span><span class="p">])</span>

        <span class="c1"># node_grad = node.op.gradient(node, sum_grad)
</span>        <span class="c1"># for input in node.inputs:
</span>        <span class="c1">#     input_grads = node.op.gradient(input, sum_grad)
</span>        <span class="c1">#     if input not in node_to_output_grads_list:
</span>        <span class="c1">#         node_to_output_grads_list[input] = []
</span>        <span class="c1">#     node_to_output_grads_list[input].append(input_grads)
</span>
        <span class="c1"># for i in range(len(node.inputs)):
</span>        <span class="c1">#     input_node = node.inputs[i]
</span>        <span class="c1">#     if input_node not in node_to_output_grads_list:
</span>        <span class="c1">#         node_to_output_grads_list[input_node] = []
</span>        <span class="c1">#     input_node_grad = node_grad[i]
</span>        <span class="c1">#     node_to_output_grads_list[input_node].append(input_node_grad)
</span>
    <span class="c1"># Collect results for gradients requested.
</span>    <span class="n">grad_node_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_list</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">grad_node_list</span>


</code></pre></div></div> <p>I don’t know why this is the correct code.</p> <p>What is the difference between my previous code and this code ?</p> <p>This is very different from pytorch like auto gradient calculation.</p> <p><a href="https://dlsys.cs.washington.edu/pdf/lecture4.pdf">Autodiff algorithm lecture</a></p> <p><strong>Yes, PyTorch leverages automatic differentiation (autodiff) to calculate gradients automatically.</strong> This powerful feature significantly simplifies the process of training neural networks.</p> <p><strong>How it Works:</strong></p> <ol> <li><strong>Computational Graph:</strong> When you perform operations on PyTorch tensors, a computational graph is created. This graph represents the sequence of operations involved in calculating the output.</li> <li><strong>Forward Pass:</strong> During the forward pass, the input data is fed into the graph, and calculations are performed to obtain the output.</li> <li><strong>Backward Pass:</strong> In the backward pass, the gradients of the output with respect to the input parameters are computed using the chain rule. PyTorch automatically traverses the computational graph in reverse order, calculating gradients for each node.</li> <li><strong>Gradient Update:</strong> The calculated gradients are then used to update the model’s parameters using an optimization algorithm like stochastic gradient descent (SGD).</li> </ol> <p><strong>Key Benefits of PyTorch’s Autodiff:</strong></p> <ul> <li><strong>Simplified Training:</strong> Autodiff eliminates the need for manual gradient calculation, making the training process more efficient.</li> <li><strong>Complex Models:</strong> It enables the training of complex deep learning models with numerous layers and parameters.</li> <li><strong>Flexibility:</strong> PyTorch’s dynamic computational graph allows for more flexibility in defining and training neural networks.</li> </ul> <p>By understanding how PyTorch’s autodiff works, you can effectively leverage this powerful tool to build and train sophisticated deep learning models.</p> <p><a href="https://github.com/yzh119/UW-dlsys-assignment1/blob/master/autodiff.py">reference repo</a></p> <p>This test code gives good example about how to use autodiff to do forward and gradient calculation.</p> <p>First we define variable in the graph.</p> <p>We do some calculations with the node in the graph. And then we first call <code class="language-plaintext highlighter-rouge">gradient()</code> to get gradient we are interested to get. And then we call <code class="language-plaintext highlighter-rouge">Excutor()</code> to create a instance of Executor to do forward pass to get the value want.</p> <p>In the constructor arg we give all the variables in graph we are interested to get value of when <code class="language-plaintext highlighter-rouge">run()</code> is called.</p> <p>To actually run and get values we need to call <code class="language-plaintext highlighter-rouge">executor.run()</code> by giving values for all inputs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_matmul_two_vars</span><span class="p">():</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">x3</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x3</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nf">matmul_op</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">)</span>

    <span class="n">grad_x2</span><span class="p">,</span> <span class="n">grad_x3</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">])</span>
    
    <span class="n">executor</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">Executor</span><span class="p">([</span><span class="n">y</span><span class="p">,</span> <span class="n">grad_x2</span><span class="p">,</span> <span class="n">grad_x3</span><span class="p">])</span>
    <span class="n">x2_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span> <span class="c1"># 3x2
</span>    <span class="n">x3_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]])</span> <span class="c1"># 2x3
</span>
    <span class="n">y_val</span><span class="p">,</span> <span class="n">grad_x2_val</span><span class="p">,</span> <span class="n">grad_x3_val</span> <span class="o">=</span> <span class="n">executor</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x2</span><span class="p">:</span> <span class="n">x2_val</span><span class="p">,</span> <span class="n">x3</span><span class="p">:</span> <span class="n">x3_val</span><span class="p">})</span>

    <span class="n">expected_yval</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x2_val</span><span class="p">,</span> <span class="n">x3_val</span><span class="p">)</span>
    <span class="n">expected_grad_x2_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">expected_yval</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">x3_val</span><span class="p">))</span>
    <span class="n">expected_grad_x3_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">x2_val</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">expected_yval</span><span class="p">))</span>

    <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ad</span><span class="p">.</span><span class="n">Node</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">expected_yval</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">grad_x2_val</span><span class="p">,</span> <span class="n">expected_grad_x2_val</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">grad_x3_val</span><span class="p">,</span> <span class="n">expected_grad_x3_val</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="ml"/><category term="framework"/><category term="ml"/><category term="framework"/><summary type="html"><![CDATA[Source assignment]]></summary></entry><entry><title type="html">Topo sort</title><link href="https://bilyz98.github.io/blog/2024/topo-sort/" rel="alternate" type="text/html" title="Topo sort"/><published>2024-11-29T11:59:00+00:00</published><updated>2024-11-29T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/topo-sort</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/topo-sort/"><![CDATA[<h2 id="topo-sort-starts-from-end-node">Topo sort starts from end node</h2> <p>This is used in autodiff graph build algorithm</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_topo_sort</span><span class="p">(</span><span class="n">node_list</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Given a list of nodes, return a topo ordering of nodes ending in them.

    A simple algorithm is to do a post-order DFS traversal on the given nodes,
    going backwards based on input edges. Since a node is added to the ordering
    after all its predecessors are traversed due to post-order DFS, we get a
    topological sort.

    </span><span class="sh">"""</span>
    <span class="n">visited</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
    <span class="n">topo_order</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_list</span><span class="p">:</span>
        <span class="nf">topo_sort_dfs</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">topo_order</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">topo_order</span>


<span class="k">def</span> <span class="nf">topo_sort_dfs</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">topo_order</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Post-order DFS</span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">visited</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">:</span>
        <span class="nf">topo_sort_dfs</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">topo_order</span><span class="p">)</span>
    <span class="n">topo_order</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

</code></pre></div></div> <h2 id="topo-sort-given-graph">Topo sort given graph</h2> <p>This is used in leetcode problem.</p> <p>Given a graph we use <code class="language-plaintext highlighter-rouge">in_degree</code> to track in degree for each node.</p> <p>Each node whose in degree is 0 is put to output array which means that this node depends on no other nodes.</p> <p>This node is also put to queue to get adjacent nodes to this <code class="language-plaintext highlighter-rouge">in_degree=0</code> node.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;queue&gt;</span><span class="cp">
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">num_node</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">edge_count</span><span class="p">;</span>
    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">num_node</span> <span class="o">&gt;&gt;</span> <span class="n">edge_count</span><span class="p">;</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">graph</span><span class="p">(</span><span class="n">num_node</span><span class="p">);</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">in_degree</span><span class="p">(</span><span class="n">num_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">edge_count</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">in</span><span class="p">,</span> <span class="n">out</span><span class="p">;</span>
        <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">in</span> <span class="o">&gt;&gt;</span> <span class="n">out</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">node_idx</span> <span class="o">=</span> <span class="n">in</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">out_node_idx</span> <span class="o">=</span> <span class="n">out</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span>
        <span class="n">graph</span><span class="p">[</span><span class="n">node_idx</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="n">out_node_idx</span><span class="p">);</span>
        <span class="n">in_degree</span><span class="p">[</span><span class="n">out_node_idx</span><span class="p">]</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">queue</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">node_q</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_node</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span><span class="p">(</span><span class="n">in_degree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">node_q</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">topo_sort</span><span class="p">;</span>
    <span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">node_q</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span>  <span class="p">{</span>
        <span class="kt">int</span> <span class="n">front_node</span> <span class="o">=</span> <span class="n">node_q</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
        <span class="n">node_q</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
        <span class="n">topo_sort</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">front_node</span><span class="p">);</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">out_node</span><span class="o">:</span><span class="n">graph</span><span class="p">[</span><span class="n">front_node</span><span class="p">])</span> <span class="p">{</span>
            <span class="n">in_degree</span><span class="p">[</span><span class="n">out_node</span><span class="p">]</span><span class="o">--</span><span class="p">;</span>
            <span class="k">if</span><span class="p">(</span><span class="n">in_degree</span><span class="p">[</span><span class="n">out_node</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">node_q</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">out_node</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">if</span><span class="p">(</span><span class="n">topo_sort</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">num_node</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_node</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">topo_sort</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">topo_sort</span><span class="p">.</span><span class="n">back</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>



<span class="p">}</span>
<span class="c1">// 64 位输出请用 printf("%lld")</span>

</code></pre></div></div> <h2 id="difference-between-tensorflow-and-pytorch">Difference between tensorflow and pytorch?</h2>]]></content><author><name></name></author><category term="ml"/><category term="framework"/><category term="ml"/><category term="graph"/><category term="algorithm"/><summary type="html"><![CDATA[Topo sort starts from end node]]></summary></entry><entry><title type="html">palindrome substring partition</title><link href="https://bilyz98.github.io/blog/2024/palindrome-substring-partition/" rel="alternate" type="text/html" title="palindrome substring partition"/><published>2024-11-25T11:59:00+00:00</published><updated>2024-11-25T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/palindrome-substring-partition</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/palindrome-substring-partition/"><![CDATA[<p>Had an interview with the team that works on opeartor fustion.</p> <p>Failed this first round interview.</p> <p>I only gave very naive solution. Not really sad.</p> <p>I am well prepared for kinds of outcomes.</p> <p>Problem:</p> <p>given two strings a and b with same length n, you can partition two strings at the same place for two strings into a1, a2 and b1 b2, ie. length of a1 is equal to length of b1 and length of a2 is equal to the length of b2. please give a algorithm to test if there is a partition that can form a concat string such at a1 + b2 or b1 + a2 is palindrome string</p> <p>corresponding leetcode problem:</p> <p>https://leetcode.com/problems/split-two-strings-to-make-palindrome/solutions/888885/c-java-greedy-o-n-o-1/</p> <p>Certainly! Here’s a step-by-step algorithm to test if there is a partition that can form a palindrome by concatenating ( a1 + b2 ) or ( b1 + a2 ):</p> <h3 id="algorithm">Algorithm</h3> <ol> <li><strong>Input</strong>: Two strings <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> of length <code class="language-plaintext highlighter-rouge">n</code>.</li> <li><strong>Output</strong>: A boolean value indicating if there exists a partition point where ( a1 + b2 ) or ( b1 + a2 ) is a palindrome.</li> </ol> <h3 id="steps">Steps:</h3> <ol> <li><strong>Initialization</strong>: <ul> <li>Loop through each possible partition point <code class="language-plaintext highlighter-rouge">i</code> from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">n</code>.</li> </ul> </li> <li><strong>Check Partitions</strong>: <ul> <li>For each partition point <code class="language-plaintext highlighter-rouge">i</code>, split the strings into <code class="language-plaintext highlighter-rouge">a1</code>, <code class="language-plaintext highlighter-rouge">a2</code>, <code class="language-plaintext highlighter-rouge">b1</code>, and <code class="language-plaintext highlighter-rouge">b2</code>. <ul> <li>( a1 = a[0:i] )</li> <li>( a2 = a[i:n] )</li> <li>( b1 = b[0:i] )</li> <li>( b2 = b[i:n] )</li> </ul> </li> </ul> </li> <li><strong>Check Palindromes</strong>: <ul> <li>Concatenate <code class="language-plaintext highlighter-rouge">a1 + b2</code> and check if it’s a palindrome.</li> <li>Concatenate <code class="language-plaintext highlighter-rouge">b1 + a2</code> and check if it’s a palindrome.</li> <li>If either concatenation is a palindrome, return <code class="language-plaintext highlighter-rouge">True</code>.</li> </ul> </li> <li><strong>Return False</strong>: <ul> <li>If no valid palindrome is found after all partitions, return <code class="language-plaintext highlighter-rouge">False</code>.</li> </ul> </li> </ol> <h3 id="pseudocode">Pseudocode</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_palindrome</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">s</span> <span class="o">==</span> <span class="n">s</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">can_form_palindrome</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span> <span class="o">=</span> <span class="n">a</span><span class="p">[:</span><span class="n">i</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span>
        <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">b</span><span class="p">[:</span><span class="n">i</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span>
        <span class="k">if</span> <span class="nf">is_palindrome</span><span class="p">(</span><span class="n">a1</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span> <span class="ow">or</span> <span class="nf">is_palindrome</span><span class="p">(</span><span class="n">b1</span> <span class="o">+</span> <span class="n">a2</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="bp">False</span>

<span class="c1"># Example usage:
</span><span class="n">a</span> <span class="o">=</span> <span class="sh">"</span><span class="s">yourstringa</span><span class="sh">"</span>
<span class="n">b</span> <span class="o">=</span> <span class="sh">"</span><span class="s">yourstringb</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">can_form_palindrome</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</code></pre></div></div> <h3 id="explanation">Explanation</h3> <ul> <li><code class="language-plaintext highlighter-rouge">is_palindrome(s)</code>: This function checks if a given string <code class="language-plaintext highlighter-rouge">s</code> is a palindrome.</li> <li><code class="language-plaintext highlighter-rouge">can_form_palindrome(a, b)</code>: This function iterates through each possible partition point, creates the substrings <code class="language-plaintext highlighter-rouge">a1</code>, <code class="language-plaintext highlighter-rouge">a2</code>, <code class="language-plaintext highlighter-rouge">b1</code>, <code class="language-plaintext highlighter-rouge">b2</code>, and checks if either <code class="language-plaintext highlighter-rouge">a1 + b2</code> or <code class="language-plaintext highlighter-rouge">b1 + a2</code> forms a palindrome.</li> </ul> <p>This algorithm runs in ( O(n^2) ) time complexity because it performs a linear check for palindromes for each of the ( n + 1 ) partition points.</p> <p>Feel free to test this with your specific strings! If you need further refinements or have more questions, I’m here to help! 😊</p> <p>Understood! We can develop a more efficient algorithm to check for a palindrome partition by leveraging additional data structures and dynamic programming principles. Here’s a refined and more efficient approach:</p> <h3 id="efficient-algorithm">Efficient Algorithm</h3> <ol> <li><strong>Input</strong>: Two strings <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> of length <code class="language-plaintext highlighter-rouge">n</code>.</li> <li><strong>Output</strong>: A boolean value indicating if there exists a partition point where ( a1 + b2 ) or ( b1 + a2 ) is a palindrome.</li> </ol> <h3 id="steps-1">Steps:</h3> <ol> <li><strong>Initialization</strong>: <ul> <li>Precompute palindromic substrings for both <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> using a 2D boolean array <code class="language-plaintext highlighter-rouge">P[i][j]</code> which indicates if the substring from index <code class="language-plaintext highlighter-rouge">i</code> to <code class="language-plaintext highlighter-rouge">j</code> is a palindrome.</li> </ul> </li> <li><strong>Precompute Palindromic Substrings</strong>: <ul> <li>Use dynamic programming to fill in the 2D boolean arrays <code class="language-plaintext highlighter-rouge">Pa</code> and <code class="language-plaintext highlighter-rouge">Pb</code> for strings <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>, respectively.</li> </ul> </li> <li><strong>Check Possible Partitions</strong>: <ul> <li>For each partition point <code class="language-plaintext highlighter-rouge">i</code> from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">n</code>: <ul> <li>Check if <code class="language-plaintext highlighter-rouge">a[:i] + b[i:]</code> forms a palindrome by ensuring both substrings are palindromes.</li> <li>Check if <code class="language-plaintext highlighter-rouge">b[:i] + a[i:]</code> forms a palindrome by ensuring both substrings are palindromes.</li> </ul> </li> </ul> </li> <li><strong>Return Result</strong>: <ul> <li>If a valid palindrome partition is found, return <code class="language-plaintext highlighter-rouge">True</code>.</li> <li>If no valid palindrome partition is found after all partitions, return <code class="language-plaintext highlighter-rouge">False</code>.</li> </ul> </li> </ol> <h3 id="pseudocode-1">Pseudocode</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_palindrome</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="o">!=</span> <span class="n">s</span><span class="p">[</span><span class="n">end</span><span class="p">]:</span>
            <span class="k">return</span> <span class="bp">False</span>
        <span class="n">start</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">end</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="bp">True</span>

<span class="k">def</span> <span class="nf">can_form_palindrome</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    
    <span class="c1"># Precompute palindromes for string a
</span>    <span class="n">Pa</span> <span class="o">=</span> <span class="p">[[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">Pa</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">for</span> <span class="n">length</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">a</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="ow">and</span> <span class="p">(</span><span class="n">length</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">Pa</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]):</span>
                <span class="n">Pa</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="c1"># Precompute palindromes for string b
</span>    <span class="n">Pb</span> <span class="o">=</span> <span class="p">[[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">Pb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">for</span> <span class="n">length</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="ow">and</span> <span class="p">(</span><span class="n">length</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">Pb</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]):</span>
                <span class="n">Pb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="c1"># Check for possible partitions
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">Pa</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">n</span> <span class="ow">or</span> <span class="n">Pb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]):</span>
            <span class="k">return</span> <span class="bp">True</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">Pb</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">n</span> <span class="ow">or</span> <span class="n">Pa</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]):</span>
            <span class="k">return</span> <span class="bp">True</span>

    <span class="k">return</span> <span class="bp">False</span>

<span class="c1"># Example usage:
</span><span class="n">a</span> <span class="o">=</span> <span class="sh">"</span><span class="s">yourstringa</span><span class="sh">"</span>
<span class="n">b</span> <span class="o">=</span> <span class="sh">"</span><span class="s">yourstringb</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">can_form_palindrome</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</code></pre></div></div> <h3 id="explanation-1">Explanation</h3> <ul> <li><code class="language-plaintext highlighter-rouge">is_palindrome(s, start, end)</code>: Helper function to check if the substring <code class="language-plaintext highlighter-rouge">s[start:end]</code> is a palindrome.</li> <li><code class="language-plaintext highlighter-rouge">can_form_palindrome(a, b)</code>: Main function to precompute palindromic substrings and check for valid partitions. <ul> <li><strong>Precomputation</strong>: Use dynamic programming to fill in the 2D boolean arrays <code class="language-plaintext highlighter-rouge">Pa</code> and <code class="language-plaintext highlighter-rouge">Pb</code> indicating palindromic substrings for <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>.</li> <li><strong>Partition Check</strong>: For each partition point <code class="language-plaintext highlighter-rouge">i</code>, check if <code class="language-plaintext highlighter-rouge">a[:i] + b[i:]</code> or <code class="language-plaintext highlighter-rouge">b[:i] + a[i:]</code> forms a palindrome by ensuring both substrings are palindromic.</li> </ul> </li> </ul> <p>This approach reduces the time complexity by precomputing the palindromic substrings and performing efficient checks during partitioning.</p> <p>Feel free to test this refined algorithm and let me know if you need further assistance or refinements! 😊</p> <h1 id="simple-greedy-solution">Simple greedy solution</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Solution {
public:
    bool checkPalindromeFormation(string a, string b) {
        return validate(a, b) || validate(b, a);
    }

    bool validate(string &amp;a, string&amp; b) {
       int l=0;
        int r = a.size() -1;
        while(l &lt; r) {
            if(a[l] != b[r]) {
                return checkpalindrome(a, l, r) || checkpalindrome(b, l, r);
            }
            l++;
            r--;
        }
        return true;
    }

    bool checkpalindrome(string&amp;s, int l, int r) {
        while(l &lt; r) {
            if(s[l] != s[r]) return false;
            l++;
            r--;
        }
        return true;
    }
};
</code></pre></div></div> <p>Explanation: <a href="https://leetcode.com/problems/split-two-strings-to-make-palindrome/solutions/891646/java-greedy-explanation-to-o-n-solution/">https://leetcode.com/problems/split-two-strings-to-make-palindrome/solutions/891646/java-greedy-explanation-to-o-n-solution/</a></p>]]></content><author><name></name></author><category term="coding"/><category term="exam"/><category term="leetcode"/><category term="dynamic"/><category term="programming"/><summary type="html"><![CDATA[Had an interview with the team that works on opeartor fustion.]]></summary></entry><entry><title type="html">Speed up matrix multiplication 2</title><link href="https://bilyz98.github.io/blog/2024/matrix-mul-comparison/" rel="alternate" type="text/html" title="Speed up matrix multiplication 2"/><published>2024-11-12T11:59:00+00:00</published><updated>2024-11-12T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/matrix-mul-comparison</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/matrix-mul-comparison/"><![CDATA[<h3 id="benefits-of-tiling">Benefits of Tiling</h3> <p>Reduced Global Memory Accesses:</p> <p>By loading tiles into shared memory, we reduce the number of global memory accesses, which are slower compared to shared memory accesses.</p> <p>Improved Cache Efficiency:</p> <p>Tiling improves cache efficiency by ensuring that data is reused within the shared memory, reducing the need to fetch data from global memory multiple times.</p> <p>Better Utilization of GPU Resources:</p> <p>Tiling allows for better utilization of the GPU’s computational resources by dividing the work into smaller, manageable chunks that fit into the GPU’s shared memory.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kt">void</span> <span class="nf">matrixMulTile</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">width</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">dev_a</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_b</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_c</span><span class="p">;</span>

    <span class="c1">// Allocate device memory</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>

    <span class="c1">// Copy matrices to device memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="n">TILE_WIDTH</span><span class="p">);</span>
  <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">((</span><span class="n">width</span> <span class="o">+</span> <span class="n">TILE_WIDTH</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span> <span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="p">(</span><span class="n">width</span><span class="o">+</span><span class="n">TILE_WIDTH</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span> <span class="n">TILE_WIDTH</span><span class="p">);</span>
  <span class="n">matrixMulTileKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span> <span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>

  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_a</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_b</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_c</span><span class="p">);</span>


<span class="p">}</span>

<span class="cp">#define TILE_WIDTH 16
</span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">matrixMulTileKernel</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">da</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">db</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span><span class="n">dout</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">tile_A</span><span class="p">[</span><span class="n">TILE_WIDTH</span> <span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
  <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">tile_B</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>

  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">width</span><span class="o">+</span><span class="n">TILE_WIDTH</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>

      <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">da</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)];</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">if</span><span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">db</span><span class="p">[(</span><span class="n">i</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>

    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>

    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">value</span> <span class="o">+=</span> <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">tile_B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>


  <span class="p">}</span>

  <span class="n">dout</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>

<span class="p">}</span>
</code></pre></div></div> <p>This code means that we have one thread for each output element in output matrix.</p> <p>Each block of threads cooperatively loads chunks tiles of A and B into shared memory. This is done in a loop to cover all tiles that contribute to final result for the block.</p> <p><a href="https://penny-xu.github.io/blog/tiled-matrix-multiplication">Another post that talk about the tiling matrix multiplication</a></p> <h2 id="each-block-of-thread-would-load-matrix-elements-multiple-times">Each block of thread would load matrix elements multiple times</h2> <h3 id="how-it-works">How It Works</h3> <ol> <li><strong>Tile Loading</strong>: <ul> <li>For each iteration of the loop (<code class="language-plaintext highlighter-rouge">for (int i = 0; i &lt; width / TILE_WIDTH; ++i)</code>), every block loads a new set of tiles from global memory into shared memory.</li> <li>This means that each block loads a tile of matrix <code class="language-plaintext highlighter-rouge">A</code> and a corresponding tile of matrix <code class="language-plaintext highlighter-rouge">B</code> multiple times, once for each tile that contributes to the block’s portion of the output matrix <code class="language-plaintext highlighter-rouge">C</code>.</li> </ul> </li> <li><strong>Computing Partial Results</strong>: <ul> <li>Once the tiles are loaded into shared memory, the threads within the block use them to compute partial results.</li> <li>After computing the partial results, the tiles in shared memory are overwritten by the next set of tiles in the next iteration of the loop.</li> </ul> </li> <li><strong>Accumulating Results</strong>: <ul> <li>The partial results are accumulated across all iterations to compute the final value for each element in the output matrix <code class="language-plaintext highlighter-rouge">C</code>.</li> </ul> </li> </ol> <h3 id="why-this-is-necessary">Why This is Necessary</h3> <ul> <li><strong>Matrix Multiplication</strong>: Matrix multiplication requires summing the products of elements from entire rows of <code class="language-plaintext highlighter-rouge">A</code> and columns of <code class="language-plaintext highlighter-rouge">B</code>. Since the matrices are too large to fit entirely into shared memory, they must be processed in smaller chunks (tiles).</li> <li><strong>Efficiency</strong>: Using tiles and shared memory helps improve the efficiency of these operations by minimizing the slower global memory accesses.</li> </ul> <h3 id="example">Example</h3> <p>For a simplified example, let’s say <code class="language-plaintext highlighter-rouge">TILE_WIDTH</code> is 2 and our matrices are 4x4. Each block processes a 2x2 sub-matrix of the output matrix <code class="language-plaintext highlighter-rouge">C</code>. For each block, the loading process might look like this:</p> <ol> <li><strong>First Iteration</strong>: <ul> <li>Load the first 2x2 tile of <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>.</li> <li>Compute partial results using this tile.</li> </ul> </li> <li><strong>Second Iteration</strong>: <ul> <li>Load the next 2x2 tile of <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>.</li> <li>Add the products to the previously computed partial results.</li> </ul> </li> </ol> <p><img src="https://github.com/user-attachments/assets/a49597da-01cf-4ef3-ae0d-9d797b487b56" alt="tmm-59dd890f48435e692c47919d0df4a5e6"/></p> <p>This process repeats until the entire matrices have been processed. So, each block loads and processes multiple tiles to compute the final output elements.</p> <h2 id="the-whole-matrix-is-loaded-multiple-times-by-all-blocks-of-threads">The whole matrix is loaded multiple times by all blocks of threads</h2> <p>Yes, you’ve got it! The matrix elements are indeed loaded multiple times into shared memory by different blocks during the matrix multiplication process.</p> <h3 id="why-this-happens">Why This Happens</h3> <ul> <li><strong>Tiling</strong>: The entire matrix cannot fit into the limited shared memory available on the GPU. Thus, the matrix is divided into smaller tiles.</li> <li><strong>Multiple Blocks</strong>: Each block processes a portion of the matrix. Since the matrix multiplication operation requires data from different parts of the matrices, these tiles are loaded multiple times into shared memory by different blocks to complete the computation.</li> </ul> <h3 id="process-recap">Process Recap</h3> <ol> <li><strong>Loading Tiles</strong>: Each block loads a tile (a small sub-matrix) of the matrices <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> into shared memory.</li> <li><strong>Processing</strong>: The threads within the block process these tiles to compute partial results for the output matrix <code class="language-plaintext highlighter-rouge">C</code>.</li> <li><strong>Repetition</strong>: This loading and processing is repeated for every required tile until the entire matrix multiplication is complete.</li> </ol> <p>By using tiling and shared memory, the code minimizes the number of slow global memory accesses and maximizes the speed of computations by taking advantage of the faster shared memory.</p> <p>This approach ensures efficient parallel processing, leading to a significant performance boost on the GPU.</p> <h2 id="why-dont-use-threadidxx-as-row-idx-">why don’t use threadIdx.x as row idx ?</h2> <p>I see where you’re coming from, and it’s a valid question! Let’s delve deeper into why <code class="language-plaintext highlighter-rouge">threadIdx.y</code> is used as the row index and not <code class="language-plaintext highlighter-rouge">threadIdx.x</code>.</p> <h3 id="memory-coalescing-and-access-patterns">Memory Coalescing and Access Patterns</h3> <ol> <li> <p><strong>Memory Coalescing</strong>: On GPUs, memory access patterns significantly impact performance. Coalescing refers to combining multiple memory accesses into a single transaction. When threads within a warp (32 threads in modern GPUs) access contiguous memory locations, those accesses can be coalesced.</p> </li> <li> <p><strong>Row-major Order</strong>: In C/C++, arrays are stored in row-major order, meaning the elements of each row are stored in contiguous memory locations. For instance, in a 2D array <code class="language-plaintext highlighter-rouge">A</code>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">A[0][0], A[0][1], A[0][2], ..., A[0][N-1]</code> are stored consecutively in memory.</li> </ul> </li> </ol> <h3 id="access-efficiency">Access Efficiency</h3> <p>When we use <code class="language-plaintext highlighter-rouge">threadIdx.y</code> as the row index and <code class="language-plaintext highlighter-rouge">threadIdx.x</code> as the column index, it ensures that all threads in the same row of a block access contiguous memory locations in the global memory.</p> <h3 id="example-1">Example</h3> <p>Consider a tile size of 4x4 for simplicity. When loading a tile from global memory to shared memory:</p> <ul> <li><strong>Using <code class="language-plaintext highlighter-rouge">threadIdx.y</code> as row index</strong>: <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)];</span>
</code></pre></div> </div> <p>Each row in <code class="language-plaintext highlighter-rouge">tile_A</code> is loaded by threads in a row of the block, accessing contiguous elements in the global memory:</p> <ul> <li>Thread (0,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)]</code></li> <li>Thread (0,1) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 1)]</code></li> <li>Thread (0,2) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 2)]</code></li> <li>Thread (0,3) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 3)]</code></li> </ul> <p>All these accesses can be coalesced into a single transaction.</p> </li> <li><strong>Using <code class="language-plaintext highlighter-rouge">threadIdx.x</code> as row index</strong>: <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)];</span>
</code></pre></div> </div> <p>Now, each column in <code class="language-plaintext highlighter-rouge">tile_A</code> is loaded by threads in a column of the block, accessing non-contiguous elements in the global memory:</p> <ul> <li>Thread (0,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)]</code></li> <li>Thread (1,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)] + width</code></li> <li>Thread (2,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)] + 2 * width</code></li> <li>Thread (3,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)] + 3 * width</code></li> </ul> <p>These accesses are not contiguous and cannot be efficiently coalesced.</p> </li> </ul> <h3 id="summary">Summary</h3> <p>Using <code class="language-plaintext highlighter-rouge">threadIdx.y</code> as the row index and <code class="language-plaintext highlighter-rouge">threadIdx.x</code> as the column index ensures that threads within the same row of a block access contiguous memory locations. This pattern optimizes memory coalescing and significantly improves performance on GPUs.</p> <p>I hope this clarifies why the indices are arranged this way! If you have more questions or need further explanations, feel free to ask. 😊</p> <p>cuda has a limit on the number of threads per block, typically 1024. If width is larger than 32, kernel launch will fail. And we get this error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@ln102%tianhe2-K matmul]$ yhrun -p gpu_v100 ./matmul_cuda
CUDA error: invalid configuration argument
cuda Duration: 345.838 ms
cuda tile Duration: 25.2786 ms
check failed: 0, cuda naive: 0, cuda tile: 2000
yhrun: error: gpu29: task 0: Exited with exit code 1
</code></pre></div></div> <p>Comparison between cpu, cuda native and cuda matrix multiplication with tiling.</p> <p>Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="cp">#define N 2 // Matrix size
#define BLOCK_SIZE 16 // Block size
</span>
<span class="kt">void</span> <span class="nf">matmul_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">input_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">input_row_idx</span><span class="o">&lt;</span> <span class="n">width</span><span class="p">;</span> <span class="n">input_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>  <span class="n">output_col_idx</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">;</span> <span class="n">output_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">element_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">element_idx</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">;</span> <span class="n">element_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="c1">// value += a[input_row_idx][element_idx] * b[element_idx][output_col_idx];</span>
          <span class="n">value</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">input_row_idx</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">element_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">element_idx</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">output_col_idx</span><span class="p">];</span>

      <span class="p">}</span>
      <span class="n">c</span><span class="p">[</span><span class="n">input_row_idx</span><span class="o">*</span><span class="n">width</span><span class="o">+</span><span class="n">output_col_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>
      <span class="c1">// c[input_row_idx][output_col_idx] = value;</span>
    <span class="p">}</span>
  <span class="p">}</span>

<span class="p">}</span>

<span class="c1">// CUDA Kernel for Matrix Multiplication</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">MatrixMul</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">val</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">c</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span> <span class="nf">matmul_cuda_naive</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">width</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">dev_a</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_b</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_c</span><span class="p">;</span>

    <span class="c1">// Allocate device memory</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>

    <span class="c1">// Copy matrices to device memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

    <span class="c1">// Launch kernel</span>

    <span class="n">dim3</span> <span class="n">threadsPerBlock</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">numBlocks</span><span class="p">((</span><span class="n">width</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="p">(</span><span class="n">width</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>
    <span class="c1">// dim3 dimBlock(width, width);</span>
    <span class="c1">// dim3 dimGrid(1, 1);</span>

    <span class="n">MatrixMul</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
    <span class="c1">// MatrixMul&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(dev_a, dev_b, dev_c, width);</span>

    <span class="c1">// Synchronize CPU and GPU</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>

    <span class="c1">// Check for errors</span>
    <span class="n">cudaError_t</span> <span class="n">error</span> <span class="o">=</span> <span class="n">cudaGetLastError</span><span class="p">();</span>
    <span class="k">if</span><span class="p">(</span><span class="n">error</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"CUDA error: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">error</span><span class="p">));</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Copy result back to host memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

    <span class="c1">// Print the result</span>
    <span class="c1">// for (int y = 0; y &lt; N; y++) {</span>
    <span class="c1">//     for (int x = 0; x &lt; N; x++) {</span>
    <span class="c1">//         printf("%d ", c[y][x]);</span>
    <span class="c1">//     }</span>
    <span class="c1">//     printf("\n");</span>
    <span class="c1">// }</span>

    <span class="c1">// Free device memory</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_a</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_b</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_c</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>


<span class="p">}</span>


<span class="kt">void</span> <span class="nf">matrixMulTile</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">width</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">dev_a</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_b</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_c</span><span class="p">;</span>

    <span class="c1">// Allocate device memory</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>

    <span class="c1">// Copy matrices to device memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="n">TILE_WIDTH</span><span class="p">);</span>
  <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">((</span><span class="n">width</span> <span class="o">+</span> <span class="n">TILE_WIDTH</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span> <span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="p">(</span><span class="n">width</span><span class="o">+</span><span class="n">TILE_WIDTH</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span> <span class="n">TILE_WIDTH</span><span class="p">);</span>
  <span class="n">matrixMulTileKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span> <span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>

  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_a</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_b</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_c</span><span class="p">);</span>


<span class="p">}</span>

<span class="cp">#define TILE_WIDTH 16
</span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">matrixMulTileKernel</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">da</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">db</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span><span class="n">dout</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">tile_A</span><span class="p">[</span><span class="n">TILE_WIDTH</span> <span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
  <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">tile_B</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>

  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">width</span><span class="o">+</span><span class="n">TILE_WIDTH</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>

      <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">da</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)];</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">if</span><span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">db</span><span class="p">[(</span><span class="n">i</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>

    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>

    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">value</span> <span class="o">+=</span> <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">tile_B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>


  <span class="p">}</span>

  <span class="n">dout</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>

<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">width</span><span class="p">;</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">h_A</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">h_B</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">h_C</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

  <span class="kt">int</span> <span class="o">*</span><span class="n">hc2</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">size</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="n">h_A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">h_B</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>


      <span class="k">auto</span> <span class="n">start</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
  <span class="n">matmul_cuda_naive</span><span class="p">(</span><span class="n">h_A</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">h_C</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
      <span class="k">auto</span> <span class="n">end</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
      <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">duration</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">milli</span><span class="o">&gt;</span> <span class="n">duration</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">;</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"cuda Duration: "</span> <span class="o">&lt;&lt;</span> <span class="n">duration</span><span class="p">.</span><span class="n">count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">" ms"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>


       <span class="n">start</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
  <span class="n">matrixMulTile</span><span class="p">(</span><span class="n">h_A</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">hc2</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
       <span class="n">end</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
      <span class="n">duration</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">;</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"cuda tile Duration: "</span> <span class="o">&lt;&lt;</span> <span class="n">duration</span><span class="p">.</span><span class="n">count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">" ms"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>


  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span><span class="n">size</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">hc2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">h_C</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">{</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">"check failed: %d, cuda naive: %d, cuda tile: %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">i</span> <span class="p">,</span> <span class="n">h_C</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hc2</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
      <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="n">printf</span><span class="p">(</span><span class="s">"correctness check pass</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>


       <span class="n">start</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
      <span class="n">matmul_cpu</span><span class="p">(</span><span class="n">h_A</span><span class="p">,</span> <span class="n">h_B</span> <span class="p">,</span> <span class="n">hc2</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
       <span class="n">end</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
       <span class="n">duration</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">;</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"cpu Duration: "</span> <span class="o">&lt;&lt;</span> <span class="n">duration</span><span class="p">.</span><span class="n">count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">" ms"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>



  <span class="n">free</span><span class="p">(</span><span class="n">h_A</span><span class="p">);</span> <span class="n">free</span><span class="p">(</span><span class="n">h_B</span><span class="p">);</span> <span class="n">free</span><span class="p">(</span><span class="n">h_C</span><span class="p">);</span>
<span class="p">}</span>

</code></pre></div></div> <p>indexing for da and db in cuda kernal explanations</p> <p>The indexing of matrix elements in the <code class="language-plaintext highlighter-rouge">matrixMulTileKernel</code> function is designed to optimize memory access patterns and improve performance by leveraging shared memory. Let’s break down the indexing for <code class="language-plaintext highlighter-rouge">da</code> and <code class="language-plaintext highlighter-rouge">db</code>:</p> <p>Simply speaking, <code class="language-plaintext highlighter-rouge">i</code> has different meaning for <code class="language-plaintext highlighter-rouge">da</code> and <code class="language-plaintext highlighter-rouge">db</code>, for <code class="language-plaintext highlighter-rouge">da</code> i loop for all columns in the same row. for <code class="language-plaintext highlighter-rouge">db</code> i loops all rows in the same column.</p> <h3 id="indexing-for-da-matrix-a">Indexing for <code class="language-plaintext highlighter-rouge">da</code> (Matrix A)</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">da</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)];</span>
</code></pre></div></div> <ul> <li><strong>Row Calculation</strong>: <code class="language-plaintext highlighter-rouge">row = blockIdx.y * TILE_WIDTH + threadIdx.y</code> <ul> <li><code class="language-plaintext highlighter-rouge">blockIdx.y * TILE_WIDTH</code> gives the starting row index for the block.</li> <li><code class="language-plaintext highlighter-rouge">threadIdx.y</code> gives the row index within the block.</li> </ul> </li> <li><strong>Column Calculation</strong>: <code class="language-plaintext highlighter-rouge">(i * TILE_WIDTH + threadIdx.x)</code> <ul> <li><code class="language-plaintext highlighter-rouge">i * TILE_WIDTH</code> shifts the column index by the tile width for each iteration.</li> <li><code class="language-plaintext highlighter-rouge">threadIdx.x</code> gives the column index within the block.</li> </ul> </li> </ul> <h3 id="indexing-for-db-matrix-b">Indexing for <code class="language-plaintext highlighter-rouge">db</code> (Matrix B)</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">db</span><span class="p">[(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
</code></pre></div></div> <ul> <li><strong>Row Calculation</strong>: <code class="language-plaintext highlighter-rouge">(i * TILE_WIDTH + threadIdx.y)</code> <ul> <li><code class="language-plaintext highlighter-rouge">i * TILE_WIDTH</code> shifts the row index by the tile width for each iteration.</li> <li><code class="language-plaintext highlighter-rouge">threadIdx.y</code> gives the row index within the block.</li> </ul> </li> <li><strong>Column Calculation</strong>: <code class="language-plaintext highlighter-rouge">col = blockIdx.x * TILE_WIDTH + threadIdx.x</code> <ul> <li><code class="language-plaintext highlighter-rouge">blockIdx.x * TILE_WIDTH</code> gives the starting column index for the block.</li> <li><code class="language-plaintext highlighter-rouge">threadIdx.x</code> gives the column index within the block.</li> </ul> </li> </ul> <h3 id="why-this-indexing">Why This Indexing?</h3> <ul> <li><strong>Memory Coalescing</strong>: Using <code class="language-plaintext highlighter-rouge">threadIdx.x</code> for columns and <code class="language-plaintext highlighter-rouge">threadIdx.y</code> for rows ensures that threads within a warp access contiguous memory locations. This pattern optimizes memory coalescing, which combines multiple memory accesses into a single transaction, significantly improving performance.</li> <li><strong>Efficient Tiling</strong>: By dividing the matrices into smaller tiles that fit into shared memory, the kernel reduces the number of global memory accesses. Each tile is loaded multiple times, but the overall number of accesses to global memory is minimized, leading to better performance.</li> </ul> <p>Test matrix size is (2000, 2000) cuda naive reduce run time by 54x compared to cpu. cuda tiling reduce run time by 17x compared to cuda naive.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@ln102%tianhe2-K matmul]$ yhrun -p gpu_v100 ./matmul_cuda
cuda Duration: 432.435 ms
cuda tile Duration: 25.7345 ms
cpu Duration: 23042.4 ms
correctness check pass
</code></pre></div></div> <p>The benefit of tiling is that it reduces number of access to global shared memory.</p> <blockquote> <p>No, tiling not just replaces random accesses with sequential ones. It actually saves tons of bandwidth to global memory.</p> </blockquote> <blockquote> <p>Let’s say we multiply two large square matrices of size S×S, where S is a multiple of 32. Obviously, the result is also a square matrix of size S×S.</p> </blockquote> <blockquote> <p>With naïve algorithm, to compute each element of the result, we gonna need to fetch S elements from both matrices. The output matrix has S^2 elements, therefore the total count of loaded elements is 2*S^3.</p> </blockquote> <blockquote> <p>With 32×32 tiling, to compute each 32×32 tile of the result, we gonna need to fetch S/32 tiles from both matrices. The output size in tiles is (S/32)^2, the total count of loaded tiles is 2<em>(S/32)^3. Each 32×32 tile contains 32^2 elements, the total count of loaded elements is therefore (32^2)</em>2<em>(S/32)^3 = (2/32)</em>S^3. Therefore, the tiling reduced global memory bandwidth by the factor of 32, which is a huge performance win.</p> </blockquote>]]></content><author><name></name></author><category term="ml"/><category term="ai"/><category term="ml"/><category term="matrix"/><category term="multiplication"/><summary type="html"><![CDATA[Benefits of Tiling Reduced Global Memory Accesses:]]></summary></entry></feed>