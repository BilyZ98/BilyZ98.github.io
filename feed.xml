<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bilyz98.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bilyz98.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-30T13:16:27+00:00</updated><id>https://bilyz98.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">nano-gpt and Transformer</title><link href="https://bilyz98.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="nano-gpt and Transformer"/><published>2024-06-28T11:59:00+00:00</published><updated>2024-06-28T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/transformer</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/transformer/"><![CDATA[<h3 id="vanilla-bigram-model-without-self-attention">Vanilla bigram model without self attention</h3> <p>As mentioned in the youtube video <a href="https://youtu.be/kCc8FmEb1nY?t=2509">https://youtu.be/kCc8FmEb1nY?t=2509</a>, this code builds a bigram model without self attention. We can use this as baseline to compare with self attention code .</p> <p>Issue: can not use torch cuda module even though I have gpu and install cuda pytorch Solution: Tried to install pytorch cuda again Get this error when I tried to install pytroch-cuda:12.1</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ClobberError: This transaction has incompatible packages due to a shared path.
  packages: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64::jpeg-9e-h5eee18b_1, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'share/man/man1/rdjpgcom.1'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64::jpeg-9e-h5eee18b_1, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'share/man/man1/wrjpgcom.1'


</code></pre></div></div> <p>Solution: Switch to new environment and reinstall pytorch with cuda</p> <p>Comparison of cpu and gpu for bigram model CPU:</p> <p>GPU:</p> <h2 id="transformer-architecture">Transformer architecture</h2>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="llm"/><summary type="html"><![CDATA[llm]]></summary></entry><entry><title type="html">pytorch tensor.to</title><link href="https://bilyz98.github.io/blog/2024/pytorch/" rel="alternate" type="text/html" title="pytorch tensor.to"/><published>2024-06-28T11:59:00+00:00</published><updated>2024-06-28T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/pytorch</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/pytorch/"><![CDATA[<h2 id="move-tensor-to-cuda-with-different-name-returned">Move tensor to cuda with different name returned</h2> <p>Question: can I still use model to do training when I have following code ? In this code, I assign m to tensor on cuda. But I am not sure whether I can still use model to do back propagation</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">BigramLanguageModel</span><span class="p">()</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

</code></pre></div></div> <p>Yes, you can still use the <code class="language-plaintext highlighter-rouge">model</code> for training and backpropagation. When you do <code class="language-plaintext highlighter-rouge">m = model.to(device)</code>, it doesn’t create a new model. Instead, it moves the <code class="language-plaintext highlighter-rouge">model</code> to the specified <code class="language-plaintext highlighter-rouge">device</code> (which is presumably a CUDA device in your case) and returns a reference to the same model. So, <code class="language-plaintext highlighter-rouge">m</code> and <code class="language-plaintext highlighter-rouge">model</code> are actually the same model, just accessed through different names.</p> <p>Here’s an example of how you might use it:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">BigramLanguageModel</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move model to device
</span>
<span class="c1"># define loss function and optimizer
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="c1"># training loop
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">targets</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># forward pass
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="c1"># backward pass and optimization
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p>In this code, <code class="language-plaintext highlighter-rouge">model</code> is used for both the forward pass and the backward pass, and it works correctly because <code class="language-plaintext highlighter-rouge">model</code> and <code class="language-plaintext highlighter-rouge">m</code> are the same model. The key point is that the model parameters need to be on the same device as the data and targets when you perform the forward and backward passes. That’s why <code class="language-plaintext highlighter-rouge">inputs</code> and <code class="language-plaintext highlighter-rouge">targets</code> are also moved to <code class="language-plaintext highlighter-rouge">device</code> before the forward pass.</p> <p>So, in your case, you can continue to use <code class="language-plaintext highlighter-rouge">model</code> in your code for training and backpropagation, even after calling <code class="language-plaintext highlighter-rouge">model.to(device)</code>. Just make sure to move your inputs and targets to the same device before you use them with your model. If you want to use <code class="language-plaintext highlighter-rouge">m</code> instead of <code class="language-plaintext highlighter-rouge">model</code> after the <code class="language-plaintext highlighter-rouge">to(device)</code> call, that’s fine too. It’s really just a matter of which name you prefer to use. They both refer to the same underlying model.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="cuda"/><summary type="html"><![CDATA[pytorch]]></summary></entry><entry><title type="html">Install neovim with old glibc</title><link href="https://bilyz98.github.io/blog/2024/install-neovim/" rel="alternate" type="text/html" title="Install neovim with old glibc"/><published>2024-06-25T11:59:00+00:00</published><updated>2024-06-25T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/install-neovim</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/install-neovim/"><![CDATA[<p>Issue: neovim can not load libc.so.6 which requires glibc_2.28 which is higher than installed glibc version.</p> <p>After asking bing chat and find out that I need to install latest glibc version with sudo which I can’t I decided to clone neovim and compile locally.</p> <p>glibc is a system level package.</p> <p>Previsou release that requires higher glibc version</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln101%tianhe2-K nvim-linux64]$ ./bin/nvim --version
./bin/nvim: /lib64/libc.so.6: version `GLIBC_2.28' not found (required by ./bin/nvim)                                                                                   ./bin/nvim: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by ./bin/nvim)                                                                                   (llmc) [nsccgz_qylin_1@ln101%tianhe2-K nvim-linux64]$ ldd ./bin/nvim
./bin/nvim: /lib64/libc.so.6: version `GLIBC_2.28' not found (required by ./bin/nvim)                                                                                   ./bin/nvim: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by ./bin/nvim)                                                                                           linux-vdso.so.1 =&gt;  (0x00007ffc5e491000)
        libm.so.6 =&gt; /lib64/libm.so.6 (0x00002ae00f451000)
        libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00002ae00f753000)
        libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00002ae00f957000)
        libgcc_s.so.1 =&gt; /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/libgcc_s.so.1 (0x00002ae00eb83000)                                                                      libc.so.6 =&gt; /lib64/libc.so.6 (0x00002ae00fb73000)
        /lib64/ld-linux-x86-64.so.2 (0x00002ae00eb4a000)
        libutil.so.1 =&gt; /lib64/libutil.so.1 (0x00002ae00ff40000)
</code></pre></div></div> <p>I installed this release which does not require glibc.2.31 and now I can run it successfully.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>https://github.com/neovim/neovim-releases/releases
</code></pre></div></div> <p>current one</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln101%tianhe2-K nvim-linux64]$ ldd ./bin/nvim
        linux-vdso.so.1 =&gt;  (0x00007ffedf36c000)
        libm.so.6 =&gt; /lib64/libm.so.6 (0x00002ba2ec6ee000)                                                                                                                      libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00002ba2ec9f0000)
        librt.so.1 =&gt; /lib64/librt.so.1 (0x00002ba2ecbf4000)
        libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00002ba2ecdfc000)
        libgcc_s.so.1 =&gt; /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/libgcc_s.so.1 (0x00002ba2ebc2a000)
        libc.so.6 =&gt; /lib64/libc.so.6 (0x00002ba2ed018000)                                                                                                                      /lib64/ld-linux-x86-64.so.2 (0x00002ba2ebbf1000)
        libutil.so.1 =&gt; /lib64/libutil.so.1 (0x00002ba2ed3e5000)
</code></pre></div></div> <p>It’s too time consuming to build the neovim from scratch.</p> <h2 id="config-path-for-neovim-to-start-with-different-config">.config path for neovim to start with different config</h2> <p>by default, Neovim looks for configuration files in the <code class="language-plaintext highlighter-rouge">~/.config/nvim</code> directory. However, when you use the <code class="language-plaintext highlighter-rouge">NVIM_APPNAME</code> environment variable, Neovim will look for the configuration in the <code class="language-plaintext highlighter-rouge">~/.config/{NVIM_APPNAME}</code> directory instead.</p> <p>So, if you set <code class="language-plaintext highlighter-rouge">NVIM_APPNAME=lunarvim</code>, Neovim will look for the configuration files in the <code class="language-plaintext highlighter-rouge">~/.config/lunarvim</code> directory. Similarly, if <code class="language-plaintext highlighter-rouge">NVIM_APPNAME=nvchad</code>, it will look in the <code class="language-plaintext highlighter-rouge">~/.config/nvchad</code> directory.</p> <p>This allows you to have multiple separate configurations that you can switch between just by changing the <code class="language-plaintext highlighter-rouge">NVIM_APPNAME</code> environment variable. It’s a powerful feature for managing multiple Neovim configurations.</p> <p>Remember to clone or place your desired configurations into the respective directories under <code class="language-plaintext highlighter-rouge">~/.config/</code>. For example, if you’re using LunarVim and NvChad, you should have <code class="language-plaintext highlighter-rouge">~/.config/lunarvim</code> and <code class="language-plaintext highlighter-rouge">~/.config/nvchad</code> directories, each containing the respective configuration files.</p> <p>If you want to use the default configuration, you can just run <code class="language-plaintext highlighter-rouge">nvim</code> without setting the <code class="language-plaintext highlighter-rouge">NVIM_APPNAME</code> environment variable, and it will look for the configuration files in the default <code class="language-plaintext highlighter-rouge">~/.config/nvim</code> directory.</p> <p>Please note that the <code class="language-plaintext highlighter-rouge">~/.config</code> directory is a standard for user-specific application configuration files on Unix-like operating systems. It’s defined by the <a href="https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html">XDG Base Directory Specification</a>. If you want to use a different directory, you’ll need to change the <code class="language-plaintext highlighter-rouge">XDG_CONFIG_HOME</code> environment variable, which defaults to <code class="language-plaintext highlighter-rouge">~/.config</code>.</p>]]></content><author><name></name></author><category term="vim"/><category term="vim"/><summary type="html"><![CDATA[vim]]></summary></entry><entry><title type="html">llm.c</title><link href="https://bilyz98.github.io/blog/2024/llm-c/" rel="alternate" type="text/html" title="llm.c"/><published>2024-06-22T11:59:00+00:00</published><updated>2024-06-22T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/llm-c</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/llm-c/"><![CDATA[<p>roadmap</p> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Running llm.c</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Running llm.c with cuda</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Multiple gpus</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Inference with fp16</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Inference with vllm</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>try other inference acceleartion tech</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Submit pr to check existence of openmpi by specifying openmpi path. ( can use conda as example)</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Submit pr to check curl result when proxy server returns 503 error.</li> </ul> <h2 id="running-llmc">Running llm.c</h2> <p><a href="https://github.com/karpathy/llm.c">https://github.com/karpathy/llm.c</a></p> <h3 id="vanilla-bigram-model-without-self-attention">Vanilla bigram model without self attention</h3> <p>As mentioned in the youtube video <a href="https://youtu.be/kCc8FmEb1nY?t=2509">https://youtu.be/kCc8FmEb1nY?t=2509</a>, this code builds a bigram model without self attention. We can use this as baseline to compare with self attention code .</p> <p>Issue: can not use torch cuda module even though I have gpu and install cuda pytorch Solution: Tried to install pytorch cuda again Get this error when I tried to install pytroch-cuda:12.1</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ClobberError: This transaction has incompatible packages due to a shared path.
  packages: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64::jpeg-9e-h5eee18b_1, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'share/man/man1/rdjpgcom.1'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64::jpeg-9e-h5eee18b_1, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'share/man/man1/wrjpgcom.1'


</code></pre></div></div> <p>Solution: Switch to new environment and reinstall pytorch with cuda</p> <p>Comparison of cpu and gpu for bigram model CPU:</p> <p>GPU:</p> <h3 id="gpu">GPU</h3> <p>Had issue running gpu There is only cuda 11.2 on my machine but torch 2.1.0 is installed which requires cuda 12.0</p> <p>Solution: Manually specify torch==1.3.1</p> <p>Get error</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yhrun <span class="nt">-n</span> 4 <span class="nt">-p</span> gpu_v100 python train_gpt2.py
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
yhrun: error: gpu55: tasks 0-3: Exited with exit code 1
</code></pre></div></div> <p>The issue is that nullcontext is introduced in python &gt;=3.7 So I need to upgrade python version</p> <p>Still can not solve problem above because I can’t not import new module to existing module list.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Currently Loaded Modulefiles:
 1) proxy/1.0   2) CUDA/10.0   3) cudnn/7.6.4-CUDA10.0   4) PyTorch/1.2.0-CUDA10.0-py3.6

 $ yhrun -n 4 -p gpu_v100 python train_gpt2.py
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
</code></pre></div></div> <p>My friend told me that I can just use conda to create new namespace and then I can ssh to the compute node and activate the conda environment. And then I can run training process.</p> <p>This means that compute node shares the same file system with login node. But the operating system is different. Because each node has its own hostname.</p> <p>Learn new thing every day.</p> <p>Here’s all available nodes I have.</p> <p>Karpathy has updated gpt2 parameter download script so now I can download parameter via shell script</p> <p>Issue: Can not connect to huggingface todownload pretrained model via proxy</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln102%tianhe2-K llm.c]$ curl -v https://huggingface.co
* About to connect() to proxy 10.20.18.21 port 3128 (#0)
*   Trying 10.20.18.21...
* Connected to 10.20.18.21 (10.20.18.21) port 3128 (#0)
* Establish HTTP proxy tunnel to huggingface.co:443
&gt; CONNECT huggingface.co:443 HTTP/1.1
&gt; Host: huggingface.co:443
&gt; User-Agent: curl/7.29.0
&gt; Proxy-Connection: Keep-Alive
&gt;
&lt; HTTP/1.1 503 Service Unavailable
&lt; Proxy-Agent: gost/2.11.1
&lt; Content-Length: 0
&lt;
* Received HTTP code 503 from proxy after CONNECT
* Connection #0 to host 10.20.18.21 left intact
curl: (56) Received HTTP code 503 from proxy after CONNECT
</code></pre></div></div> <p>Solution: I decide to download on my local laptop and then upload these model parameter files to gpu nodes.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod </span>u+x ./dev/download_starter_pack.sh
./dev/download_starter_pack.sh
make train_gpt2fp32cu
./train_gpt2fp32cu
</code></pre></div></div> <p>cuda env:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Currently Loaded Modulefiles:
 1) proxy/1.0   2) python/3.6.7_anaconda3   3) CUDA/11.2   4) gmp/4.2.4   5) mpfr/2.4.2   6) mpc/0.8.1   7) gcc/9.2.0
</code></pre></div></div> <p>Output :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step   61/74: train loss 3.213066 (312.014672 ms, 13127 tok/s)
step   62/74: train loss 3.450736 (314.262273 ms, 13033 tok/s)
step   63/74: train loss 3.370245 (315.130342 ms, 12997 tok/s)
step   64/74: train loss 3.407992 (316.778140 ms, 12930 tok/s)
step   65/74: train loss 3.580323 (315.324538 ms, 12989 tok/s)
step   66/74: train loss 3.029552 (317.274858 ms, 12909 tok/s)
step   67/74: train loss 3.296448 (317.588671 ms, 12897 tok/s)
step   68/74: train loss 3.675703 (314.929981 ms, 13006 tok/s)
step   69/74: train loss 3.297087 (313.282229 ms, 13074 tok/s)
step   70/74: train loss 3.646337 (315.271277 ms, 12991 tok/s)
step   71/74: train loss 3.566427 (316.123225 ms, 12956 tok/s)
step   72/74: train loss 3.732521 (315.446478 ms, 12984 tok/s)
step   73/74: train loss 3.825229 (318.325142 ms, 12867 tok/s)
step   74/74: train loss 3.380326 (318.066751 ms, 12877 tok/s)
val loss 3.491223
generating:
---
BUCKINGHAM:
But of my penitent ambition
Rome Slicom against Reimy, justice about him!
In case the witness should speak with joy:
Shall now that by these dwelling House,
Suspicions are declaim'd of the Albanian king.
Go
---
total average iteration time: 312.354733 ms
</code></pre></div></div> <h3 id="multiple-gpus">Multiple GPUs</h3> <p>Run with MPI. Don’t know mpi works internally but I will just start using it to train model.</p> <p>I will learn the internals later.</p> <p>Now I just login to gpu node and run the following command</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make train_gpt2cu
mpirun <span class="nt">-np</span> &lt;number of GPUs&gt; ./train_gpt2cu
</code></pre></div></div> <p>Issue: failed to compile with openmpi I used hpc cluster which has openmpi library installed in directory that is different from standard directory.</p> <p>Here’s Makefile in llm.c</p> <div class="language-makefile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">ifeq</span> <span class="nv">($(NO_MULTI_GPU), 1)</span>
  <span class="err">$(info</span> <span class="err">→</span> <span class="err">Multi-GPU</span> <span class="err">(OpenMPI</span> <span class="err">+</span> <span class="err">NCCL)</span> <span class="err">is</span> <span class="err">manually</span> <span class="err">disabled)</span>
<span class="k">else</span>
  <span class="k">ifneq</span> <span class="nv">($(OS), Windows_NT)</span>
    <span class="c"># Detect if running on macOS or Linux
</span>    <span class="k">ifeq</span> <span class="nv">($(SHELL_UNAME), Darwin)</span>
      <span class="err">$(info</span> <span class="err">✗</span> <span class="err">Multi-GPU</span> <span class="err">on</span> <span class="err">CUDA</span> <span class="err">on</span> <span class="err">Darwin</span> <span class="err">is</span> <span class="err">not</span> <span class="err">supported,</span> <span class="err">skipping</span> <span class="err">OpenMPI</span> <span class="err">+</span> <span class="err">NCCL</span> <span class="err">support)</span>
    <span class="err">else</span> <span class="k">ifeq</span> <span class="nv">($(shell [ -d /usr/lib/x86_64-linux-gnu/openmpi/lib/ ] &amp;&amp; [ -d /usr/lib/x86_64-linux-gnu/openmpi/include/ ] &amp;&amp; echo "exists"), exists)</span>
      <span class="err">$(info</span> <span class="err">✓</span> <span class="err">OpenMPI</span> <span class="err">found,</span> <span class="err">OK</span> <span class="err">to</span> <span class="err">train</span> <span class="err">with</span> <span class="err">multiple</span> <span class="err">GPUs)</span>
      <span class="nv">NVCC_INCLUDES</span> <span class="o">+=</span> <span class="nt">-I</span>/usr/lib/x86_64-linux-gnu/openmpi/include
      <span class="nv">NVCC_LDFLAGS</span> <span class="o">+=</span> <span class="nt">-L</span>/usr/lib/x86_64-linux-gnu/openmpi/lib/
      <span class="nv">NVCC_LDLIBS</span> <span class="o">+=</span> <span class="nt">-lmpi</span> <span class="nt">-lnccl</span>
      <span class="nv">NVCC_FLAGS</span> <span class="o">+=</span> <span class="nt">-DMULTI_GPU</span>
    <span class="k">else</span>
      <span class="err">$(info</span> <span class="err">✗</span> <span class="err">OpenMPI</span> <span class="err">is</span> <span class="err">not</span> <span class="err">found,</span> <span class="err">disabling</span> <span class="err">multi-GPU</span> <span class="err">support)</span>
      <span class="err">$(info</span> <span class="err">---&gt;</span> <span class="err">On</span> <span class="err">Linux</span> <span class="err">you</span> <span class="err">can</span> <span class="err">try</span> <span class="err">install</span> <span class="err">OpenMPI</span> <span class="err">with</span> <span class="err">`sudo</span> <span class="err">apt</span> <span class="err">install</span> <span class="err">openmpi-bin</span> <span class="err">openmpi-doc</span> <span class="err">libopenmpi-dev`)</span>
    <span class="k">endif</span>
  <span class="k">endif</span>
<span class="k">endif</span>
</code></pre></div></div> <p>It checks existence of openmpi library in <code class="language-plaintext highlighter-rouge">/usr/lib/x86_64-linux-gnu/openmpi/lib/</code> Openmpi library is at ` ~/local/lib/` in my hpc cluster. Should I raise a pr?</p> <p>Issue： Get compilation error when linking nccl</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/GPUFS/app_GPU/compiler/CUDA/11.2.0/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 -DMULTI_GPU -DENABLE_BF16 train_gpt2.cu -lcublas -lcublasLt -L~/local/lib/  -I~/local/include/   -lmpi -lnccl -o train_gpt2cu
llmc/zero.cuh(28): error: identifier "ncclBfloat16" is undefined

llmc/zero.cuh(209): error: identifier "ncclAvg" is undefined

llmc/zero.cuh(219): error: identifier "ncclAvg" is undefined

3 errors detected in the compilation of "train_gpt2.cu".
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>I have load nccl module but I still get this error and I don’t know how to fix it. Try to compile train_gpt2fp32cu</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load  CUDA/11.2
module load  gcc/9.2.0
<span class="c">#module load  openmpi/1.10.2-pgi-17.1</span>
module load openmpi/3.1.4-icc-18.0.1
module load  nccl/2.9.9-1-cuda-11.0
module list
which nvcc
<span class="nb">pushd </span>llm.c
<span class="c">#make train_gpt2cu</span>
make train_gpt2fp32cu
mpirun <span class="nt">-np</span> 2 ./train_gpt2fp32cu
<span class="nb">popd</span>

</code></pre></div></div> <p>Get out of memory error when running</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 74                                                 |
| val_num_batches       | 8                                                  |
+-----------------------+----------------------------------------------------+
allocated 474 MiB for model parameters
| train_num_batches     | 74                                                 |
| val_num_batches       | 8                                                  |
+-----------------------+----------------------------------------------------+
allocated 474 MiB for model parameters
allocated 5706 MiB for activations
allocated 5706 MiB for activations
val loss 4.513921
val loss 4.513921
allocated 474 MiB for parameter gradients
allocated 252 MiB for activation gradients
allocated 474 MiB for AdamW optimizer state m
allocated 474 MiB for AdamW optimizer state v
allocated 474 MiB for parameter gradients
allocated 252 MiB for activation gradients
[CUDA ERROR] at file train_gpt2_fp32.cu:1443:
out of memory
</code></pre></div></div> <p>I am not famaliar with how cuda can work with multiple gpus when doing training.</p> <p>Should I learn a little bit more about how can I use multiple gpus to do computation when working with cuda?</p> <p>Why do we have to use mpi to run with multiple gpus?</p> <p>Let’s check whether single gpu code actually uses single gpu or not.</p> <p>There’s only one gpu running when training with single gpu. And it works pretty well. So I want to know how to use multiple gpus to train model.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) [nsccgz_qylin_1@gpu29%tianhe2-K zt]$ nvidia-smi
Fri Jun 21 12:17:35 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:8A:00.0 Off |                    0 |
| N/A   62C    P0   276W / 300W |   8354MiB / 16160MiB |     98%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   32C    P0    38W / 300W |      3MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   31C    P0    37W / 300W |      2MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   31C    P0    37W / 300W |      3MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A    153170      C   ./train_gpt2fp32cu               8351MiB |
+-----------------------------------------------------------------------------+
</code></pre></div></div> <p>Here’s the code that initializes multi gpu training config in train_gpt2.cu</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MultiGpuConfig</span> <span class="nf">multi_gpu_config_init</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">***</span><span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
<span class="cp">#ifdef MULTI_GPU
</span>    <span class="c1">// Initialize MPI.</span>
    <span class="n">MultiGpuConfig</span> <span class="n">result</span><span class="p">;</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Init</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">));</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">process_rank</span><span class="p">));</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">num_processes</span><span class="p">));</span>
    <span class="n">result</span><span class="p">.</span><span class="n">local_device_idx</span> <span class="o">=</span> <span class="n">multi_gpu_get_local_device_idx</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">process_rank</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">num_processes</span><span class="p">);</span>
    <span class="n">cudaCheck</span><span class="p">(</span><span class="n">cudaSetDevice</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">local_device_idx</span><span class="p">));</span>
    <span class="n">ncclUniqueId</span> <span class="n">nccl_id</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">process_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">ncclCheck</span><span class="p">(</span><span class="n">ncclGetUniqueId</span><span class="p">(</span><span class="o">&amp;</span><span class="n">nccl_id</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Bcast</span><span class="p">((</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">nccl_id</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">nccl_id</span><span class="p">),</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">));</span>
    <span class="n">ncclCheck</span><span class="p">(</span><span class="n">ncclCommInitRank</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">nccl_comm</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">num_processes</span><span class="p">,</span> <span class="n">nccl_id</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">process_rank</span><span class="p">));</span>
    <span class="n">cudaCheck</span><span class="p">(</span><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">nccl_stream</span><span class="p">));</span>
    <span class="c1">// event without timing for maximum performance</span>
    <span class="n">cudaCheck</span><span class="p">(</span><span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">compute_nccl_sync</span><span class="p">,</span> <span class="n">cudaEventDisableTiming</span><span class="p">));</span>
    <span class="n">nvtxNameCudaStreamA</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">nccl_stream</span><span class="p">,</span> <span class="s">"nccl stream"</span><span class="p">);</span>
    <span class="n">nvtxNameCudaEventA</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">compute_nccl_sync</span><span class="p">,</span> <span class="s">"nccl compute sync"</span><span class="p">);</span>

</code></pre></div></div> <p>Ask bingchat to explain this code This code is for initializing a multi-GPU configuration using MPI (Message Passing Interface), CUDA, and NCCL (NVIDIA Collective Communications Library). Here’s a breakdown of what the code does:</p> <ol> <li> <p><code class="language-plaintext highlighter-rouge">MultiGpuConfig result;</code>: This line initializes a <code class="language-plaintext highlighter-rouge">MultiGpuConfig</code> object named <code class="language-plaintext highlighter-rouge">result</code>.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Init(argc, argv));</code>: This line initializes the MPI environment.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Comm_rank(MPI_COMM_WORLD, &amp;result.process_rank));</code>: This line determines the rank of the calling process in the communicator.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Comm_size(MPI_COMM_WORLD, &amp;result.num_processes));</code>: This line determines the number of processes in the communicator.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">result.local_device_idx = multi_gpu_get_local_device_idx(result.process_rank, result.num_processes);</code>: This line gets the local device index based on the process rank and the number of processes.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cudaCheck(cudaSetDevice(result.local_device_idx));</code>: This line sets the device to be used for GPU executions.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">ncclUniqueId nccl_id;</code>: This line declares a unique NCCL ID.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">if (result.process_rank == 0) { ncclCheck(ncclGetUniqueId(&amp;nccl_id)); }</code>: This line generates a new NCCL unique ID and assigns it to <code class="language-plaintext highlighter-rouge">nccl_id</code> if the process rank is 0.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Bcast((void *)&amp;nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD));</code>: This line broadcasts the NCCL unique ID from the root process (process with rank 0) to all other processes in the communicator.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">ncclCheck(ncclCommInitRank(&amp;result.nccl_comm, result.num_processes, nccl_id, result.process_rank));</code>: This line initializes the NCCL communicator according to the rank, the number of processes, and the unique NCCL ID.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cudaCheck(cudaStreamCreate(&amp;result.nccl_stream));</code>: This line creates a new asynchronous stream.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cudaCheck(cudaEventCreate(&amp;result.compute_nccl_sync, cudaEventDisableTiming));</code>: This line creates an event with no timing data. This is used for synchronization purposes.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nvtxNameCudaStreamA(result.nccl_stream, "nccl stream");</code>: This line names the CUDA stream “nccl stream” for profiling and debugging purposes.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nvtxNameCudaEventA(result.compute_nccl_sync, "nccl compute sync");</code>: This line names the CUDA event “nccl compute sync” for profiling and debugging purposes.</p> </li> </ol> <p>This function is designed to be used in a multi-GPU environment where multiple processes are running in parallel, each possibly on a different GPU.</p> <p>The use of MPI allows these processes to communicate with each other, and the use of NCCL allows them to collectively perform operations such as reductions and broadcasts.</p> <p>The CUDA streams and events are used for managing and synchronizing the GPU computations.</p> <p>The NVTX calls are used for profiling and debugging the GPU code.</p> <p>Note that this code assumes that the <code class="language-plaintext highlighter-rouge">MULTI_GPU</code> macro is defined; if it is not, this function will not do anything.</p> <p>Is there any way to run with multi-gpus without callling like this ?</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make train_gpt2cu
mpirun <span class="nt">-np</span> &lt;number of GPUs&gt; ./train_gpt2cu
</code></pre></div></div> <p>So now I will try to run very simple mpi and cuda program to test cuda and mpi env on my hpc cluster.</p> <pre><code class="language-cu">#include &lt;stdio.h&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;mpi.h&gt;

__global__ void helloFromGPU(void) {
    printf("Hello World from GPU %d!\n", blockIdx.x);
}

int main(int argc, char** argv) {
    // Initialize the MPI environment
    MPI_Init(&amp;argc, &amp;argv);

    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);

    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);

    // Set the device to the rank of the current MPI process
    cudaSetDevice(world_rank);

    printf("Hello World from CPU of MPI process %d!\n", world_rank);

    // Launch the kernel on the GPU
    helloFromGPU&lt;&lt;&lt;world_size, 1&gt;&gt;&gt;();

    // Wait for GPU to finish before accessing on host
    cudaDeviceSynchronize();

    // Finalize the MPI environment.
    MPI_Finalize();
}

~                                                                                                                                                                                   ~                                                                                                                                                                                   ~
</code></pre> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) $ nvcc -I/GPUFS/nsccgz_qylin_1/local/include -L/GPUFS/nsccgz_qylin_1/local/lib  -lmpi hello_mpi_cuda.cu -o hello_mpi_cuda
(base) $ ls
hello.cu  hello_cuda  hello_mpi  hello_mpi.c  hello_mpi_cuda  hello_mpi_cuda.cu
(base) $ mpirun -np 2 ./hello_mpi_cuda
--------------------------------------------------------------------------
WARNING: No preset parameters were found for the device that Open MPI
detected:

  Local host:            gpu29
  Device name:           mlx5_9
  Device vendor ID:      0x02c9
  Device vendor part ID: 4116

Default device parameters will be used, which may result in lower
performance.  You can edit any of the files specified by the
btl_openib_device_param_files MCA parameter to set values for your
device.

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_no_device_params_found to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   gpu29
  Local device: mlx5_9
--------------------------------------------------------------------------
Hello World from CPU of MPI process 0!
Hello World from CPU of MPI process 1!
Hello World from GPU 0!
Hello World from GPU 1!
Hello World from GPU 0!
Hello World from GPU 1!
[gpu29:229661] 1 more process has sent help message help-mpi-btl-openib.txt / no device params found
[gpu29:229661] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[gpu29:229661] 1 more process has sent help message help-mpi-btl-openib.txt / error in device init
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">nvcc</code> is just like <code class="language-plaintext highlighter-rouge">gcc</code> which is used to compile cuda code.</p> <p>What is <code class="language-plaintext highlighter-rouge">nccl</code>? Basically <code class="language-plaintext highlighter-rouge">nccl</code> is just like <code class="language-plaintext highlighter-rouge">mpi</code>. It’s used to do collective communication between gpus.</p> <p>This struct is used as config to maintain as information about each gpu.</p> <pre><code class="language-cu">// Parameters specific to training on multiple GPUs.
typedef struct {
  int process_rank;      // Rank of this process among all MPI processes on all hosts. 0 if no multi-GPU.
  int num_processes;     // Total number of processes on all hosts. 1 if no multi-GPU.
  int local_device_idx;  // This process GPU index on current machine. 0 if no multi-GPU.
  ncclComm_t nccl_comm;  // NCCL communication primitive, used for collective mutli-GPU work.
} MultiGpuConfig;

// Determine which GPU this process should use.
// Processes on the same machines use different GPU indicies. Processes on other machines don't.
// Copied from NCCL examples: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/examples.html#example-2-one-device-per-process-or-thread
int multi_gpu_get_local_device_idx(int process_rank, int num_processes) {
  char hostname[1024];
  hostname[1023] = '\0';
  // All processes on the same machine will share the same hostname.
  gethostname(hostname, 1023);
  for (int i=0; i &lt; 1024; i++) {
    if (hostname[i] == '.') {
        hostname[i] = '\0';
        break;
    }
  }
  uint64_t hostname_hash = 5381;
  for (int c = 0; hostname[c] != '\0'; c++){ hostname_hash = ((hostname_hash &lt;&lt; 5) + hostname_hash) ^ hostname[c]; }

  // Distribute all hostname hashes to all processes.
  uint64_t* all_hostsname_hashes = (uint64_t*)malloc(num_processes * sizeof(uint64_t));
  all_hostsname_hashes[process_rank] = hostname_hash;
  mpiCheck(MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, all_hostsname_hashes, sizeof(uint64_t), MPI_BYTE, MPI_COMM_WORLD));

  // Identify which GPU we need to use.
  int local_device_idx = 0;
  for (int current_process = 0; current_process &lt; num_processes; ++current_process) {
     if (current_process == process_rank) {
      // Found my gpu, local_device_idx now has my target GPU index.
      break;
     }
     if (all_hostsname_hashes[current_process] == all_hostsname_hashes[process_rank]) {
      // This process ID runs on the same machine, but it's not me, skip this GPU
      local_device_idx++;
     }
  }

  free(all_hostsname_hashes);
  return local_device_idx;
}
</code></pre> <p>Get this error.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>error: identifier "ncclAvg" is undefined
</code></pre></div></div> <p>Check nccl commit message and found that ncclAvg is added in version 2.10.3 I have only <code class="language-plaintext highlighter-rouge">nccl/2.9.9-1-cuda-11.0</code>.</p> <p>What should I do?</p> <p>Just learned that I can use <code class="language-plaintext highlighter-rouge">conda</code> to install cuda and nccl. Let’s try it.</p> <p>So what is conda and how does it work? I think conda is just a package manager like <code class="language-plaintext highlighter-rouge">apt</code> in ubuntu. <code class="language-plaintext highlighter-rouge">conda</code> helps with environment management and package installation.</p> <p><code class="language-plaintext highlighter-rouge">apt</code> helps with package installation and update.</p> <p><code class="language-plaintext highlighter-rouge">spack</code> is another package manager that is used in hpc cluster.</p> <p><a href="https://stackoverflow.com/a/78227826/14600569">https://stackoverflow.com/questions/77873047/what-are-the-key-differences-between-spack-and-conda-package-managers</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda <span class="nb">install </span>nvidia::cuda-toolkit
</code></pre></div></div> <p>Get another compilation error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/gcc/x86_64-conda-linux-gnu/12.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib//libcublas.so: undefined reference to `memcpy@GLIBC_2.14'
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/gcc/x86_64-conda-linux-gnu/12.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/libpmix.so.2: undefined reference to `clock_gettime@GLIBC_2.17'
collect2: error: ld returned 1 exit status
make: *** [train_gpt2cu] Error 255
(llmc) [nsccgz_qylin_1@gpu29%tianhe2-K llm.c]$ gcc --version
gcc (conda-forge gcc 12.3.0-11) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre></div></div> <p>check glibc version.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu29%tianhe2-K llm.c]$ ldd --version
ldd (GNU libc) 2.17
Copyright (C) 2012 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
Written by Roland McGrath and Ulrich Drepper.
(llmc) [nsccgz_qylin_1@gpu29%tianhe2-K llm.c]$ which ldd
/usr/bin/ldd
</code></pre></div></div> <p>I think I need to use ldd in conda env to check glibc version. How can I do this? I can not install specified glibc version with conda.</p> <p>Remove conda env and create a new one. Install nccl first</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge
    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge
    cuda-version-12.5          |       hd4f0392_3          21 KB  conda-forge
    libgcc-ng-13.2.0           |      h77fa898_11         777 KB  conda-forge
    libgomp-13.2.0             |      h77fa898_11         434 KB  conda-forge
    libstdcxx-ng-13.2.0        |      hc0a3c3a_11         3.7 MB  conda-forge
    nccl-2.22.3.1              |       hbc370b7_0       107.3 MB  conda-forge
    ------------------------------------------------------------
                                           Total:       112.2 MB

The following NEW packages will be INSTALLED:

  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge
  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu
  cuda-version       conda-forge/noarch::cuda-version-12.5-hd4f0392_3
  libgcc-ng          conda-forge/linux-64::libgcc-ng-13.2.0-h77fa898_11
  libgomp            conda-forge/linux-64::libgomp-13.2.0-h77fa898_11
  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-13.2.0-hc0a3c3a_11
  nccl               conda-forge/linux-64::nccl-2.22.3.1-hbc370b7_0
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span>nvidia/label/cuda-12.5.0::cuda-toolkit
</code></pre></div></div> <p>conda adjust channels priority</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda config --describe channel_priority
conda config --set channel_priority flexible
conda config --prepend channels conda-forge
conda config --prepend channels nvidia
conda config --show channels
</code></pre></div></div> <p>Error no nvtx3 and no openmpi</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable
✓ OpenMP found
✗ OpenMPI is not found, disabling multi-GPU support
---&gt; On Linux you can try install OpenMPI with `sudo apt install openmpi-bin openmpi-doc libopenmpi-dev`
✓ nvcc found, including GPU/CUDA support
---------------------------------------------
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmcc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt   -o train_gpt2cu
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>I remove channel conda-forge and try to install cuda from main anaconda channel.</p> <p>channel config is stored in <code class="language-plaintext highlighter-rouge">~/.condarc</code></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda info
conda remove <span class="nt">--name</span> myenv <span class="nt">--all</span>
</code></pre></div></div> <p>Fix openmpi issue after installing cuda from main channel</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln101 llm.c]$ PRECISION=FP32 make train_gpt2cu
__nvcc_device_query failed to call cudaLoader::cuInit(0) with error 0x64 (CUDA_ERROR_NO_DEVICE)
---------------------------------------------
→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable
✓ OpenMP found
✓ OpenMPI found, OK to train with multiple GPUs
✓ nvcc found, including GPU/CUDA support
---------------------------------------------
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
nvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>Forget that I should ssh to compute node. Let’s try again</p> <p>Still get no nvtx3 error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ PRECISION=FP32 make train_gpt2cu
---------------------------------------------
→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable
✓ OpenMP found
✓ OpenMPI found, OK to train with multiple GPUs
✓ nvcc found, including GPU/CUDA support
---------------------------------------------
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>Get installment error again</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ conda install cuda-nvtx -c nvidia
Channels:
 - nvidia
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: failed

InvalidSpec: The package "nvidia/linux-64::cuda==12.5.0=0" is not available for the specified platform

(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ exit
</code></pre></div></div> <p>https://github.com/NVIDIA/NVTX I will manually pull header files to node if I can’t now solve this issue this time by install 12.0.0 version of cuda</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install nvidia/label/cuda-12.0.0::cuda-toolkit
</code></pre></div></div> <p>Fix nvtx header file after copying to system path</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In file included from train_gpt2.cu:65:
llmc/zero.cuh:16:10: fatal error: nccl.h: No such file or directory
   16 | #include &lt;nccl.h&gt;
      |          ^~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:65:
llmc/zero.cuh:16:10: fatal error: nccl.h: No such file or directory
   16 | #include &lt;nccl.h&gt;
      |          ^~~~~~~~
compilation terminated.
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K nccl_2.22.3-1+cuda12.4_x86_64]$ cp -r include/* /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/
(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K nccl_2.22.3-1+cuda12.4_x86_64]$ cp -r lib/* /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /GPUFS/nsccgz_qylin_1/local/lib/libopen-pal.so.40: undefined reference to `pci_device_cfg_read'
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/</code> is not included in <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> when doing <code class="language-plaintext highlighter-rouge">ld</code> command. So I need to include that in <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> before running <code class="language-plaintext highlighter-rouge">ld</code> command.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ echo $LD_LIBRARY_PATH
/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/t/spack/opt/spack/linux-centos7-haswell/gcc-4.8.5/libevent-2.1.12-oysfi7miuhw62ginwjrr2uy6yldr2oav/lib:/GPUFS/app_GPU/application/anaconda3/5.3.1/envs/python-3.6/lib:/GPUFS/nsccgz_qylin_1/ryz/MARL-test/util/lib:/GPUFS/nsccgz_qylin_1/ryz/icf_test/build_GPTL/gptl_gcc/lib::/GPUFS/nsccgz_qylin_1/software/spack/opt/spack/linux-centos7-skylake_avx512/
</code></pre></div></div> <p>Should I switch to spack next time?</p> <p>Finally I am able to compile this train_gpt2cu after several days of failing and trying. I put conda lib at the first of <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> and it works.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ export  LD_LIBRARY_PATH=/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:$
LD_LIBRARY_PATH
(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ echo $LD_LIBRARY_PATH
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/t/spack/opt/spack/linux-centos7-haswell/gcc-4.8.5/libevent-2.1.12-oysfi7miuhw62ginwjrr2uy6yldr2oav/lib:/GPUFS/app_GPU/application/anaconda3/5.3.1/envs/python-3.6/lib:/GPUFS/nsccgz_qylin_1/ryz/MARL-test/util/lib:/GPUFS/nsccgz_qylin_1/ryz/icf_test/build_GPTL/gptl_gcc/lib::/GPUFS/nsccgz_qylin_1/software/spack/opt/spack/linux-centos7-skylake_avx512/:/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/
</code></pre></div></div> <p>Issue: cuda driver version is not compatible with cuda runtime version. What should I do?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
gpu 30
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:8A:00.0 Off |                    0 |
| N/A   39C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   34C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   33C    P0    37W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   36C    P0    51W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre></div></div> <p>Guess I have to install cuda 11.2 to match the driver version. Have to download nccl with cuda 11.</p> <p>However, I don’t find cuda:toolkit that is compatible with cuda 11.2 in conda.</p> <p>I think one solution is to compile nccl manually on this compute node with old cuda driver.</p> <p>But now I just switch use A800 which comes with cuda 12.0 driver. Let’s go. I have two A800 available so I can test cross node training and multi gpu card training. This is great.</p> <p>Issue: Seems that program is loading a .bin file with _bf16</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
</code></pre></div></div> <p>Finally I think I successfully run model training on two gpus on A800. Here’s compile and run script.</p> <p>compile.sh</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">echo</span> <span class="nv">$LD_LIBRARY_PATH</span>
yhrun <span class="nt">-n1</span> <span class="nt">-p</span> GPU_A800 make train_gpt2cu
</code></pre></div></div> <p>run.sh</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
conda activate llmc
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">echo</span> <span class="nv">$LD_LIBRARY_PATH</span>
yhrun <span class="nt">-n2</span> <span class="nt">-p</span> GPU_A800 /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/mpirun <span class="nt">-np</span> 2 train_gpt2cu
</code></pre></div></div> <p>output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| weight init method    | OpenAI's GPT-2 checkpoint                          |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| weight init method    | OpenAI's GPT-2 checkpoint                          |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 37                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 37                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 2                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
HellaSwag eval not found at dev/data/hellaswag/hellaswag_val.bin, skipping its evaluation
You can run `python dev/data/hellaswag.py` to export and use it with `-h 1`.
num_parameters: 124475904 =&gt; bytes: 248951808
allocated 237 MiB for model parameters
batch_size B=4 * seq_len T=1024 * num_processes=2 and total_batch_size=8192

step   15/37 | train loss 3.640566 | norm 2.2486 | lr 3.00e-04 | 268.99 ms | -100.0% bf16 MFU | 63701 tok/s
step   14/37 | train loss 3.381429 | norm 2.5808 | lr 3.00e-04 | 287.48 ms | -100.0% bf16 MFU | 57996 tok/s
step   16/37 | train loss 3.413064 | norm 2.0632 | lr 3.00e-04 | 174.35 ms | -100.0% bf16 MFU | 62144 tok/s
step   15/37 | train loss 3.640008 | norm 2.3061 | lr 3.00e-04 | 147.72 ms | -100.0% bf16 MFU | 57748 tok/s
step   17/37 | train loss 3.584605 | norm 2.0251 | lr 3.00e-04 | 158.54 ms | -100.0% bf16 MFU | 61209 tok/s
step   16/37 | train loss 3.412876 | norm 2.0334 | lr 3.00e-04 | 135.09 ms | -100.0% bf16 MFU | 58018 tok/s
step   18/37 | train loss 3.486408 | norm 1.6377 | lr 3.00e-04 | 115.45 ms | -100.0% bf16 MFU | 62047 tok/s
step   17/37 | train loss 3.584733 | norm 2.0324 | lr 3.00e-04 | 141.30 ms | -100.0% bf16 MFU | 58014 tok/s
step   19/37 | train loss 3.450470 | norm 2.1760 | lr 3.00e-04 | 120.89 ms | -100.0% bf16 MFU | 62521 tok/s
step   18/37 | train loss 3.487690 | norm 1.6469 | lr 3.00e-04 | 128.45 ms | -100.0% bf16 MFU | 58510 tok/s
step   20/37 | train loss 3.542054 | norm 2.2739 | lr 3.00e-04 | 66.91 ms | -100.0% bf16 MFU | 67331 tok/s
step   19/37 | train loss 3.451924 | norm 2.1979 | lr 3.00e-04 | 118.83 ms | -100.0% bf16 MFU | 59375 tok/s
step   20/37 | train loss 3.544027 | norm 2.2397 | lr 3.00e-04 | 108.95 ms | -100.0% bf16 MFU | 60644 tok/s
</code></pre></div></div> <p>6x throughput compared to v100.</p> <p>However, I am not sure if this trully accelerate training. How can I verify that multiple gpu training accelerate training process ?</p> <h3 id="cpu">CPU</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
python dev/data/tinyshakespeare.py
python train_gpt2.py
make train_gpt2
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>8 ./train_gpt2
</code></pre></div></div> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 20: train loss 4.527330 (took 2636.617334 ms)
step 21: train loss 4.065797 (took 2701.692621 ms)
step 22: train loss 3.965316 (took 2681.297241 ms)
step 23: train loss 3.449409 (took 2650.111416 ms)
step 24: train loss 4.490954 (took 2637.116332 ms)
step 25: train loss 4.035361 (took 2659.843151 ms)
step 26: train loss 3.445302 (took 2652.557792 ms)
step 27: train loss 3.993789 (took 2649.868369 ms)
step 28: train loss 4.199468 (took 2638.095098 ms)
step 29: train loss 4.538460 (took 2669.385015 ms)
val loss 4.350866
step 30: train loss 4.306292 (took 2658.306411 ms)
step 31: train loss 4.851407 (took 2634.616368 ms)
step 32: train loss 4.577479 (took 2670.470130 ms)
step 33: train loss 4.124943 (took 2660.545565 ms)
step 34: train loss 4.330319 (took 2669.532886 ms)
step 35: train loss 3.399416 (took 2639.378693 ms)
step 36: train loss 3.661207 (took 2632.377219 ms)
step 37: train loss 3.330453 (took 2637.114896 ms)
step 38: train loss 3.567853 (took 2645.744510 ms)
step 39: train loss 3.902004 (took 2635.939546 ms)
val loss 4.319361
generating:
---
EditBOOK IX:
Under the boasted sute of Georges:
So lordly is the prize had sin is high;
Hell is the way to God: frankish friends from blessed daughters
To Bermuda have heard the saying,
Then how to place the artscape.
Strong should a bellow
---
step 40: train loss 3.952987 (took 2665.948189 ms)
</code></pre></div></div> <p>Some questions? How many low end gpus are there in the market? I am thinking about utilizing low end gpus to train model, large or small model.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="cuda"/><summary type="html"><![CDATA[llm minikune]]></summary></entry><entry><title type="html">Basic digital electronic</title><link href="https://bilyz98.github.io/blog/2024/digital-electronic/" rel="alternate" type="text/html" title="Basic digital electronic"/><published>2024-06-18T11:59:00+00:00</published><updated>2024-06-18T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/digital-electronic</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/digital-electronic/"><![CDATA[<h2 id="basic-digital">Basic digital</h2> <p>Transistor to ALU <a href="https://youtu.be/HjneAhCy2N4?si=OpouvGQDJhw4sSKE">https://youtu.be/HjneAhCy2N4?si=OpouvGQDJhw4sSKE</a></p> <p>Transistor to memory <a href="https://youtu.be/rM9BjciBLmg?si=TQe2Wijej4iezyzV">https://youtu.be/rM9BjciBLmg?si=TQe2Wijej4iezyzV</a></p> <p>I learn from this video that memory is made of several gates to remember bit information and controls whether to enable write.</p> <p>Another interesting thing I learn is that 2 dimensions of gates are used to reduce number of wires to store same amount of information.</p>]]></content><author><name></name></author><category term="ml-fundamental"/><category term="transistor"/><summary type="html"><![CDATA[transistor]]></summary></entry><entry><title type="html">K8s Advance</title><link href="https://bilyz98.github.io/blog/2024/k8s-advance/" rel="alternate" type="text/html" title="K8s Advance"/><published>2024-06-16T11:59:00+00:00</published><updated>2024-06-16T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/k8s-advance</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/k8s-advance/"><![CDATA[<h2 id="kubectl-inspect-volume-content"><code class="language-plaintext highlighter-rouge">kubectl</code> inspect volume content</h2> <p><a href="https://stackoverflow.com/questions/49529005/how-to-inspect-the-content-of-persistent-volume-by-kubernetes-on-azure-cloud-ser">https://stackoverflow.com/questions/49529005/how-to-inspect-the-content-of-persistent-volume-by-kubernetes-on-azure-cloud-ser</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: pvc-inspector
spec:
  containers:
  - image: busybox
    name: pvc-inspector
    command: ["tail"]
    args: ["-f", "/dev/null"]
    volumeMounts:
    - mountPath: /pvc
      name: pvc-mount
  volumes:
  - name: pvc-mount
    persistentVolumeClaim:
      claimName: YOUR_CLAIM_NAME_HERE
</span><span class="no">EOF
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl exec -it pvc-inspector -- sh
$ ls /pvc
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl delete pod pvc-inspector
</code></pre></div></div>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="k8s"/><summary type="html"><![CDATA[k8s minikune]]></summary></entry><entry><title type="html">Difference between Dockerfile and Docker Compose</title><link href="https://bilyz98.github.io/blog/2024/difference-between-dockerfile-and-docker-compose/" rel="alternate" type="text/html" title="Difference between Dockerfile and Docker Compose"/><published>2024-06-16T07:31:03+00:00</published><updated>2024-06-16T07:31:03+00:00</updated><id>https://bilyz98.github.io/blog/2024/difference-between-dockerfile-and-docker-compose</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/difference-between-dockerfile-and-docker-compose/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Docker Rocksdb build</title><link href="https://bilyz98.github.io/blog/2024/docker-rocksdb-build/" rel="alternate" type="text/html" title="Docker Rocksdb build"/><published>2024-06-16T07:29:37+00:00</published><updated>2024-06-16T07:29:37+00:00</updated><id>https://bilyz98.github.io/blog/2024/docker-rocksdb-build</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/docker-rocksdb-build/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">K3s beginner</title><link href="https://bilyz98.github.io/blog/2024/k3s/" rel="alternate" type="text/html" title="K3s beginner"/><published>2024-06-14T11:59:00+00:00</published><updated>2024-06-14T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/k3s</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/k3s/"><![CDATA[<h2 id="whats-k3s-and-why-we-need-it-">What’s K3s and why we need it ?</h2> <p>K3s is a lightweight distribution of k8s to make k8s accessible in resource-constrained environments. Advantages of k3s:</p> <ol> <li>Size and efficiency</li> <li>Easy to install</li> <li>Some built-in tools</li> </ol> <h2 id="install">Install</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-sfL</span> https://get.k3s.io | sh -
</code></pre></div></div> <p>Check k3s process status</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> systemctl status k3s.service
</code></pre></div></div> <p>Got this issue <code class="language-plaintext highlighter-rouge">FATA[0000] starting kubernetes: preparing server: init cluster datastore and https: listen tcp :6443: bind: address already in use</code> Solutions:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo lsof -i :6443 
sudo kill -9 $(sudo lsof -t -i:6443)

sudo kill -9 &lt;PID&gt;
sudo systemctl restart k3s
</code></pre></div></div> <p>Issue: Not able to get k3s cluster status.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl get all -n kube-system

E0616 16:26:56.936188 1761783 memcache.go:265] couldn't get current server API group list: Get "https://192.168.49.2:8443/api?timeout=32s": dial tcp 192.168.49.2:8443: connect: no route to host
E0616 16:27:00.007979 1761783 memcache.go:265] couldn't get current server API group list: Get "https://192.168.49.2:8443/api?timeout=32s": dial tcp 192.168.49.2:8443: connect: no route to host
E0616 16:27:03.080047 1761783 memcache.go:265] couldn't get current server API group list: Get "https://192.168.49.2:8443/api?timeout=32s": dial tcp 192.168.49.2:8443: connect: no route to host
</code></pre></div></div> <p>I have install k8s on this node before. So I think the issue is that I do not copy the k3s config to <code class="language-plaintext highlighter-rouge">.kube</code> folder. Tried that</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Copy K3s kubeconfig to ~/.kube/config</span>
<span class="nb">sudo cp</span> /etc/rancher/k3s/k3s.yaml ~/.kube/config

<span class="c"># Change the owner of the config file to the current user</span>
<span class="nb">sudo chown</span> <span class="nv">$USER</span> ~/.kube/config

 kubectl get all <span class="nt">-n</span> kube-system
</code></pre></div></div> <p>Now I am able to get k3s cluster status.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                          READY   STATUS      RESTARTS   AGE
pod/coredns-6799fbcd5-2s8tn                   1/1     Running     0          46m
pod/local-path-provisioner-6c86858495-sfghq   1/1     Running     0          46m
pod/helm-install-traefik-crd-22tc9            0/1     Completed   0          46m
pod/helm-install-traefik-976dn                0/1     Completed   1          46m
pod/metrics-server-54fd9b65b-g5k5z            1/1     Running     0          46m
pod/svclb-traefik-ee33812d-mvwjp              2/2     Running     0          45m
pod/traefik-7d5f6474df-fjdgf                  1/1     Running     0          45m

NAME                     TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
service/kube-dns         ClusterIP      10.43.0.10     &lt;none&gt;        53/UDP,53/TCP,9153/TCP       46m
service/metrics-server   ClusterIP      10.43.199.96   &lt;none&gt;        443/TCP                      46m
service/traefik          LoadBalancer   10.43.62.153   28.10.10.62   80:30148/TCP,443:30314/TCP   45m
</code></pre></div></div> <p>Check this article to know more about installing k3s on ubuntu <a href="https://www.digitalocean.com/community/tutorials/how-to-setup-k3s-kubernetes-cluster-on-ubuntu">https://www.digitalocean.com/community/tutorials/how-to-setup-k3s-kubernetes-cluster-on-ubuntu</a></p> <h2 id="k3s-use-local-docker-image">K3s use local docker image</h2> <p>If you have a Docker image on your local machine and you want to use it in a Kubernetes cluster, you can follow these steps:</p> <ol> <li> <p><strong>Tag your image</strong>: Tag the Docker image with a version number. For example, if your image is named <code class="language-plaintext highlighter-rouge">my-image</code> and you want to tag it as <code class="language-plaintext highlighter-rouge">v1</code>, you can use the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> docker tag my-image:latest my-image:v1
</code></pre></div> </div> </li> <li> <p><strong>Load the image into your nodes</strong>: If you’re using Minikube, you can use the <code class="language-plaintext highlighter-rouge">minikube docker-env</code> command to configure your current shell to use Minikube’s Docker daemon, then use <code class="language-plaintext highlighter-rouge">docker save</code> and <code class="language-plaintext highlighter-rouge">docker load</code> to move your image. If you’re using K3s, you can load the image directly into the K3s nodes using <code class="language-plaintext highlighter-rouge">ctr</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> docker save my-image:v1 | ctr <span class="nt">-n</span> k8s.io images import -
</code></pre></div> </div> </li> <li> <p><strong>Use the image in Kubernetes</strong>: Now you can use your image in a Kubernetes pod. Here’s an example of a pod specification:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
 <span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">my-pod</span>
 <span class="na">spec</span><span class="pi">:</span>
   <span class="na">containers</span><span class="pi">:</span>
   <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-container</span>
     <span class="na">image</span><span class="pi">:</span> <span class="s">my-image:v1</span>
</code></pre></div> </div> </li> </ol> <p>Remember to replace <code class="language-plaintext highlighter-rouge">my-image:v1</code> with your actual image name and tag.</p> <h2 id="difference-between-deployment-and-pod">Difference between Deployment and Pod</h2> <p>A Pod is a group of one or more containers, with shared storage/network resources, and a specification for how to run the containers. A Pod’s contents are always co-located and co-scheduled, and run in a shared context. A basic pod definition for running a single container of nginx</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-container</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.14.2</span>
</code></pre></div></div> <p>A Deployment is a higher-level concept that manages Pods and provides declarative updates to Pods along with a lot of other useful features. Therefore, a Deployment is a higher-level concept that manages ReplicaSets and Pods. A basic deployment configution</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiversion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-deployment</span>
    <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
    <span class="na">replicas</span><span class="pi">:</span> <span class="m">2</span>
    <span class="na">selector</span><span class="pi">:</span>
        <span class="na">matchLabels</span><span class="pi">:</span>
            <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">template</span><span class="pi">:</span>
        <span class="na">metadata</span><span class="pi">:</span>
            <span class="na">labels</span><span class="pi">:</span>
                <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
        <span class="na">spec</span><span class="pi">:</span>
            <span class="na">containers</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-container</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.14.2</span>
            <span class="na">ports</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div> <p>Deployments wrap pod definition, providing additional management layer. Pods reqquire manual updates and intervention for deploying and scaling Deployments enable automatic updates, rollbacks and scaling.</p> <p>Issue: Still could not pull image from local</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  2s    default-scheduler  Successfully assigned default/rocksdb-74cc66c9d6-fhsz5 to common-testbed
  Normal   Pulling    2s    kubelet            Pulling image "docker.io/rocksdb:v1"
  Warning  Failed     0s    kubelet            Failed to pull image "docker.io/rocksdb:v1": failed to pull and unpack image "docker.io/library/rocksdb:v1": failed to resolve reference "docker.io/library/rocksdb:v1": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
  Warning  Failed     0s    kubelet            Error: ErrImagePull
</code></pre></div></div> <p>Solution: Try to push image to docker.io and pull from it. Sure, here’s a basic example of how you can push a Docker image to Docker Hub (docker.io) and then use that image in a Kubernetes deployment YAML file.</p> <p>First, you need to tag your Docker image with the Docker Hub username and repository name:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker tag local-image:tag yourusername/yourrepository:tag
</code></pre></div></div> <p>Then, you can push the Docker image to Docker Hub:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker push yourusername/yourrepository:tag
</code></pre></div></div> <p>After pushing the image to Docker Hub, you can use it in a Kubernetes deployment YAML file:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">your-deployment</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">your-app</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">your-app</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">your-app</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">yourusername/yourrepository:tag</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker tag rocksdb:latest zzt1234/test-rocksdb:v2
docker push zzt1234/test-rocksdb:v2
</code></pre></div></div> <p>In this YAML file, replace <code class="language-plaintext highlighter-rouge">yourusername/yourrepository:tag</code> with the Docker Hub username, repository name, and tag you used earlier. This will pull the image from Docker Hub and use it for the Kubernetes deployment.</p> <p>Please replace <code class="language-plaintext highlighter-rouge">yourusername</code>, <code class="language-plaintext highlighter-rouge">yourrepository</code>, <code class="language-plaintext highlighter-rouge">tag</code>, <code class="language-plaintext highlighter-rouge">your-deployment</code>, <code class="language-plaintext highlighter-rouge">your-app</code>, and <code class="language-plaintext highlighter-rouge">8080</code> with your actual values. Also, make sure you’re logged in to Docker Hub in the environment where you’re running these commands. You can do this using the <code class="language-plaintext highlighter-rouge">docker login</code> command.</p> <p>Remember to apply the deployment using <code class="language-plaintext highlighter-rouge">kubectl apply -f your-deployment.yaml</code>.</p> <p>Another issue: Still can not start container successfully. Want to check logs of container but failed to connect to it</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl logs rocksdb-57c5b55686-mnwvr rocksdb
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  docker git:(master) ✗ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating (auto-restart) (Result: exit-code) since Mon 2024-06-17 17:03:06 HKT; 5s ago
       Docs: https://kubernetes.io/docs/home/
    Process: 1458057 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status&gt;   Main PID: 1458057 (code=exited, status=1/FAILURE)
</code></pre></div></div> <p>Not able</p> <p>Asked bingchat to give me solutions for how to solve this kubelet connection problem</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>journalctl -u kubelet
</code></pre></div></div> <p>Get this log output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>May 12 02:49:40 common-testbed kubelet[3632]: E0512 02:49:40.413488    3632 controller.go:146] "Failed to ensure lease exists, will retry" err="leases.coordination.k8s&gt;May 12 02:49:44 common-testbed kubelet[3632]: I0512 02:49:44.813157    3632 kubelet_node_status.go:70] "Attempting to register node" node="common-testbed"
May 12 02:49:44 common-testbed kubelet[3632]: E0512 02:49:44.816687    3632 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes \"common-te&gt;May 12 02:49:47 common-testbed kubelet[3632]: E0512 02:49:47.110624    3632 eviction_manager.go:258] "Eviction manager: failed to get summary stats" err="failed to get&gt;May 12 02:49:47 common-testbed kubelet[3632]: E0512 02:49:47.416828    3632 controller.go:146] "Failed to ensure lease exists, will retry" err="leases.coordination.k8s&gt;May 12 02:49:48 common-testbed kubelet[3632]: E0512 02:49:48.872385    3632 dns.go:153] "Nameserver limits exceeded" err="Nameserver limits were exceeded, some nameser&gt;May 12 02:49:50 common-testbed kubelet[3632]: E0512 02:49:50.873045    3632 dns.go:153] "Nameserver limits exceeded" err="Nameserver limits were exceeded, some nameser&gt;May 12 02:49:51 common-testbed kubelet[36
</code></pre></div></div> <p>So now I decide to restart kubelet service for k3s because this kubelet service was started for previous k8s cluster. I am not sure if there is any domain name issue with it. I don’t know how to solve this problem properly so I decided to reinstall kubelet service.</p> <p>Failed to execute kubectl command after replacing k3s.service file content new(not working):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Unit]
Description=Lightweight Kubernetes
Documentation=https://k3s.io
After=network-online.target

[Service]
ExecStart=/usr/local/bin/k3s \
    server \
    --kubelet-arg='address=0.0.0.0' \
    --kubelet-arg='anonymous-auth=false' \
    --kubelet-arg='authentication-token-webhook=true' \
    --kubelet-arg='authorization-mode=Webhook' \
    --kubelet-arg='client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt' \
    --kubelet-arg='tls-cert-file=/var/lib/rancher/k3s/server/tls/kubelet.crt' \
    --kubelet-arg='tls-private-key-file=/var/lib/rancher/k3s/server/tls/kubelet.key'
KillMode=process
Delegate=yes
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
Restart=always
RestartSec=5s

[Install]
WantedBy=multi-user.target

</code></pre></div></div> <p>old</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Unit]
Description=Lightweight Kubernetes
Documentation=https://k3s.io
Wants=network-online.target
After=network-online.target

[Install]
WantedBy=multi-user.target

[Service]
Type=notify
EnvironmentFile=-/etc/default/%N
EnvironmentFile=-/etc/sysconfig/%N
EnvironmentFile=-/etc/systemd/system/k3s.service.env
KillMode=process
Delegate=yes
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
TimeoutStartSec=0
Restart=always
RestartSec=5s
ExecStartPre=/bin/sh -xc '! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service 2&gt;/dev/null'
ExecStartPre=-/sbin/modprobe br_netfilter
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/k3s \
    server \
    --kubelet-arg='address=0.0.0.0' \
    --kubelet-arg='anonymous-auth=false' \
    --kubelet-arg='authentication-token-webhook=true' \
    --kubelet-arg='authorization-mode=Webhook' \
    --kubelet-arg='client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt' \
    --kubelet-arg='tls-cert-file=/var/lib/rancher/k3s/server/tls/kubelet.crt' \
    --kubelet-arg='tls-private-key-file=/var/lib/rancher/k3s/server/tls/kubelet.key'
</code></pre></div></div> <p>Got error port:6443 already in use Tried to delete all processes that use 6443 First, need to stop kubelet service because it uses 6443 port</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo systemctl stop kubelet
</code></pre></div></div> <p>Then check the process that uses 6443 port</p> <p>I found that there is obsolete content in kubelet starting file</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubel
sudo vim /etc/kubernetes/kubelet.conf

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJSkFNTXZ6a0h3ZWN3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TXpFeU1qa3dPVEUxTXpGYUZ3MHpNekV5TWpZd09USXdNekZhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURiVWhXYlg1a241M0xLbHVDa2kzME96QUloR3ltVFdEdS9xZjlOUU1JcmtXdnoybUp6M2ptNHAzSFMKbllkY01RKytpL2FxVDN5MDNuSDhycXZ3ZFVRWnVsSS9xTXQreFdOTVg3V0NHR3lqOEhDY0JtdXdaT1A4WjE3dApBNVByNFRrM3VJOFNHMnJoRTBtTithMk5rQnVCZjNFRnR4NllPUnByKytneUtmMmxhQTlqV1A2cytML1plWi94CktYaFRicittclBPbTlXMXBxYnVwamJNNkJJUmROU0dYcWRiL0orcVRabDNQcStZZmtUaHJ6VjU3ZG9rOVBEWUEKMXd1MW8yZ3RMUTBISGcwb2R4V1ZyN2ZKenhNSXU0bWJ5ZGZiUTRPU2g2T2dDNzlNdFEvOGFtQ1hFTlJxK2RPaQo0TUhSQlNGZTh2K0FDMmQwY2NndjRkV1hURm1UQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJSaEpoNUkvdys0c1diNjNlcXYycklqOEd2ekhEQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQUR2OWU4T2dRNAoybzBJeUg5OFg2aDI0TnIrbEQ3R1o3KzBDUzdYcnZsYWtrV1hwT3RLYmRVWDI5Y3BPZ2p4WHZLV2NqMzhtK0wxCmJwWUlRK3Q0bXh0SmFlZVJlT2FzcXJzZVBQbC9POFMxRTlxN2xGd2dNWWs4amc3SVFjeHU5QzJBNG5OOGgxeXQKdU1qWi9mUXlkUmdMSkhnYm15Vkw5NGtpZndSOWxJM0RZTExwM2dlYTQ4ejBFZ0ZpOERoVXkrZ1llVDk5dzZXaQp0YmtFNHRTZ05jZFNjS2ZFMDNvTnFrUXJ4dkJXc1lOZnlOS0dPclV1YTNVTjhqN2NRcHFOa0plRlhxTUhsT0sxCmtJbi9XOVl4UC9rWGx2UkIvZXFiTC9uVlYxeTg4SC9tWGFsNHlIZ1owckNOSm1SOW9vRHJsVS9aRWtpMWRZeEIKdzdhdjFMOFU2M1BaCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://28.10.10.62:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:node:k8s-master
  name: system:node:k8s-master@kubernetes
current-context: system:node:k8s-master@kubernetes
kind: Config
preferences: {}
users:
- name: system:node:k8s-master
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem
</code></pre></div></div> <p>So what I need to do now is to uninstall kubelet or clean config file. I don’t know the exact relationship between kubelet and k3s yet.</p> <p>I run this command to remove all existing service and clean config files</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo kubeadm reset --cleanup-tmp-dir -f --cri-socket=unix:///run/containerd/containerd.sock

[reset] Reading configuration from the cluster...
[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks
[reset] Deleted contents of the etcd data directory: /var/lib/etcd
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki /etc/kubernetes/tmp]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
</code></pre></div></div> <p>Clear original k8s installment including kubeadm, kubectl, kubulet. Uninstalling Kubernetes involves removing various components including <code class="language-plaintext highlighter-rouge">kubelet</code>, <code class="language-plaintext highlighter-rouge">kubeadm</code>, <code class="language-plaintext highlighter-rouge">kubectl</code>, and the <code class="language-plaintext highlighter-rouge">.kube</code> configuration directory. Here’s a general guide on how you can do it:</p> <ol> <li><strong>Drain and delete the nodes</strong>: If you have a multi-node setup, you need to drain the nodes before deleting them. Replace <code class="language-plaintext highlighter-rouge">&lt;node-name&gt;</code> with the name of your node.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl drain &lt;node-name&gt; <span class="nt">--delete-local-data</span> <span class="nt">--force</span> <span class="nt">--ignore-daemonsets</span>
kubectl delete node &lt;node-name&gt;
</code></pre></div></div> <ol> <li><strong>Reset kubeadm</strong>: This will revert all the changes made by kubeadm init and kubeadm join.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>kubeadm reset
</code></pre></div></div> <ol> <li><strong>Remove kubelet and kubeadm</strong>:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get purge kubeadm kubectl kubelet kubernetes-cni kube<span class="k">*</span>
<span class="nb">sudo </span>apt-get autoremove
</code></pre></div></div> <ol> <li><strong>Delete the .kube directory</strong>:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">rm</span> <span class="nt">-rf</span> ~/.kube
</code></pre></div></div> <ol> <li><strong>Remove all the docker container network interfaces</strong>:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">rm</span> <span class="si">$(</span>docker ps <span class="nt">-a</span> <span class="nt">-q</span><span class="si">)</span>
docker rmi <span class="si">$(</span>docker images <span class="nt">-q</span><span class="si">)</span>
</code></pre></div></div> <p>Got this issue</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  docker git:(master) ✗ kubectl describe pod
WARN[0000] Unable to read /etc/rancher/k3s/k3s.yaml, please start server with --write-kubeconfig-mode to modify kube config permissions
error: error loading config file "/etc/rancher/k3s/k3s.yaml": open /etc/rancher/k3s/k3s.yaml: permission denied
</code></pre></div></div> <p>Solution: This error is related to the permissions of the <code class="language-plaintext highlighter-rouge">k3s.yaml</code> file. Here are a few ways you can resolve this issue:</p> <ol> <li><strong>Change the permissions of the file</strong>²⁴: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> sudo chmod 644 /etc/rancher/k3s/k3s.yaml
</code></pre></div> </div> <p>This command changes the permissions of the file to be readable by all users on your system.</p> </li> <li><strong>Change the KUBECONFIG environment variable</strong>¹: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> KUBECONFIG=~/.kube/config
 export KUBECONFIG=~/.kube/config
 mkdir ~/.kube 2&gt; /dev/null
 sudo k3s kubectl config view --raw &gt; "$KUBECONFIG"
 chmod 600 "$KUBECONFIG"
</code></pre></div> </div> <p>This set of commands changes the <code class="language-plaintext highlighter-rouge">KUBECONFIG</code> environment variable to point to a different location (<code class="language-plaintext highlighter-rouge">~/.kube/config</code>), creates the <code class="language-plaintext highlighter-rouge">.kube</code> directory if it doesn’t exist, copies the current configuration into the new location, and then changes the permissions of the new configuration file.</p> </li> <li><strong>Start the k3s server with modified kube config permissions</strong>³: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> sudo k3s server --write-kubeconfig-mode 644
</code></pre></div> </div> <p>This command starts the k3s server with the <code class="language-plaintext highlighter-rouge">--write-kubeconfig-mode</code> flag set to <code class="language-plaintext highlighter-rouge">644</code>, which changes the permissions of the kubeconfig file when it is written.</p> </li> </ol> <p>Still getting error that container is not able to start successfully</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Jun 17 22:07:14 common-testbed k3s[3214677]: I0617 22:07:14.939722 3214677 scope.go:117] "RemoveContainer" containerID="e0c6ed40764db0173c3464f2e4b387a2bc9bd13bbf5e3ab&gt;Jun 17 22:07:14 common-testbed k3s[3214677]: E0617 22:07:14.940136 3214677 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"r&gt;Jun 17 22:07:25 common-testbed k3s[3214677]: I0617 22:07:25.939271 3214677 scope.go:117] "RemoveContainer" containerID="e0c6ed40764db0173c3464f2e4b387a2bc9bd13bbf5e3ab&gt;Jun 17 22:07:25 common-testbed k3s[3214677]: E0617 22:07:25.939711 3214677 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"r&gt;Jun 17 22:07:40 common-testbed k3s[3214677]: I0617 22:07:40.939511 3214677 scope.go:117] "RemoveContainer" containerID="e0c6ed40764db0173c3464f2e4b387a2bc9bd13bbf5e3ab&gt;Jun 17 22:07:40 common-testbed k3s[3214677]: E0617 22:07:40.939941 3214677 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"r&gt;~
</code></pre></div></div> <p>Solution: Remove all existing deployments and pods and restart again</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Delete all deployments in the default namespace
kubectl delete deployments --all

# Delete all pods in the default namespace
kubectl delete pods --all

</code></pre></div></div> <p>Still get error that rocksdb container fails to finish successfully. And I can not check logs of container. Solution: BingChat tells me that it’s because of http_proxy env variable issue. Do this</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export NO_PROXY=$NO_PROXY,&lt;node-ip-address&gt;:6443
unset http_proxy
unset https_proxy

</code></pre></div></div> <p>Still can not call <code class="language-plaintext highlighter-rouge">kubectl logs</code> command to check container logs.</p> <p>Uninstall k3s</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
sudo /usr/local/bin/k3s-uninstall.sh
</code></pre></div></div> <p>https://docs.k3s.io/installation/configuration</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" INSTALL_K3S_EXEC="server" sh -s - --flannel-backend none
</code></pre></div></div> <p>Get metrics server endpoint not available error after reinstallment of k3s Solution Don’t have solution yet.</p> <h2 id="try-minikube">Try minikube</h2> <p>Follow this doc <a href="https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download#Service">https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download#Service</a></p> <p>And then set no proxy to run <code class="language-plaintext highlighter-rouge">kubectl</code> command</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> export no_proxy=$(minikube ip)
</code></pre></div></div> <p>Run this command to set alias</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>alias kubectl="minikube kubectl --"
</code></pre></div></div> <p>Issue: Get image pulling issue</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Normal   Scheduled  3m32s                default-scheduler  Successfully assigned default/rocksdb-858cd64b59-hnqtg to minikube
  Warning  Failed     3m22s                kubelet            Failed to pull image "zzt1234/test-rocksdb:v2": Error response from daemon: Get "https://registry-1.docker.io/v2/": dial tcp 34.226.69.105:443: connect: no route to host
</code></pre></div></div> <p>I think the reason is thatimage pulling does not go through proxy. Need to find a way to let minikube to use proxy to pull image.</p> <p>https://stackoverflow.com/questions/73756734/minikube-start-error-to-pull-new-external-images-you-may-need-to-configure-a-pr Read this doc to solve image pulling issue</p> <p>Set proxy in doker daemon setting file like this https://docs.docker.com/config/daemon/systemd/#httphttps-proxy</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>minikube start --docker-env HTTP_PROXY=http://127.0.0.1:8081   --docker-env HTTPS_PROXY=http://127.0.0.1:8081
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  4s    default-scheduler  Successfully assigned default/hello-minikube to minikube
  Normal   Pulling    3s    kubelet            Pulling image "gcr.io/google_containers/echoserver:1.4"
  Warning  Failed     3s    kubelet            Failed to pull image "gcr.io/google_containers/echoserver:1.4": Error response from daemon: Get "https://gcr.io/v2/": proxyconnect tcp: dial tcp 127.0.0.1:8081: connect: connection refused
  Warning  Failed     3s    kubelet            Error: ErrImagePull
</code></pre></div></div> <p>Actually command above about setting proxy is not right. I think I should set proxy address to real ip address of host machine.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> minikube start --docker-env HTTP_PROXY=http://28.10.10.62:8081   --docker-env HTTPS_PROXY=http://28.10.10.62:8081
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Normal   Scheduled  4s    default-scheduler  Successfully assigned default/hello-minikube to minikube
  Normal   Pulling    3s    kubelet            Pulling image "gcr.io/google_containers/echoserver:1.4"
  Warning  Failed     2s    kubelet            Failed to pull image "gcr.io/google_containers/echoserver:1.4": [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of gcr.io/google_containers/echoserver:1.4 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/
  Warning  Failed     2s    kubelet            Error: ErrImagePull
</code></pre></div></div> <p>Now this issue is finially solved.</p> <p>So what do I learn from this problem solving steps? minikube actually set up a docker vm in host machine to pull image. So when I run <code class="language-plaintext highlighter-rouge">minikube start</code> command, it actually starts a docker vm in host machine. and when I run <code class="language-plaintext highlighter-rouge">kubectl</code> command, it actually runs in docker vm.</p> <p>Finally able to see logs inside k8s container</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  docker git:(master) ✗ kubectl logs rocksdb-858cd64b59-vk28f rocksdb
RocksDB:    version 8.11.3
Date:       Tue Jun 18 04:21:49 2024
CPU:        40 * Intel(R) Xeon(R) Gold 6230N CPU @ 2.30GHz
CPUCache:   28160 KB
Set seed to 1718684509751930 because --seed was 0
Initializing RocksDB Options from the specified file
Initializing RocksDB Options from command-line flags
Integrated BlobDB: blob cache disabled
Keys:       16 bytes each (+ 0 bytes user-defined timestamp)
Values:     100 bytes each (50 bytes after compression)
Entries:    1000000
Prefix:    0 bytes
Keys per prefix:    0
RawSize:    110.6 MB (estimated)
FileSize:   62.9 MB (estimated)
Write rate: 0 bytes/second
Read rate: 0 ops/second
Compression: NoCompression
Compression sampling rate: 0
Memtablerep: SkipListFactory
Perf Level: 1
------------------------------------------------
Initializing RocksDB Options from the specified file
Initializing RocksDB Options from command-line flags
Integrated BlobDB: blob cache disabled
DB path: [/data/]
fillseq      :       2.114 micros/op 473062 ops/sec 2.114 seconds 1000000 operations;   52.3 MB/s
</code></pre></div></div> <p>Looks like that minikube sets docker proxy to map host port to container port.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ├─3420670 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
 ├─3534038 /usr/bin/docker-proxy -proto tcp -host-ip 127.0.0.1 -host-port 32778 -container-ip 192.168.58.2 -container-port 32443
 ├─3534054 /usr/bin/docker-proxy -proto tcp -host-ip 127.0.0.1 -host-port 32779 -container-ip 192.168.58.2 -container-port 8443
 ├─3534076 /usr/bin/docker-proxy -proto tcp -host-ip 127.0.0.1 -host-port 32780 -container-ip 192.168.58.2 -container-port 5000
 ├─3534090 /usr/bin/docker-proxy -proto tcp -host-ip 127.0.0.1 -host-port 32781 -container-ip 192.168.58.2 -container-port 2376
 └─3534110 /usr/bin/docker-proxy -proto tcp -host-ip 127.0.0.1 -host-port 32782 -container-ip 192.168.58.2 -container-port 22
</code></pre></div></div> <h2 id="kubectl-delete-deployment-and-service"><code class="language-plaintext highlighter-rouge">kubectl</code> delete deployment and service</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Delete deployment</span>
kubectl delete deployment &lt;deployment-name&gt;

<span class="c"># Delete service</span>
kubectl delete service &lt;service-name&gt;
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List deployments</span>
kubectl get deployments

<span class="c"># List services</span>
kubectl get services

</code></pre></div></div> <h2 id="submit-container-job-to-k3s">Submit container job to k3s</h2> <p>After pushing local image to docker hub I am able to get normal image pulling message.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl describe pod
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  7s    default-scheduler  Successfully assigned default/rocksdb-57c5b55686-mnwvr to common-testbed
  Normal  Pulling    7s    kubelet            Pulling image "zzt1234/test-rocksdb:v2"
</code></pre></div></div>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="k3s"/><category term="k8s"/><summary type="html"><![CDATA[k3s]]></summary></entry><entry><title type="html">Docker beginner</title><link href="https://bilyz98.github.io/blog/2024/docker-file-compose-diff/" rel="alternate" type="text/html" title="Docker beginner"/><published>2024-06-11T11:59:00+00:00</published><updated>2024-06-11T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/docker-file-compose-diff</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/docker-file-compose-diff/"><![CDATA[<h2 id="difference-between-dockerfile-and-docker-compose">Difference between Dockerfile and Docker Compose</h2> <p>Dockerfile and Docker Compose are both important components of Docker, but they serve different purposes:</p> <ol> <li><strong>Dockerfile</strong>¹²³⁴⁵: <ul> <li>A Dockerfile is a text file that contains instructions for building a Docker image¹²³⁴⁵.</li> <li>It’s essentially Docker’s version of a Makefile or build.gradle⁵.</li> <li>It includes commands like <code class="language-plaintext highlighter-rouge">RUN</code>, <code class="language-plaintext highlighter-rouge">ADD</code>, <code class="language-plaintext highlighter-rouge">COPY</code>, <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code>, etc².</li> <li>You build an image from a Dockerfile².</li> <li>An image is a template for running containers, consisting of multiple read-only layers².</li> <li>Dockerfile is used to create a container image².</li> </ul> </li> <li><strong>Docker Compose</strong>¹²³⁴⁵: <ul> <li>Docker Compose is a tool for defining and running multi-container Docker applications¹²³⁴⁵.</li> <li>It uses a YAML file (docker-compose.yml) to define the services that make up your app so they can be run together in an isolated environment³.</li> <li>A single Compose file can contain multiple container deployment configurations².</li> <li>Docker Compose is used to deploy an instance of that image as a container².</li> <li>It’s more readable, more maintainable, more intuitive².</li> </ul> </li> </ol> <p>In summary, a Dockerfile is used to build images, while Docker Compose is used to define and run multi-container applications¹²³⁴⁵. The Dockerfile uses the <code class="language-plaintext highlighter-rouge">docker build</code> command, while the docker-compose.yml file uses the <code class="language-plaintext highlighter-rouge">docker-compose up</code> command⁴. A docker-compose.yml file can reference a Dockerfile, but a Dockerfile can’t reference a docker-compose file⁴.</p> <h2 id="specify-docker-image-storage-location">Specify docker image storage location</h2> <p>To specify a different storage location for Docker images, you need to modify Docker’s configuration. Here are the steps for Linux and Windows:</p> <p><strong>For Linux</strong>¹²:</p> <ol> <li>Stop the Docker daemon if it’s running. You can do this with the command: <code class="language-plaintext highlighter-rouge">sudo systemctl stop docker</code>.</li> <li>Create a <code class="language-plaintext highlighter-rouge">daemon.json</code> file in <code class="language-plaintext highlighter-rouge">/etc/docker/</code> if it doesn’t exist¹.</li> <li>Open the <code class="language-plaintext highlighter-rouge">daemon.json</code> file and add the following, replacing <code class="language-plaintext highlighter-rouge">"/mnt"</code> with your desired path¹:</li> </ol> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"data-root"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/mnt"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <ol> <li>Save and close the file.</li> <li>Start the Docker daemon with the command: <code class="language-plaintext highlighter-rouge">sudo systemctl start docker</code>.</li> </ol> <p><strong>For Windows</strong>³:</p> <ol> <li>Quit Docker Desktop if it’s running.</li> <li>Open the <code class="language-plaintext highlighter-rouge">daemon.json</code> file located at <code class="language-plaintext highlighter-rouge">C:\ProgramData\Docker\config\</code>.</li> <li>Add the following to the file, replacing <code class="language-plaintext highlighter-rouge">"D:\\Virtual Machines\\Docker"</code> with your desired path³:</li> </ol> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"data-root"</span><span class="p">:</span><span class="w"> </span><span class="s2">"D:</span><span class="se">\\</span><span class="s2">Virtual Machines</span><span class="se">\\</span><span class="s2">Docker"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <ol> <li>Save and close the file.</li> <li>Start Docker Desktop.</li> </ol> <p>After making these changes, Docker will store its images in the specified directory. You can confirm the new storage location by running the command <code class="language-plaintext highlighter-rouge">docker info</code> and checking the <code class="language-plaintext highlighter-rouge">Docker Root Dir</code> value¹²³. Please note that you need to have the necessary permissions to read and write to the specified directory¹²³.</p> <h2 id="docker-remove-container-and-image">Docker remove container and image</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># show all containers</span>
docker ps <span class="nt">-a</span>

docker <span class="nb">rm</span> &lt;container_id&gt;
docker rmi &lt;image_id&gt;
</code></pre></div></div> <h2 id="docker-pull-image-with-proxy">Docker pull image with proxy</h2> <ol> <li>Create a systemd drop-in directory for the Docker service if it doesn’t exist²⁴:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> /etc/systemd/system/docker.service.d
</code></pre></div></div> <ol> <li>Create a file called <code class="language-plaintext highlighter-rouge">/etc/systemd/system/docker.service.d/http-proxy.conf</code>²⁴. Add the following content to the file, replacing <code class="language-plaintext highlighter-rouge">proxy.example.com:80</code> with your proxy host and port²⁴:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>Service]
<span class="nv">Environment</span><span class="o">=</span><span class="s2">"HTTP_PROXY=http://proxy.example.com:80/"</span>
<span class="nv">Environment</span><span class="o">=</span><span class="s2">"HTTPS_PROXY=http://proxy.example.com:80/"</span>
</code></pre></div></div> <p>If you have internal Docker registries that you need to contact without proxying, you can specify them via the <code class="language-plaintext highlighter-rouge">NO_PROXY</code> environment variable²:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">Environment</span><span class="o">=</span><span class="s2">"NO_PROXY=localhost,127.0.0.0/8,docker-registry.somecorporation.com"</span>
</code></pre></div></div> <ol> <li>Reload the systemd daemon to apply the changes²⁴:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl daemon-reload
</code></pre></div></div> <ol> <li>Restart the Docker service²⁴:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl restart docker
</code></pre></div></div> <p>Now, Docker will use the specified proxy when pulling images²⁴.</p> <p>Remember, you need to have the necessary permissions to create and modify files in <code class="language-plaintext highlighter-rouge">/etc/systemd/system/docker.service.d</code>²⁴. If you don’t, you may need to use <code class="language-plaintext highlighter-rouge">sudo</code> or log in as root²⁴. Also, ensure that your proxy server is properly configured and reachable from your Docker host²⁴.</p> <h2 id="ubuntu-repository-mirror-in-china">Ubuntu repository mirror in China</h2> <p>http://ftp.sjtu.edu.cn/ubuntu/</p> <h2 id="solve-unable-to-connect-to-archiveubuntucom-during-docker-build">Solve unable to connect to archive.ubuntu.com during docker build</h2> <p>Solution I tried but not work: https://gist.github.com/dyndna/12b2317b5fbade37e747</p> <p>https://stackoverflow.com/questions/24991136/docker-build-could-not-resolve-archive-ubuntu-com-apt-get-fails-to-install-a</p> <p>Tried replacing sources.list with tsinghua source list but not work.</p> <p>Tried to pull the image and enter the conter to see if I can ping Failed.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run -it ubuntu bash
</code></pre></div></div> <p>Tried solution in this doc https://talk.plesk.com/threads/docker-is-unable-to-connect-to-the-internet.370357/</p> <p>Tried ping and curl on host machine ping does not work but curl can work. So I think I found the root cause. I set proxy for curl but I did not set proxy for ping.</p> <p>And I should set proxy for container so that it can do <code class="language-plaintext highlighter-rouge">apt-get update</code> successfully. Add these lines to set proxy that is used in host machine for container.</p> <div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Update sources.list</span>
<span class="k">RUN </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/http:\/\/archive.ubuntu.com\/ubuntu\//http:\/\/mirrors.tuna.tsinghua.edu.cn\/ubuntu\//g'</span> /etc/apt/sources.list

<span class="k">ENV</span><span class="s"> http_proxy http://28.10.10.62:8081</span>
<span class="k">ENV</span><span class="s"> https_proxy http://28.10.10.62:8081 </span>

</code></pre></div></div> <h2 id="docker-remove-all-containers">docker remove all containers</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># show all containers</span>
docker ps <span class="nt">-a</span>

docker <span class="nb">rm</span> <span class="si">$(</span>docker ps <span class="nt">-a</span> <span class="nt">-q</span> <span class="nt">-f</span> <span class="nv">status</span><span class="o">=</span>exited<span class="si">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="docker"/><summary type="html"><![CDATA[cloud]]></summary></entry></feed>