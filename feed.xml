<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bilyz98.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bilyz98.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-16T13:40:14+00:00</updated><id>https://bilyz98.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">chibicc - Simple c compiler if statement</title><link href="https://bilyz98.github.io/blog/2025/chibicc-if-statement/" rel="alternate" type="text/html" title="chibicc - Simple c compiler if statement"/><published>2025-01-14T11:59:00+00:00</published><updated>2025-01-14T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/chibicc-if-statement</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/chibicc-if-statement/"><![CDATA[<p><a href="https://github.com/BilyZ98/chibicc/commit/2f132cf8e68f0adf92bae038b75ea6da425e223c">Commit history for if statement feature</a></p> <h2 id="what-is-changed-">What is changed ?</h2> <p>For parser, new node type called <code class="language-plaintext highlighter-rouge">ND_IF</code> is introduced. Three new nodes are introduced for <code class="language-plaintext highlighter-rouge">Node</code> type in parser. They are called <code class="language-plaintext highlighter-rouge">cond</code>, <code class="language-plaintext highlighter-rouge">then</code>, <code class="language-plaintext highlighter-rouge">els</code> which corresponds to code in <code class="language-plaintext highlighter-rouge">if(cond){ } else {}</code>. New production rule is introduced to deal with <code class="language-plaintext highlighter-rouge">if</code> statement</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// stmt = "return" expr ";" </span>
<span class="c1">//        | "{" compound_stmt</span>
<span class="c1">//        | expr_stmt</span>
<span class="c1">//        | "if" "(" expr ")" stmt ("else" stmt)?</span>
<span class="k">static</span> <span class="n">Node</span><span class="o">*</span> <span class="nf">stmt</span><span class="p">(</span><span class="n">Token</span><span class="o">**</span><span class="n">rest</span><span class="p">,</span> <span class="n">Token</span><span class="o">*</span> <span class="n">tok</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"return"</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">new_unary</span><span class="p">(</span><span class="n">ND_RETURN</span><span class="p">,</span> <span class="n">expr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">));</span>
    <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">";"</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">node</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"{"</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">compound_stmt</span><span class="p">(</span><span class="n">rest</span><span class="p">,</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"if"</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">tok</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">,</span> <span class="s">"("</span><span class="p">);</span>
    <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">new_node</span><span class="p">(</span><span class="n">ND_IF</span><span class="p">);</span>
    <span class="n">node</span><span class="o">-&gt;</span><span class="n">cond</span> <span class="o">=</span> <span class="n">expr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
    <span class="n">tok</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">")"</span><span class="p">);</span>
    <span class="n">node</span><span class="o">-&gt;</span><span class="n">then</span> <span class="o">=</span> <span class="n">stmt</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"else"</span><span class="p">))</span> <span class="p">{</span>
      <span class="n">node</span><span class="o">-&gt;</span><span class="n">els</span> <span class="o">=</span> <span class="n">stmt</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">tok</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">node</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="nf">expr_stmt</span><span class="p">(</span><span class="n">rest</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p>For code generator, assembly code generation for <code class="language-plaintext highlighter-rouge">if</code> condition is introduced in <code class="language-plaintext highlighter-rouge">gen_stmt</code>. First we generate assembly code for <code class="language-plaintext highlighter-rouge">cond</code> node, and then we generate <code class="language-plaintext highlighter-rouge">je $0, %rax</code> to check condtion of <code class="language-plaintext highlighter-rouge">cond</code> node, and call <code class="language-plaintext highlighter-rouge">jump .L.else.%d</code> to do them instruction jump, <code class="language-plaintext highlighter-rouge">%d</code> is used to uniquly identify each <code class="language-plaintext highlighter-rouge">else</code> block. Because multiple <code class="language-plaintext highlighter-rouge">if</code> statement can be nested at the same time.</p>]]></content><author><name></name></author><category term="compiler"/><category term="c"/><category term="compiler"/><summary type="html"><![CDATA[Commit history for if statement feature]]></summary></entry><entry><title type="html">chibicc - Simple c compiler for statement</title><link href="https://bilyz98.github.io/blog/2025/chibicc-for-statement/" rel="alternate" type="text/html" title="chibicc - Simple c compiler for statement"/><published>2025-01-14T11:59:00+00:00</published><updated>2025-01-14T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/chibicc-for-statement</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/chibicc-for-statement/"><![CDATA[<p><a href="https://github.com/BilyZ98/chibicc/commit/ed1f13abd63cc10e7cbd76c6f6de784df0f801c1">Commit history of for statement feature</a></p> <h2 id="what-is-changed-to-introduce-for-loop-">What is changed to introduce for loop ?</h2> <p>No big changes on top of if statement feature.</p> <p>For parser, add another grammar/production rule for <code class="language-plaintext highlighter-rouge">for</code> statement.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// stmt = "return" expr ";" </span>
<span class="c1">//        | "{" compound_stmt</span>
<span class="c1">//        | expr_stmt</span>
<span class="c1">//        | "if" "(" expr ")" stmt ("else" stmt)?</span>
<span class="c1">//        | "for" "(" expr_stmt expr? ";" expr? ")" stmt </span>
<span class="k">static</span> <span class="n">Node</span><span class="o">*</span> <span class="nf">stmt</span><span class="p">(</span><span class="n">Token</span><span class="o">**</span><span class="n">rest</span><span class="p">,</span> <span class="n">Token</span><span class="o">*</span> <span class="n">tok</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"return"</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">new_unary</span><span class="p">(</span><span class="n">ND_RETURN</span><span class="p">,</span> <span class="n">expr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">));</span>
    <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">";"</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">node</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"{"</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">compound_stmt</span><span class="p">(</span><span class="n">rest</span><span class="p">,</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"if"</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">tok</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">,</span> <span class="s">"("</span><span class="p">);</span>
    <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">new_node</span><span class="p">(</span><span class="n">ND_IF</span><span class="p">);</span>
    <span class="n">node</span><span class="o">-&gt;</span><span class="n">cond</span> <span class="o">=</span> <span class="n">expr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
    <span class="n">tok</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">")"</span><span class="p">);</span>
    <span class="n">node</span><span class="o">-&gt;</span><span class="n">then</span> <span class="o">=</span> <span class="n">stmt</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"else"</span><span class="p">))</span> <span class="p">{</span>
      <span class="n">node</span><span class="o">-&gt;</span><span class="n">els</span> <span class="o">=</span> <span class="n">stmt</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">tok</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">node</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"for"</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">tok</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">,</span> <span class="s">"("</span><span class="p">);</span>
    <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">new_node</span><span class="p">(</span><span class="n">ND_FOR</span><span class="p">);</span>
    <span class="n">node</span><span class="o">-&gt;</span><span class="n">init</span> <span class="o">=</span> <span class="n">expr_stmt</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">";"</span><span class="p">))</span> <span class="p">{</span>
      <span class="n">node</span><span class="o">-&gt;</span><span class="n">cond</span> <span class="o">=</span> <span class="n">expr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">tok</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">";"</span><span class="p">);</span>

    <span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">")"</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">node</span><span class="o">-&gt;</span><span class="n">inc</span> <span class="o">=</span> <span class="n">expr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">tok</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">")"</span><span class="p">);</span>
    <span class="n">node</span><span class="o">-&gt;</span><span class="n">then</span> <span class="o">=</span> <span class="n">stmt</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
    <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">tok</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">node</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="nf">expr_stmt</span><span class="p">(</span><span class="n">rest</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
<span class="p">}</span>

</code></pre></div></div> <p>Introduce <code class="language-plaintext highlighter-rouge">init</code> and <code class="language-plaintext highlighter-rouge">inc</code> node inside of <code class="language-plaintext highlighter-rouge">Node</code> type to represent initialization and increment operation in <code class="language-plaintext highlighter-rouge">for</code> statement. <code class="language-plaintext highlighter-rouge">for(init;cond; inc){}</code></p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">typedef</span> <span class="k">struct</span> <span class="n">Node</span> <span class="n">Node</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">Node</span> <span class="p">{</span>
  <span class="n">NodeKind</span> <span class="n">kind</span><span class="p">;</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">lhs</span><span class="p">;</span> <span class="c1">//left hand side</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">rhs</span><span class="p">;</span> <span class="c1">// right hand side</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">next</span><span class="p">;</span> <span class="c1">// next node</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">body</span><span class="p">;</span> <span class="c1">// {} body node</span>

  <span class="c1">// "if" or "for" statement</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">cond</span><span class="p">;</span> 
  <span class="n">Node</span><span class="o">*</span> <span class="n">then</span><span class="p">;</span> 
  <span class="n">Node</span><span class="o">*</span> <span class="n">els</span><span class="p">;</span> 
  <span class="n">Node</span><span class="o">*</span> <span class="n">init</span><span class="p">;</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">inc</span><span class="p">;</span>


  <span class="c1">// char name;</span>
  <span class="n">Obj</span><span class="o">*</span> <span class="n">obj</span><span class="p">;</span> <span class="c1">// used if kind == ND_VAR</span>
  <span class="kt">int</span> <span class="n">val</span><span class="p">;</span>  <span class="c1">// used if kind == ND_NUM</span>
<span class="p">};</span>

</code></pre></div></div> <p>For code generator, generate <code class="language-plaintext highlighter-rouge">.L.begin.%d:</code> to indicate the start of the for block. Use <code class="language-plaintext highlighter-rouge">cmp $0, %%rax</code> and <code class="language-plaintext highlighter-rouge">jmp .L.end.%d</code> after <code class="language-plaintext highlighter-rouge">cond</code> to go out of for block. Use <code class="language-plaintext highlighter-rouge">jmp .L.begin.%d</code> to jmp back to the begining of the for block at the end. And then comparison at the begining will decide whether to jump out of the for block or not.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">case</span> <span class="n">ND_FOR</span><span class="p">:</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="n">count_depth</span><span class="p">();</span>
      <span class="n">gen_stmt</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">init</span><span class="p">);</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">".L.begin.%d:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">c</span><span class="p">);</span>
      <span class="k">if</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">cond</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">gen_expr</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">cond</span><span class="p">);</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"  cmp $0, %%rax</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"  je .L.end.%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">c</span><span class="p">);</span>
      <span class="p">}</span>

      <span class="n">gen_stmt</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">then</span><span class="p">);</span>
      <span class="k">if</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">inc</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">gen_expr</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">inc</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">"  jmp .L.begin.%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">c</span><span class="p">);</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">".L.end.%d:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">c</span><span class="p">);</span>
      <span class="k">return</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="compiler"/><category term="c"/><category term="compiler"/><summary type="html"><![CDATA[Commit history of for statement feature]]></summary></entry><entry><title type="html">Llm Kv Cache First Attempt</title><link href="https://bilyz98.github.io/blog/2025/llm-kv-cache-first-attempt/" rel="alternate" type="text/html" title="Llm Kv Cache First Attempt"/><published>2025-01-14T00:00:00+00:00</published><updated>2025-01-14T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/llm-kv-cache-first-attempt</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/llm-kv-cache-first-attempt/"><![CDATA[<p><a href="https://github.com/karpathy/nanoGPT/pull/76">nano-gpt kv cache pr example</a> <a href="https://github.com/huggingface/transformers/blob/6bc0fbcfa7acb6ac4937e7456a76c2f7975fefec/src/transformers/modeling_outputs.py#L714">huggingface transformers kv cache source code on github</a></p> <p>https://zhuanlan.zhihu.com/p/646577898</p> <p>https://zhuanlan.zhihu.com/p/624740065</p> <p><a href="https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast">huggingface transformers API documentation</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">chibicc - Simple c compiler block {} node</title><link href="https://bilyz98.github.io/blog/2025/chibicc-block/" rel="alternate" type="text/html" title="chibicc - Simple c compiler block {} node"/><published>2025-01-13T11:59:00+00:00</published><updated>2025-01-13T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/chibicc-block</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/chibicc-block/"><![CDATA[<p><a href="https://github.com/BilyZ98/chibicc/commit/50d55515fe3a882f90fec3fbee8b5795239b60f8">Commit history for block feature</a></p> <h2 id="expected-an-expression-error-after-adding-block--node">expected an expression error after adding block {} node</h2> <p>Problem:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span> <span class="nv">a</span><span class="o">=</span>3<span class="p">;</span> <span class="k">return </span>a<span class="p">;</span> <span class="o">}</span>
   ^ expected an expression
make: <span class="k">***</span> <span class="o">[</span>Makefile:12: <span class="nb">test</span><span class="o">]</span> Error 1
</code></pre></div></div> <p>Root cause:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// compound_stmt = stmt* "}"</span>
<span class="k">static</span> <span class="n">Node</span><span class="o">*</span> <span class="nf">compound_stmt</span><span class="p">(</span><span class="n">Token</span><span class="o">**</span> <span class="n">rest</span><span class="p">,</span> <span class="n">Token</span><span class="o">*</span> <span class="n">tok</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">Node</span> <span class="n">head</span> <span class="o">=</span> <span class="p">{};</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">cur</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">head</span><span class="p">;</span>
  <span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"}"</span><span class="p">))</span> <span class="p">{</span>
    <span class="c1">// This is the bug, should be tok not tok-&gt;next;</span>
    <span class="n">cur</span> <span class="o">=</span> <span class="n">cur</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">stmt</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">new_node</span><span class="p">(</span><span class="n">ND_BLOCK</span><span class="p">);</span>
  <span class="n">node</span><span class="o">-&gt;</span><span class="n">body</span> <span class="o">=</span> <span class="n">head</span><span class="p">.</span><span class="n">next</span><span class="p">;</span>
  <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>

  <span class="k">return</span> <span class="n">node</span><span class="p">;</span>

<span class="p">}</span>
</code></pre></div></div> <p>Fix:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// compound_stmt = stmt* "}"</span>
<span class="k">static</span> <span class="n">Node</span><span class="o">*</span> <span class="nf">compound_stmt</span><span class="p">(</span><span class="n">Token</span><span class="o">**</span> <span class="n">rest</span><span class="p">,</span> <span class="n">Token</span><span class="o">*</span> <span class="n">tok</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">Node</span> <span class="n">head</span> <span class="o">=</span> <span class="p">{};</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">cur</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">head</span><span class="p">;</span>
  <span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"}"</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">cur</span> <span class="o">=</span> <span class="n">cur</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">stmt</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">new_node</span><span class="p">(</span><span class="n">ND_BLOCK</span><span class="p">);</span>
  <span class="n">node</span><span class="o">-&gt;</span><span class="n">body</span> <span class="o">=</span> <span class="n">head</span><span class="p">.</span><span class="n">next</span><span class="p">;</span>
  <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>

  <span class="k">return</span> <span class="n">node</span><span class="p">;</span>

<span class="p">}</span>
</code></pre></div></div> <h2 id="what-is-done-to-introduce-block-concept">What is done to introduce block concept?</h2> <p>Introduce another node in node struct called <code class="language-plaintext highlighter-rouge">body</code> to store the code content inside block</p> <p>No change is made to tokenizer</p> <p>For parser, introduce <code class="language-plaintext highlighter-rouge">compound_stmt</code> production/grammar rule for <code class="language-plaintext highlighter-rouge">stmt</code> generation rule.</p> <p>For code generator, start generating code from <code class="language-plaintext highlighter-rouge">Function-&gt;body</code> part which is a block itself. Each <code class="language-plaintext highlighter-rouge">body</code> inside each block has its own list of nodes. Previously we only have on list of nodes. Now we have one list of nodes for each block node.</p> <p>I guess this is for variable scope purpose, althought this has not been done in this commit history.</p>]]></content><author><name></name></author><category term="compiler"/><category term="c"/><category term="compiler"/><summary type="html"><![CDATA[Commit history for block feature]]></summary></entry><entry><title type="html">chibicc - Simple c compiler return keyword</title><link href="https://bilyz98.github.io/blog/2025/chibicc-return-keyword/" rel="alternate" type="text/html" title="chibicc - Simple c compiler return keyword"/><published>2025-01-12T11:59:00+00:00</published><updated>2025-01-12T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/chibicc-return-keyword</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/chibicc-return-keyword/"><![CDATA[<h2 id="add-return-keyword-to-simple-c-compiler">Add return keyword to simple c compiler</h2> <p><a href="https://github.com/BilyZ98/chibicc/commit/f94ca394ade26ca861bd205d3714f103eb4dedb9">Commit history</a></p> <p>For tokenizer, <code class="language-plaintext highlighter-rouge">convert_keywords()</code> is added to convert kind of token from identity to keyword. So this means that all basic tokens are identity at first and later convert to keywrod type token.</p> <p>For parser, add extra prudction rule/grammar rule in expr geneartion. <code class="language-plaintext highlighter-rouge">stmt = "return" expr ";" | expr-stmt</code>. New node type <code class="language-plaintext highlighter-rouge">ND_RETURN</code> is added.</p> <p>For code generation, <code class="language-plaintext highlighter-rouge">jmp .L.return</code> is added to jump to specified assembly code.</p> <p>For test, add <code class="language-plaintext highlighter-rouge">return 1; 2; 3</code> to test return actually works.</p>]]></content><author><name></name></author><category term="compiler"/><category term="c"/><category term="compiler"/><summary type="html"><![CDATA[Add return keyword to simple c compiler]]></summary></entry><entry><title type="html">Learned idnex survey</title><link href="https://bilyz98.github.io/blog/2025/learned-index-survery/" rel="alternate" type="text/html" title="Learned idnex survey"/><published>2025-01-08T11:59:00+00:00</published><updated>2025-01-08T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/learned-index-survery</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/learned-index-survery/"><![CDATA[<h2 id="what-is-learned-index-and-why">What is learned index and why?</h2> <p>To search a key in b+ tree, traditional way is to do binary search. This complexity time is O(Log_n)</p> <p>Learned index build model to learn the distribution of keys in space. The input is the key number and the output of the model is the location of the key in the storage array space. The time complexity is O(1) which is faster than binary search.</p> <h2 id="current-work">Current work</h2> <p><a href="https://arxiv.org/pdf/1712.01208">The case for learned index structures</a> proposes to replace every node in b+ tree with learned model.</p> <p>Each node contains a model trained from the keys covered by its key ranges.</p> <p>It’s said that in the paper that this hierachy style allows allows model to learn the rough key distribtuion at its key ranges. Bottom level model covers smalles range of keys, it’s easy for the model learn. Top level covers larget range of keys but it only cares about the big structure of the keys distribution and it leaves fine-grained key position to bottom level models.</p> <p>Search process:</p> <p>Model at each level gives its key position prediction until reaches the leaf child.</p> <p>Since model might not give the correct position prediction, learned index make sures that each model prediction error is within a predefined error bound.</p> <p>So it’s guarantee that key is located within the [lower_bound, upper_bound] range given by the model.</p> <p>This paper only mentions how to build model for read only scenario.</p> <p><a href="https://arxiv.org/pdf/1905.08898">ALEX</a> solves the updatable learned index problem.</p> <p>How ?</p> <p>Insert process:</p> <p>For non-full data node, it inserts the key to the predicted position from the model if there is a empty slow in the array..</p> <p>The predcited position might be occupied, so it shifts other elements towards the closet gap by one. ( I am not sure if the model should be trained after this shift operation, I think we should train a new model because that position of shifted key is changed, but the model train cost is too high if we do this for each occupied key.)</p> <p>For full-data node, it can choose to split the data node or matain a single data node but with allocation of largers storage space and retarined a new model.</p> <p>For data node expansion, new position in array of original key is given by new model.</p> <p>This is not the same as traditional binary tree which just does simple copy.</p> <p>For internal nodes, ALEX choose to split the internal nodes horizontally, i.e at the same level.</p> <p>Alex can choose to turn data nodes to internal nodes which is the same as split the node vertically, increasing the depth of the tree?</p> <h2 id="whats-next-">What’s next ?</h2> <p><a href="Learned index survey">https://arxiv.org/pdf/2403.06456</a> talks about development of learned index. It classifies learned index into multi-dimensional and mutable/immutable categories.</p> <p>What’s not talked about is how learned index is efficiently integrated into existing system.</p> <h2 id="references">References</h2> <p><a href="https://www.youtube.com/watch?v=2A3tiAmaq_c">Jeaf dean’s talk about ml for sys in NIPS’25</a></p> <p><a href="https://zongheng.me/pubs/qdtree-sigmod20.pdf">qd-tree</a> . Why only for data analytics?</p> <p><a href="https://www.cidrdb.org/cidr2019/papers/p117-kraska-cidr19.pdf">SageDB: A Learned Database System</a></p> <p><a href="https://dbgroup.cs.tsinghua.edu.cn/ligl/papers/experiment-learned-index.pdf">Learned Index: A Comprehensive Experimental Evaluation</a></p> <p><a href="https://www.usenix.org/conference/osdi20/presentation/wei">Fast RDMA-based Ordered Key-Value Store using Remote Learned Cache</a></p>]]></content><author><name></name></author><category term="learned"/><category term="index"/><category term="research"/><category term="learned"/><category term="index"/><category term="research"/><summary type="html"><![CDATA[What is learned index and why? To search a key in b+ tree, traditional way is to do binary search. This complexity time is O(Log_n)]]></summary></entry><entry><title type="html">chibicc C compiler - multi char variable name</title><link href="https://bilyz98.github.io/blog/2025/chibicc-multi-char-var-name/" rel="alternate" type="text/html" title="chibicc C compiler - multi char variable name"/><published>2025-01-05T11:59:00+00:00</published><updated>2025-01-05T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/chibicc-multi-char-var-name</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/chibicc-multi-char-var-name/"><![CDATA[<h3 id="find-bug-in-code-that-leads-to-seg-fault">Find bug in code that leads to seg fault.</h3> <p><a href="https://github.com/BilyZ98/chibicc/commit/3ca91cc6431246e1b23f4503b6442e77e7457246">https://github.com/BilyZ98/chibicc/commit/3ca91cc6431246e1b23f4503b6442e77e7457246</a></p> <p>The bug code is at parse.c. I use gdb to find out the bug location.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>77      ../sysdeps/x86_64/multiarch/strlen-evex.S: No such file or directory.
(gdb) bt
#0  __strlen_evex () at ../sysdeps/x86_64/multiarch/strlen-evex.S:77
#1  0x0000555555555ff7 in find_var (start=0x7fffffffe399 "a;", len=1) at parse.c:194
#2  0x0000555555556140 in primary (rest=0x7fffffffdd30, tok=0x55555555a360) at parse.c:220
#3  0x0000555555555fbf in unary (rest=0x7fffffffdd30, tok=0x55555555a360) at parse.c:187
#4  0x0000555555555e73 in mul (rest=0x7fffffffdd60, tok=0x55555555a360) at parse.c:160
#5  0x0000555555555d9a in add (rest=0x7fffffffdd90, tok=0x55555555a360) at parse.c:138
#6  0x0000555555555c1f in relational (rest=0x7fffffffddc0, tok=0x55555555a360) at parse.c:109
#7  0x0000555555555b46 in equality (rest=0x7fffffffddf0, tok=0x55555555a360) at parse.c:90
#8  0x0000555555555ac1 in assign (rest=0x7fffffffde40, tok=0x55555555a360) at parse.c:78
#9  0x0000555555555a98 in expr (rest=0x7fffffffde40, tok=0x55555555a360) at parse.c:73
#10 0x0000555555555a3d in expr_stmt (rest=0x7fffffffde98, tok=0x55555555a360) at parse.c:65
#11 0x00005555555559e4 in stmt (rest=0x7fffffffde98, tok=0x55555555a360) at parse.c:46
#12 0x0000555555556248 in parse (tok=0x55555555a360) at parse.c:245
#13 0x00005555555558c9 in main (argc=2, argv=0x7fffffffe038) at main.c:9
</code></pre></div></div> <p>function</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="n">Obj</span><span class="o">*</span> <span class="nf">find_var</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span> <span class="n">start</span><span class="p">,</span> <span class="kt">int</span> <span class="n">len</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span><span class="p">(</span><span class="n">Obj</span><span class="o">*</span> <span class="n">o_idx</span> <span class="o">=</span> <span class="n">local_obj_ptr</span><span class="p">;</span> <span class="n">o_idx</span><span class="p">;</span> <span class="n">o_idx</span><span class="o">=</span><span class="n">o_idx</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span><span class="p">(</span><span class="n">len</span> <span class="o">==</span> <span class="n">strlen</span><span class="p">(</span><span class="n">o_idx</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">strncmp</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">o_idx</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">len</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">o_idx</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Finally found the bug.</p> <p>I did not include this define at <code class="language-plaintext highlighter-rouge">chibicc.h</code></p> <p>lol.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#define _POSIX_C_SOURCE 200809L
</code></pre></div></div> <p>This line of code enable functions like <code class="language-plaintext highlighter-rouge">strndup, getline, and clock_gettime</code> which are not part of the standard C library but are available in the POSIX standard. Spend really long time on this.</p> <p>Correct code:</p> <p><a href="https://github.com/BilyZ98/chibicc/tree/1651006f9035bb0a1f3afffd1fb328948006cc28">https://github.com/BilyZ98/chibicc/tree/1651006f9035bb0a1f3afffd1fb328948006cc28</a></p>]]></content><author><name></name></author><category term="compiler"/><category term="c"/><category term="compiler"/><category term="cpp"/><summary type="html"><![CDATA[Find bug in code that leads to seg fault.]]></summary></entry><entry><title type="html">C++ and linux kernel memory allocation</title><link href="https://bilyz98.github.io/blog/2025/cpp-interview-questions/" rel="alternate" type="text/html" title="C++ and linux kernel memory allocation"/><published>2025-01-02T11:59:00+00:00</published><updated>2025-01-02T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/cpp-interview-questions</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/cpp-interview-questions/"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/EIVDCWHr8EFEl4LX6UqWHQ">How malloc allocates memory?</a></p> <p>kernel memory allocation</p> <ul> <li> <p>buddy algorithm</p> </li> <li> <p>slab allocator</p> </li> </ul> <p>The slab allocator and buddy algorithm are two different memory allocation techniques used in the Linux kernel, each with its own strengths and use cases.</p> <h3 id="buddy-algorithm">Buddy Algorithm</h3> <p>The buddy algorithm is used for allocating memory in large contiguous blocks. It works by dividing memory into blocks of size 2^n and maintaining a binary tree structure. When a memory request is made, the allocator finds the smallest block that can satisfy the request. If a larger block is found, it is split into smaller blocks (buddies) until the desired size is obtained. This method is efficient for allocating large blocks of memory and helps reduce fragmentation](https://www.tutorialspoint.com/allocating-kernel-memory-buddy-system-and-slab-system).</p> <h3 id="slab-allocator">Slab Allocator</h3> <p>The slab allocator, on the other hand, is designed for allocating small, frequently used objects. It maintains caches of pre-allocated memory chunks (slabs) for different object sizes. When an object is needed, it is allocated from the appropriate slab, which reduces the overhead of frequent allocations and deallocations. This method is particularly useful for kernel objects like process descriptors, file descriptors, and other small data structures](https://www.tutorialspoint.com/allocating-kernel-memory-buddy-system-and-slab-system).</p> <h3 id="relationship">Relationship</h3> <p>The relationship between the two lies in their complementary roles within the Linux kernel. The buddy algorithm is used for allocating large contiguous blocks of memory, which can then be subdivided by the slab allocator for smaller, frequently used objects. This combination allows the Linux kernel to efficiently manage memory for both large and small allocations, optimizing performance and reducing fragmentation](https://www.tutorialspoint.com/allocating-kernel-memory-buddy-system-and-slab-system).</p> <p><a href="https://www.tutorialspoint.com/allocating-kernel-memory-buddy-system-and-slab-system">ref 1</a></p> <h3 id="which-algorithm-does-linux-use-when-brk-syscall-is-called-to-allocate-memory-">which algorithm does linux use when brk() syscall is called to allocate memory ?</h3> <p>When the <code class="language-plaintext highlighter-rouge">brk()</code> system call is used in Linux to allocate memory, it primarily relies on the <strong>buddy algorithm</strong> for managing the memory. The <code class="language-plaintext highlighter-rouge">brk()</code> system call adjusts the end of the data segment of a process, effectively increasing or decreasing the amount of memory allocated to the process. The buddy algorithm is used to allocate large contiguous blocks of memory, which can then be managed by the process.</p> <h3 id="difference-between-malloc-and-new-in-c-">Difference between malloc and new in c++ ?</h3> <p>malloc is a system call function. When calling malloc it will not call constructor of class or do memory initialization. <code class="language-plaintext highlighter-rouge">free</code> to free memory.</p> <p>new is a primitive word in c++ itself. <code class="language-plaintext highlighter-rouge">new</code> will call constructor of class and do memory initialization. <code class="language-plaintext highlighter-rouge">new</code> is managed by c++ compiler. <code class="language-plaintext highlighter-rouge">delete</code> to free memory and destructor is called for <code class="language-plaintext highlighter-rouge">delete</code></p> <h3 id="memory-allocation-for-process-in-linux">Memory allocation for process in linux</h3> <ol> <li> <p>For small memory allocation <code class="language-plaintext highlighter-rouge">brk</code> is called.</p> <p><code class="language-plaintext highlighter-rouge">brk</code> will allocate memory from heap and the allocated memory will not be returned to kernel immediately after <code class="language-plaintext highlighter-rouge">free</code>. It will stay in the process memory mapping space for future use.</p> </li> <li> <p>For large memory allocation <code class="language-plaintext highlighter-rouge">mmap</code> is called. For memory size allocation request &gt; 128KB, memory allocated is in file mapping space in process memory space. The physical memory is returned to kernel immediately after <code class="language-plaintext highlighter-rouge">free</code> call which is different from <code class="language-plaintext highlighter-rouge">brk</code></p> </li> </ol> <p><a href="https://mp.weixin.qq.com/s/EIVDCWHr8EFEl4LX6UqWHQ">ref article about memory allocation </a></p>]]></content><author><name></name></author><category term="compiler"/><category term="c"/><category term="compiler"/><category term="cpp"/><category term="linux"/><summary type="html"><![CDATA[How malloc allocates memory?]]></summary></entry><entry><title type="html">chibicc C compiler - parser review and expression evaluator</title><link href="https://bilyz98.github.io/blog/2025/chibicc-parser-review-and-calculator/" rel="alternate" type="text/html" title="chibicc C compiler - parser review and expression evaluator"/><published>2025-01-01T11:59:00+00:00</published><updated>2025-01-01T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2025/chibicc-parser-review-and-calculator</id><content type="html" xml:base="https://bilyz98.github.io/blog/2025/chibicc-parser-review-and-calculator/"><![CDATA[<h2 id="lea-and-mov-explanation">lea and mov explanation</h2> <p><code class="language-plaintext highlighter-rouge">lea -4(%rbp), %rax</code> means that memory address that is 4 bytes below current base pointer <code class="language-plaintext highlighter-rouge">rbp</code> is stored in <code class="language-plaintext highlighter-rouge">rax</code> register.</p> <p>For example,</p> <p>assume <code class="language-plaintext highlighter-rouge">%rbp</code> holds value <code class="language-plaintext highlighter-rouge">0x7fffffffddd0</code> offset is 8,</p> <p>then this code</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lea -8(%rbp), %rax
</code></pre></div></div> <p>means</p> <ol> <li>the effective address calculation is <code class="language-plaintext highlighter-rouge">0x7fffffffddd0 -8 = 0x7fffffffdd8</code></li> <li>The value <code class="language-plaintext highlighter-rouge">0x7fffffffdd8</code> is stored in <code class="language-plaintext highlighter-rouge">rax</code> register</li> </ol> <p>I get confused about whether <code class="language-plaintext highlighter-rouge">mov src, dest</code> or <code class="language-plaintext highlighter-rouge">mov dest, src</code> is correct.</p> <p>I learn from this <a href="https://stackoverflow.com/questions/5890724/mov-instruction-in-x86-assembly">stack overflow post about mov in x86 and AT&amp;T</a> that both are valid.</p> <p><code class="language-plaintext highlighter-rouge">mov src, dest</code> is correct in AT&amp;T and <code class="language-plaintext highlighter-rouge">mov dest, src</code> is valid in Intel syntax.</p> <p><a href="https://electronicsreference.com/assembly-language/mov/">Intel x86 mov explanation</a></p> <h3 id="gcc-uses-att-assembly-standard">gcc uses AT&amp;T assembly standard</h3> <p>GCC (GNU Compiler Collection) uses the AT&amp;T assembly syntax by default. This is the standard assembly syntax used in Unix-like systems. However, GCC also supports Intel syntax, and you can switch to it using specific compiler flags if needed.</p> <p>If you’re working on a project and need to use Intel syntax, you can enable it with the <code class="language-plaintext highlighter-rouge">-masm=intel</code> flag. For example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcc <span class="nt">-masm</span><span class="o">=</span>intel <span class="nt">-o</span> myprogram myprogram.c
</code></pre></div></div> <h2 id="simple-math-expression-evaluator">Simple math expression evaluator</h2> <p>Problem:</p> <p>Implement math expresion evaluator for expression containing <code class="language-plaintext highlighter-rouge">+,-,*,()</code></p> <p>Solution:</p> <p>This problem is very typical and important. It is a problem that uses stack to solve the priority issue between different operator like <code class="language-plaintext highlighter-rouge">+</code> and <code class="language-plaintext highlighter-rouge">*</code>.</p> <p>Since <code class="language-plaintext highlighter-rouge">*</code> has higher priority than <code class="language-plaintext highlighter-rouge">+</code>, so when we meet <code class="language-plaintext highlighter-rouge">+</code> during string scan we can not evalute it immediately because there might be a <code class="language-plaintext highlighter-rouge">*</code> following this <code class="language-plaintext highlighter-rouge">+</code>.</p> <p>For example <code class="language-plaintext highlighter-rouge">3+2*3</code>, we should evaluate <code class="language-plaintext highlighter-rouge">2*3</code> first.</p> <p><a href="https://www.nowcoder.com/practice/c215ba61c8b1443b996351df929dc4d4">ref two stack implementation for expression evaluator</a></p> <ol> <li> <p>for <code class="language-plaintext highlighter-rouge">(</code> , we just push it into ops stack</p> </li> <li> <p>for ‘)’ , we keep evaluating expression until <code class="language-plaintext highlighter-rouge">(</code> is met on <code class="language-plaintext highlighter-rouge">ops</code> stack</p> </li> <li> <p>for numbers, we just push it into nums stack.</p> </li> <li> <p>for <code class="language-plaintext highlighter-rouge">+,-,*</code>, we evaluate the expression in <code class="language-plaintext highlighter-rouge">ops</code> stack if priority of the op of the top of <code class="language-plaintext highlighter-rouge">ops</code> stack is equal or higher than current op in string.</p> </li> </ol> <p>Pay attention that we need to stop if top of the <code class="language-plaintext highlighter-rouge">ops</code> stack is <code class="language-plaintext highlighter-rouge">(</code>, because <code class="language-plaintext highlighter-rouge">()</code> has highest priority.</p> <p>Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="cp">#include</span> <span class="cpf">&lt;cctype&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;string&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stack&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;unordered_map&gt;</span><span class="cp">
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>

<span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">char</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">op_pri</span> <span class="o">=</span> <span class="p">{</span>
  <span class="p">{</span><span class="sc">'+'</span><span class="p">,</span><span class="mi">1</span><span class="p">},</span>
  <span class="p">{</span><span class="sc">'-'</span><span class="p">,</span> <span class="mi">1</span><span class="p">},</span>
  <span class="p">{</span><span class="sc">'*'</span><span class="p">,</span><span class="mi">2</span><span class="p">}</span>
<span class="p">};</span>

<span class="kt">int</span> <span class="nf">cal_one</span><span class="p">(</span><span class="n">stack</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;&amp;</span> <span class="n">ops</span><span class="p">,</span> <span class="n">stack</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span> <span class="n">nums</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="n">nums</span><span class="p">.</span><span class="n">top</span><span class="p">();</span> 
  <span class="n">nums</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
  <span class="kt">int</span> <span class="n">a</span> <span class="o">=</span> <span class="n">nums</span><span class="p">.</span><span class="n">top</span><span class="p">();</span>
  <span class="n">nums</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>

  <span class="kt">char</span> <span class="n">c</span>  <span class="o">=</span> <span class="n">ops</span><span class="p">.</span><span class="n">top</span><span class="p">();</span>
  <span class="n">ops</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
  <span class="kt">int</span> <span class="n">num</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">c</span> <span class="o">==</span> <span class="sc">'+'</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">num</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span>

  <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">c</span> <span class="o">==</span> <span class="sc">'-'</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">num</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">;</span>
  <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">c</span> <span class="o">==</span> <span class="sc">'*'</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">num</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="n">num</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="nf">solve</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">s</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">stack</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span> <span class="n">ops</span><span class="p">;</span>
  <span class="n">stack</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">nums</span><span class="p">;</span>

  <span class="n">nums</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">.</span><span class="n">length</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">char</span> <span class="n">c</span><span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>

    <span class="k">if</span><span class="p">(</span><span class="n">c</span> <span class="o">==</span> <span class="sc">'('</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">ops</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">c</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">c</span> <span class="o">==</span> <span class="sc">')'</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span><span class="p">(</span><span class="n">ops</span><span class="p">.</span><span class="n">top</span><span class="p">()</span> <span class="o">!=</span> <span class="sc">'('</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">int</span> <span class="n">res</span> <span class="o">=</span> <span class="n">cal_one</span><span class="p">(</span><span class="n">ops</span><span class="p">,</span> <span class="n">nums</span><span class="p">);</span>
          <span class="n">nums</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">res</span><span class="p">);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
          <span class="n">ops</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
          <span class="k">break</span><span class="p">;</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="c1">// digit</span>
      <span class="k">if</span><span class="p">(</span><span class="n">isdigit</span><span class="p">(</span><span class="n">c</span><span class="p">))</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">cur_num</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">while</span><span class="p">(</span><span class="n">j</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">.</span><span class="n">length</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">isdigit</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span> <span class="p">{</span>
          <span class="n">cur_num</span> <span class="o">=</span> <span class="n">cur_num</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">s</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">-</span><span class="sc">'0'</span><span class="p">;</span>
          <span class="n">j</span><span class="o">++</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span>
        <span class="n">nums</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">cur_num</span><span class="p">);</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>

        <span class="c1">// + - *</span>
        <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="sc">'('</span> <span class="o">||</span> <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="sc">'-'</span> <span class="o">||</span> <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="sc">'+'</span><span class="p">))</span> <span class="p">{</span>
          <span class="n">nums</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="c1">// This ops.top() != '(' is important. (expr) is highest priority</span>
        <span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">ops</span><span class="p">.</span><span class="n">empty</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">ops</span><span class="p">.</span><span class="n">top</span><span class="p">()</span> <span class="o">!=</span> <span class="sc">'('</span><span class="p">)</span> <span class="p">{</span>
          <span class="k">if</span><span class="p">(</span><span class="n">op_pri</span><span class="p">[</span><span class="n">ops</span><span class="p">.</span><span class="n">top</span><span class="p">()]</span> <span class="o">&gt;=</span> <span class="n">op_pri</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>  <span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">res</span> <span class="o">=</span> <span class="n">cal_one</span><span class="p">(</span><span class="n">ops</span><span class="p">,</span> <span class="n">nums</span><span class="p">);</span>
            <span class="n">nums</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">res</span><span class="p">);</span>
          <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="k">break</span><span class="p">;</span>
          <span class="p">}</span>
        <span class="p">}</span>
        <span class="n">ops</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">c</span><span class="p">);</span>
      <span class="p">}</span>

    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">ops</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">res</span> <span class="o">=</span> <span class="n">cal_one</span><span class="p">(</span><span class="n">ops</span><span class="p">,</span> <span class="n">nums</span><span class="p">);</span>
    <span class="n">nums</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">res</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="n">nums</span><span class="p">.</span><span class="n">top</span><span class="p">();</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">argv</span><span class="p">[])</span> <span class="p">{</span>


  <span class="n">string</span> <span class="n">s</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>


  <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">s</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>

  <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">solve</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>


<span class="p">}</span>
</code></pre></div></div> <p>Test case:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ g++ math_eval.cc
<span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ ./a.out <span class="s2">"1"</span>
1
1
<span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ ./a.out <span class="s2">"2"</span>
2
2
<span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ ./a.out <span class="s2">"2+1"</span>
2+1
3
<span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ ./a.out <span class="s2">"2+2*5"</span>
2+2<span class="k">*</span>5
12
<span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ ./a.out <span class="s2">"2+2*5-8"</span>
2+2<span class="k">*</span>5-8
4
<span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ ./a.out <span class="s2">"2+2*(5-8)"</span>
2+2<span class="k">*</span><span class="o">(</span>5-8<span class="o">)</span>
<span class="nt">-4</span>
<span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ ./a.out <span class="s2">"2+2*(5-8)*3"</span>
2+2<span class="k">*</span><span class="o">(</span>5-8<span class="o">)</span><span class="k">*</span>3
<span class="nt">-16</span>
<span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ ./a.out <span class="s2">"2+2*(5-8*2)"</span>
2+2<span class="k">*</span><span class="o">(</span>5-8<span class="k">*</span>2<span class="o">)</span>
<span class="nt">-20</span>
<span class="o">(</span>base<span class="o">)</span> ➜  chibicc git:<span class="o">(</span>main<span class="o">)</span> ✗ ./a.out <span class="s2">"2+2*(5*8-2)"</span>
2+2<span class="k">*</span><span class="o">(</span>5<span class="k">*</span>8-2<span class="o">)</span>
78
</code></pre></div></div>]]></content><author><name></name></author><category term="compiler"/><category term="c"/><category term="compiler"/><summary type="html"><![CDATA[lea and mov explanation]]></summary></entry><entry><title type="html">Autodiff implementation - kernel and memory management</title><link href="https://bilyz98.github.io/blog/2024/uw-sysml-assign2/" rel="alternate" type="text/html" title="Autodiff implementation - kernel and memory management"/><published>2024-12-24T11:59:00+00:00</published><updated>2024-12-24T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/uw-sysml-assign2</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/uw-sysml-assign2/"><![CDATA[<p>This is fun to implement</p> <p>Never aware that we can do management in python language like this.</p> <p>I always think that memory management in only done in cpp.</p> <p>Interesting, so interesting.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def memory_plan(self, feed_shapes):
        """Allocates tvm.nd.array for every node except feed_dict nodes.

        Implementation note:
        Easy Option: Alloc a tvm.nd.array per node that persists across run()

        Use self.node_to_arr_map to store node-&gt;tvm.nd.array mapping
        to allow mapping to persist across multiple executor.run().

        Parameters
        ----------
        feed_shapes: node-&gt;shapes mapping for feed_dict nodes.
        """
        """TODO: Your code here"""


</code></pre></div></div> <h2 id="what-is-placeholder-in-tvm-">What is placeholder in tvm ?</h2> <p>In TVM, a <code class="language-plaintext highlighter-rouge">placeholder</code> is used to define an input tensor for a computation. It essentially acts as a symbolic representation of the input data, specifying the shape and optionally the data type. This is useful for defining the structure of the computation graph before actual data is provided.</p> <p>Here’s a simple example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="c1"># Define the shape of the input tensor
</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="sh">"</span><span class="s">float32</span><span class="sh">"</span>

<span class="c1"># Create a placeholder tensor
</span><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Print the placeholder tensor
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Placeholder tensor A:</span><span class="sh">"</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</code></pre></div></div> <p>In this example:</p> <ul> <li><code class="language-plaintext highlighter-rouge">te.placeholder</code> creates a tensor <code class="language-plaintext highlighter-rouge">A</code> with the specified shape <code class="language-plaintext highlighter-rouge">(3, 4)</code> and data type <code class="language-plaintext highlighter-rouge">"float32"</code>.</li> <li>This tensor can then be used in further computations, such as defining operations that use <code class="language-plaintext highlighter-rouge">A</code> as input.</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">placeholder</code> is a fundamental concept in TVM’s Tensor Expression (TE) language, allowing you to build complex computation graphs by defining how data flows through various operations¹².</p> <p>¹: <a href="https://tvm.apache.org/docs/reference/api/python/te.html">TVM Documentation</a></p> <p>²: <a href="https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html">TVM Tutorial on Tensor Expressions</a></p> <p>(1) tvm.te — tvm 0.19.dev0 documentation - The Apache Software Foundation. https://tvm.apache.org/docs/reference/api/python/te.html.</p> <p>(2) TVM 自底向上（三）：TE 的概念和编译原理 - 知乎. https://zhuanlan.zhihu.com/p/534313816.</p> <p>(3) Working with Operators Using Tensor Expression — tvm 0.19.dev0 …. https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html.</p> <p>(4) TVM学习（六）细读前端 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/346514871.</p> <p>(5) External Tensor Functions — tvm 0.19.dev0 documentation. https://tvm.apache.org/docs/how_to/work_with_schedules/extern_op.html.</p> <h2 id="tvm-create-ndarray-with-specified-shape">tvm create nd.array with specified shape</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="c1"># Specify the shape and data type
</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="sh">"</span><span class="s">float32</span><span class="sh">"</span>

<span class="c1"># Create an empty TVM NDArray with the specified shape
</span><span class="n">tvm_array</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Print the TVM NDArray
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TVM NDArray with specified shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tvm_array</span><span class="p">)</span>

</code></pre></div></div> <h2 id="difference-between-tvm-and-ncnn">Difference between tvm and ncnn</h2> <p>tvm is a open source deep learning compiler for cpus, gpus and specialized hardware.</p> <p>ncnn is a neural network inference framework optimized for mobile and embedded devices.</p> <p>We can assume input shape of mat_mul is 2d in this task.</p> <h2 id="matmul-kernel-for-unknown-input-dimension-shape">matmul kernel for unknown input dimension shape</h2> <p>This code is from gpt.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="k">def</span> <span class="nf">make_flexible_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="c1"># Determine the shapes of the input tensors
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">shapeA</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">shapeA</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">shapeA</span>
    
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">shapeB</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">shapeB</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">shapeB</span>
    
    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch</span> <span class="nf">else </span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch</span> <span class="nf">else </span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Create placeholders for the input tensors
</span>    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    
    <span class="c1"># Define the reduction axis
</span>    <span class="n">k_axis</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># Compute the matrix multiplication based on transpose flags and dimensionality
</span>    <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeA</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeA</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
    
    <span class="c1"># Create a schedule for the computation
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    
    <span class="c1"># Apply optimizations: split, reorder, vectorize, parallel
</span>    <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">bo</span><span class="p">,</span> <span class="n">bi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">io</span><span class="p">,</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">jo</span><span class="p">,</span> <span class="n">ji</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">bo</span><span class="p">,</span> <span class="n">io</span><span class="p">,</span> <span class="n">jo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">bi</span><span class="p">,</span> <span class="n">ii</span><span class="p">,</span> <span class="n">ji</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">bo</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">io</span><span class="p">,</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">jo</span><span class="p">,</span> <span class="n">ji</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">io</span><span class="p">,</span> <span class="n">jo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">ii</span><span class="p">,</span> <span class="n">ji</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">io</span><span class="p">)</span>
    
    <span class="c1"># Lower the schedule to generate the IR code
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">tvm</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    
    <span class="c1"># Build the function
</span>    <span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">func</span>

<span class="c1"># Example usage
</span><span class="n">tgt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">tgt_host</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">func_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">flexible_matrix_mul</span><span class="sh">"</span>
<span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># Batch size of 32
</span><span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>  <span class="c1"># Batch size of 32
</span><span class="n">transposeA</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">transposeB</span> <span class="o">=</span> <span class="bp">False</span>
<span class="nf">make_flexible_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">)</span>

</code></pre></div></div> <h2 id="conv2d-tvm-kernel">conv2d tvm kernel</h2> <p>Code is generated by gpt</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span><span class="p">,</span> <span class="n">topi</span>

<span class="k">def</span> <span class="nf">make_conv2d</span><span class="p">(</span><span class="n">shapeX</span><span class="p">,</span> <span class="n">shapeF</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">assert</span><span class="p">(</span><span class="n">shapeX</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeF</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">shapeX</span>
    <span class="n">M</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">shapeF</span>

    <span class="c1"># Create placeholders for the input tensor and filter
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">F</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># Define the reduction axes
</span>    <span class="n">rc</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rc</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">rr</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">R</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rr</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">rs</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rs</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Compute the convolution
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">H</span> <span class="o">-</span> <span class="n">R</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">S</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
        <span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="n">rr</span><span class="p">,</span> <span class="n">w</span> <span class="o">+</span> <span class="n">rs</span><span class="p">]</span> <span class="o">*</span> <span class="n">F</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">]),</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span>
    <span class="p">)</span>

    <span class="c1"># Create a schedule for the computation
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">Y</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># Apply optimizations: split, reorder, vectorize, parallel
</span>    <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
    <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span>
    <span class="n">ho</span><span class="p">,</span> <span class="n">hi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">wo</span><span class="p">,</span> <span class="n">wi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">ho</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">hi</span><span class="p">,</span> <span class="n">wi</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">ho</span><span class="p">)</span>

    <span class="c1"># Lower the schedule to generate the IR code
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">tvm</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="c1"># Build the function
</span>    <span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">func</span>

<span class="c1"># Example usage
</span><span class="n">tgt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">tgt_host</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">func_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">conv2d</span><span class="sh">"</span>
<span class="n">shapeX</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>  <span class="c1"># Example input shape (N, C, H, W)
</span><span class="n">shapeF</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   <span class="c1"># Example filter shape (M, C, R, S)
</span><span class="nf">make_conv2d</span><span class="p">(</span><span class="n">shapeX</span><span class="p">,</span> <span class="n">shapeF</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">)</span>


</code></pre></div></div> <h2 id="install-tvm-by-building-from-source">Install tvm by building from source</h2> <p>Can not install tvm through pip. Have to download source code and build it myself. I don’t know why.</p> <p>Folow steps in this <a href="https://github.com/apache/tvm/issues/13507">issue</a> to compile locally.</p> <p>Need to disable gtest inroder to pass cmake</p> <p><a href="https://tvm.apache.org/docs/install/from_source.html#install-from-source">Offical install document</a></p> <p>Finally finish installing tvm after building locally.</p> <p>Run this command to verify</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>uwsyml<span class="o">)</span> ➜  tvm git:<span class="o">(</span>main<span class="o">)</span> python <span class="nt">-c</span> <span class="s2">"import tvm; print(tvm.__file__)"</span>
/mnt/nvme1n1/zt/tvm/python/tvm/__init__.py
</code></pre></div></div> <p>nosetests does not use python in conda. It uses that in /usr/bin which is not what I want.</p> <p>It reports error that it can not find numpy which I have already installed in conda environment</p> <h2 id="update-code-to-use-latest-function-in-tvm-instead-of-old-function-in-tvm">Update code to use latest function in tvm instead of old function in tvm</h2> <p>New code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="k">def</span> <span class="nf">test_matrix_elementwise_mul</span><span class="p">():</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">arr_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_y</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_z</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">elemwise_mul</span> <span class="o">=</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_mul</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_add</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">elemwise_mul</span><span class="p">(</span><span class="n">arr_x</span><span class="p">,</span> <span class="n">arr_y</span><span class="p">,</span> <span class="n">arr_z</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">arr_z</span><span class="p">.</span><span class="nf">asnumpy</span><span class="p">()</span>
    <span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>


    
<span class="k">def</span> <span class="nf">make_elemwise_mul</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <p>Old code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">test_matrix_elementwise_mul_by_const</span><span class="p">():</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">const_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">arr_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_y</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">elemwise_mul_by_const</span> <span class="o">=</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_mul_by_const</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">const_val</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_mul_by_const</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">elemwise_mul_by_const</span><span class="p">(</span><span class="n">arr_x</span><span class="p">,</span> <span class="n">arr_y</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">arr_y</span><span class="p">.</span><span class="nf">asnumpy</span><span class="p">()</span>
    <span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">const_val</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_elemwise_mul_by_const</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">const_k</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">const_k</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>
</code></pre></div></div> <h3 id="how-node-value-is-stored-in-autodiff-">How node value is stored in autodiff ?</h3> <p>It’s stored in dict <code class="language-plaintext highlighter-rouge">node_to_val_map</code></p> <p>The graph is just computation graph. Each node is an operation node.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">convert_to_numpy_ret_vals</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Parameters
        ----------
        feed_dict: a dictionary of node-&gt;np.ndarray supplied by user.
        convert_to_numpy_ret_vals: whether to convert ret vals to np.array.

        Returns
        -------
        A list of values for nodes in eval_node_list. tvm.nd.array or np.ndarray.
        </span><span class="sh">"""</span>
        <span class="k">def</span> <span class="nf">are_feed_shapes_equal</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="n">sb</span><span class="p">):</span>
            <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sb</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
                <span class="k">return</span> <span class="bp">False</span>
            <span class="n">unmatched_item</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">sa</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span> <span class="o">^</span> <span class="nf">set</span><span class="p">(</span><span class="n">sb</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
            <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">unmatched_item</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="n">node_to_val_map</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">feed_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">ndarray</span><span class="p">.</span><span class="n">NDArray</span><span class="p">),</span>\
                <span class="sh">"</span><span class="s">feed_dict value type not supported</span><span class="sh">"</span>    
            <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

</code></pre></div></div> <h3 id="how-node-computation-is-done-in-graph-">How node computation is done in graph ?</h3> <p>In run function , <code class="language-plaintext highlighter-rouge">compute</code> is called for each operation node.</p> <p>Each opeartion node in computation graph has its own compute function.</p> <p>For example , <code class="language-plaintext highlighter-rouge">AddOp</code> has its own compute function and this compute function calls passed-in compiled_func to do the function call from compiled code.</p> <p>Note that this <code class="language-plaintext highlighter-rouge">compiled_func</code> is built before forward of computation graph.</p> <p>And the return function of <code class="language-plaintext highlighter-rouge">make_elemwise_add</code> is a tvm build function that takes <code class="language-plaintext highlighter-rouge">[A,B,C]</code> three tensor as input instead of parameters in <code class="language-plaintext highlighter-rouge">compute</code> function.</p> <p><code class="language-plaintext highlighter-rouge">tgt</code> and <code class="language-plaintext highlighter-rouge">shape</code> is defined during function compilation in <code class="language-plaintext highlighter-rouge">make_elemwise_add</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">convert_to_numpy_ret_vals</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Parameters
        ----------
        feed_dict: a dictionary of node-&gt;np.ndarray supplied by user.
        convert_to_numpy_ret_vals: whether to convert ret vals to np.array.

        Returns
        -------
        A list of values for nodes in eval_node_list. tvm.nd.array or np.ndarray.
        </span><span class="sh">"""</span>
        <span class="k">def</span> <span class="nf">are_feed_shapes_equal</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="n">sb</span><span class="p">):</span>
            <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sb</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
                <span class="k">return</span> <span class="bp">False</span>
            <span class="n">unmatched_item</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">sa</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span> <span class="o">^</span> <span class="nf">set</span><span class="p">(</span><span class="n">sb</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
            <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">unmatched_item</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="n">node_to_val_map</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">feed_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">ndarray</span><span class="p">.</span><span class="n">NDArray</span><span class="p">),</span>\
                <span class="sh">"</span><span class="s">feed_dict value type not supported</span><span class="sh">"</span>    
            <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># collect shapes for all placeholders
</span>        <span class="n">feed_shapes</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_to_val_map</span><span class="p">:</span>
            <span class="n">feed_shapes</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">].</span><span class="n">shape</span>

        <span class="c1"># infer shape if feed_shapes changed since last run
</span>        <span class="c1"># e.g. call run() on test data after trainng
</span>        <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="nf">are_feed_shapes_equal</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">feed_shapes</span><span class="p">)):</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">infer_shape</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">feed_shapes</span> <span class="o">=</span> <span class="n">feed_shapes</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">memory_plan</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">compile_funcs</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">)</span>

        <span class="c1"># Traverse graph in topo order and compute values for all nodes.
</span>        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">topo_order</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_to_val_map</span><span class="p">:</span>
                <span class="c1"># Skip placeholder nodes. Values already provided by feed_dict.
</span>                <span class="k">continue</span>
            <span class="n">input_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_to_val_map</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">node_val</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">node_to_arr_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
            <span class="c1"># node_val is modified in-place
</span>            <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="n">node</span><span class="p">,</span> <span class="n">input_vals</span><span class="p">,</span> <span class="n">node_val</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">node_to_compiled_func</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
            <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_val</span>
        <span class="c1"># Collect node values.
</span>        <span class="k">if</span> <span class="n">convert_to_numpy_ret_vals</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">node_to_val_map</span><span class="p">[</span><span class="n">n</span><span class="p">].</span><span class="nf">asnumpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">eval_node_list</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">node_to_val_map</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">eval_node_list</span><span class="p">]</span>


</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AddOp</span><span class="p">(</span><span class="n">Op</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node_A</span><span class="p">,</span> <span class="n">node_B</span><span class="p">):</span>
        <span class="n">new_node</span> <span class="o">=</span> <span class="n">Op</span><span class="p">.</span><span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
        <span class="n">new_node</span><span class="p">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_A</span><span class="p">,</span> <span class="n">node_B</span><span class="p">]</span>
        <span class="n">new_node</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">(%s+%s)</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">node_A</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">node_B</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_node</span>

    <span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">input_vals</span><span class="p">,</span> <span class="n">output_val</span><span class="p">,</span> <span class="n">compiled_func</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">input_vals</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">assert</span> <span class="n">input_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span> <span class="o">==</span> <span class="n">input_vals</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">shape</span>
        <span class="nf">compiled_func</span><span class="p">(</span><span class="n">input_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_vals</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_val</span><span class="p">)</span>  

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">output_grad</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">input_shapes</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Need to handle input_vals[0].shape != input_vals[1].shape</span><span class="sh">"""</span>
        <span class="k">return</span> <span class="nf">broadcast_rule</span><span class="p">(</span><span class="n">input_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">compiled_func</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">input_shapes</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_add</span><span class="p">(</span>
            <span class="n">input_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_add</span><span class="sh">"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_elemwise_add</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">B</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="nc">B</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <h3 id="matrix_mul-impl-debug">matrix_mul impl debug</h3> <p>Get this erro while debgging matrix_mul</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>======================================================================
ERROR: test_tvm_op.test_matrix_multiply
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/zt/miniconda3/envs/uwsyml/lib/python3.8/site-packages/nose/case.py", line 197, in runTest
    self.test(*self.arg)
  File "/mnt/nvme1n1/zt/assignment2-2018/tests/test_tvm_op.py", line 94, in test_matrix_multiply
    matrix_mul(arr_x, arr_y, arr_z)
  File "/mnt/nvme1n1/zt/tvm/python/tvm/runtime/module.py", line 201, in __call__
    return self.entry_func(*args)
  File "/mnt/nvme1n1/zt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 245, in __call__
    raise_last_ffi_error()
  File "/mnt/nvme1n1/zt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm._ffi.base.TVMError: Traceback (most recent call last): 0: operator()                                                                                                                                                                                           
at /mnt/nvme1n1/zt/tvm/src/runtime/library_module.cc:82                                                                                                                                   
TVMError: Assert fail: T.Cast("int32", matrix_mul_B_shape[0]) == 500, Argument matrix_mul.B.shape[0] has an unsatisfied constraint: 500 == T.Cast("int32", matrix_mul_B_shape[0])   
</code></pre></div></div> <p>Wrong code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="o">//</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">wrong</span><span class="p">,</span> <span class="n">we</span> <span class="n">should</span> <span class="n">put</span> <span class="n">this</span> <span class="n">code</span> <span class="n">before</span> <span class="k">if</span> <span class="n">transposeA</span>  
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">come here</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a shape </span><span class="sh">'</span><span class="p">,</span> <span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="sh">'</span><span class="s">b shape</span><span class="sh">'</span><span class="p">,</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>



</code></pre></div></div> <p>Correct code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="c1"># print('a shape ', A.shape, 'b shape', B.shape)
</span>        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>

</code></pre></div></div> <p>I asked gpt to find the bug in this code it does not find the bug.</p> <h3 id="softmax-cross-entropy-impl">softmax cross entropy impl</h3> <p>Wrong code</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def make_matrix_softmax_cross_entropy(shape, tgt, tgt_host, func_name,
                                      dtype="float32"):
    """TODO: Your code here"""
    """Hint: output shape should be (1,)"""

    A = te.placeholder(shape, dtype=dtype, name='A')
    A_ = te.placeholder(shape, dtype=dtype, name='A_')

    B = te.compute(
           shape,
            lambda i, j: A_[i,j ] * te.log(A[i, j]),
            name='B'
            )

    row, col = shape
    axis_j = te.reduce_axis((0, col))
    axis_k = te.reduce_axis((0,row))
    C = te.compute(
            (1,),
            lambda :  -te.sum(B[ axis_j, axis_k], axis=[axis_j, axis_k]),
            name='C'
            )

    D = te.compute(
            (1,),
            lambda:  C / (row*col),
            name = 'D'
            )

    s = te.create_schedule(D.op)
    f = tvm.build(s, [A, A_, D], tgt, target_host=tgt_host, name=func_name)
    return f
</code></pre></div></div> <p>Got this error with fowllowing code</p> <p>error:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "/home/zt/miniconda3/envs/uwsyml/lib/python3.8/site-packages/nose/case.py", line 197, in runTest
    self.test(*self.arg)
  File "/mnt/nvme1n1/zt/assignment2-2018/tests/test_tvm_op.py", line 238, in test_softmax_cross_entropy
    matrix_softmax_cross_entropy = tvm_op.make_matrix_softmax_cross_entropy(shape, tgt, tgt_host, "softmax_cross_entropy")
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/tvm_op.py", line 217, in make_matrix_softmax_cross_entropy
    B = te.compute(

    ICHECK(0 == level_) &lt;&lt; "Reductions are only allowed at the top level of compute. "
tvm.error.InternalError: Traceback (most recent call last):
  7: operator()
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:168
  6: tvm::te::ComputeOp::ComputeOp(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tvm::runtime::Map&lt;tvm::runtime::String, tvm::runtime::ObjectRef, void, void&gt;, tvm::runtime::Array&lt;tvm::tir::IterVar, void&gt;, tvm::runtime::Array&lt;tvm::PrimExpr, void&gt;)
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:161
  5: VerifyComputeOp
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:581
  4: Run
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:551
  3: tvm::tir::ExprVisitor::VisitExpr_(tvm::tir::AddNode const*)
        at /mnt/nvme1n1/zt/tvm/src/tir/ir/expr_functor.cc:60
  2: VisitExpr
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:560
  1: VisitExpr_
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:564
  0: VisitExpr_
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:566
  File "/mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc", line 566
InternalError: Check failed: (0 == level_) is false: Reductions are only allowed at the top level of compute. Please create another tensor for further composition.
</code></pre></div></div> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_softmax_cross_entropy</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: output shape should be (1,)</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">A_</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A_</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">))</span>
    <span class="n">axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">row</span><span class="p">))</span>

    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
           <span class="p">(</span><span class="n">row</span><span class="p">,),</span>
           <span class="k">lambda</span> <span class="n">i</span> <span class="p">:</span> <span class="o">-</span><span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span> <span class="p">]</span> <span class="o">*</span> <span class="n">te</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_j</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span>
            <span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">axis_k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_k</span><span class="p">)</span><span class="o">/</span> <span class="n">row</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
        <span class="p">)</span>

</code></pre></div></div> <p>I think the error is saying that we should not use reduction and te.log() together?</p> <p>https://discuss.tvm.apache.org/t/non-top-level-reductions-in-compute-statements/5693</p> <p><a href="https://github.com/wyc-ruiker/CSE-599W-2018/blob/master/assignment2/python/dlsys/tvm_op.py">softmax cross entropy reference impl</a></p> <p>Try another impl code</p> <p>Fix code above after calculating te.log first and then do te.sum</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_softmax_cross_entropy</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: output shape should be (1,)</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">A_</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A_</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">j</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">row</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">log</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="n">sum_cross_entropy</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">row</span><span class="p">,),</span>
            <span class="c1"># lambda i: te.sum(B[i, axis_j], axis=axis_j ),
</span>            <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="o">-</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_j</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">sum_cross_entropy</span><span class="sh">'</span>
            <span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
        <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">sum_cross_entropy</span><span class="p">[</span><span class="n">axis_k</span><span class="p">]</span><span class="o">/</span><span class="n">row</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_k</span> <span class="p">)</span> <span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
        <span class="p">)</span>
</code></pre></div></div> <p>Correct code :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_softmax_cross_entropy</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: output shape should be (1,)</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">A_</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A_</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">softmax_axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax_j</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">softmax_axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax_k</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">max_x</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">),</span> 
           <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">softmax_axis_j</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="n">softmax_axis_j</span><span class="p">),</span> 
           <span class="n">name</span><span class="o">=</span> <span class="sh">'</span><span class="s">max_x</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">e_x</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="p">]</span> <span class="o">-</span> <span class="n">max_x</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">e_x</span><span class="sh">"</span>
            <span class="p">)</span>
    <span class="n">ex_sum</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">),</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">e_x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">softmax_axis_k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">softmax_axis_k</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">ex_sm</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">softmax</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">e_x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">ex_sum</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax_x</span><span class="sh">'</span><span class="p">)</span>
 

    <span class="n">axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">j</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">row</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">log</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">softmax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="n">sum_cross_entropy</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">row</span><span class="p">,),</span>
            <span class="c1"># lambda i: te.sum(B[i, axis_j], axis=axis_j ),
</span>            <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="o">-</span><span class="n">A_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_j</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">sum_cross_entropy</span><span class="sh">'</span>
            <span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
        <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">sum_cross_entropy</span><span class="p">[</span><span class="n">axis_k</span><span class="p">]</span><span class="o">/</span><span class="n">row</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_k</span> <span class="p">)</span> <span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
        <span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">A_</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <h3 id="fix-infer_shape-error">Fix infer_shape error</h3> <p><code class="language-plaintext highlighter-rouge">infer_shape()</code> is called for nodes in feed_dict. I don’t know why this happens.</p> <p>I am fixing it.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>type of node &lt;class 'dlsys.autodiff.Node'&gt; node name X
Traceback (most recent call last):
  File "tests/mnist_dlsys.py", line 373, in &lt;module&gt;
    m(executor_ctx, num_epochs, print_loss_val_each_epoch)
  File "tests/mnist_dlsys.py", line 132, in mnist_logreg
    loss_val, grad_W1_val, grad_b1_val, _ = executor.run(
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/autodiff.py", line 706, in run
    self.infer_shape(feed_shapes)
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/autodiff.py", line 616, in infer_shape
    infer_shape = node.op.infer_shape(node, input_shapes)
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/autodiff.py", line 320, in infer_shape
    assert False, "placeholder %s shape provided by feed_shape" % node.name
AssertionError: placeholder X shape provided by feed_shape
</code></pre></div></div> <p>I fixed this issue by swapping <code class="language-plaintext highlighter-rouge">shape[0]</code> and <code class="language-plaintext highlighter-rouge">shape[1]</code> directly instead of calling <code class="language-plaintext highlighter-rouge">np.transpose</code></p> <h3 id="speed-up-matrix-multilication">Speed up matrix multilication</h3> <p>Original matrix mul code and execution time Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="c1"># print('a shape ', A.shape, 'b shape', B.shape)
</span>        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(uwsyml) ➜  assignment2-2018 git:(master) ✗ python tests/mnist_dlsys.py -l -m mlp
=== Build 3-layer MLP model...
Loading data...
Start training loop...
/mnt/nvme1n1/zt/tvm/python/tvm/driver/build_module.py:280: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.
  warnings.warn(
epoch 0
shape a 1 784 shapeB 0 784
shape a 1 256 shapeB 0 256
shape a 1 100 shapeB 0 100
shape a 1 10 shapeB 0 10
shape a 1 100 shapeB 0 100
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
loss = 0.565684; Time taken this epoch = 39.259721 s
epoch 1
loss = 0.302340; Time taken this epoch = 37.834584 s
epoch 2
loss = 0.227699; Time taken this epoch = 37.836843 s
epoch 3
loss = 0.199743; Time taken this epoch = 37.733063 s
epoch 4
loss = 0.174254; Time taken this epoch = 37.731381 s
epoch 5
loss = 0.189644; Time taken this epoch = 37.791435 s
epoch 6
loss = 0.125607; Time taken this epoch = 37.795841 s
epoch 7
loss = 0.104398; Time taken this epoch = 37.821751 s
epoch 8
loss = 0.088052; Time taken this epoch = 37.845443 s
epoch 9
loss = 0.073229; Time taken this epoch = 37.798183 s
Validation set accuracy = 0.971600
Average Time per Training Epoch = 37.944825 s
</code></pre></div></div> <p>Optmized code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="c1"># print('a shape ', A.shape, 'b shape', B.shape)
</span>        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">xo</span><span class="p">,</span> <span class="n">xi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">yo</span><span class="p">,</span> <span class="n">yi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">xo</span><span class="p">,</span> <span class="n">yo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">yi</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">xo</span><span class="p">)</span>


    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(uwsyml) ➜  assignment2-2018 git:(master) ✗ python tests/mnist_dlsys.py -l -m mlp
=== Build 3-layer MLP model...
Loading data...
Start training loop...
/mnt/nvme1n1/zt/tvm/python/tvm/driver/build_module.py:280: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.
  warnings.warn(
epoch 0
shape a 1 784 shapeB 0 784
shape a 1 256 shapeB 0 256
shape a 1 100 shapeB 0 100
shape a 1 10 shapeB 0 10
shape a 1 100 shapeB 0 100
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
loss = 0.565684; Time taken this epoch = 2.756940 s
epoch 1
loss = 0.302340; Time taken this epoch = 0.805593 s
epoch 2
loss = 0.227699; Time taken this epoch = 0.840440 s
epoch 3
loss = 0.199743; Time taken this epoch = 0.827800 s
epoch 4
loss = 0.174254; Time taken this epoch = 0.829301 s
epoch 5
loss = 0.189644; Time taken this epoch = 0.835602 s
epoch 6
loss = 0.125607; Time taken this epoch = 0.836001 s
epoch 7
loss = 0.104398; Time taken this epoch = 0.831005 s
epoch 8
loss = 0.088052; Time taken this epoch = 0.829629 s
epoch 9
loss = 0.073229; Time taken this epoch = 0.830049 s
Validation set accuracy = 0.971600
Average Time per Training Epoch = 1.022236 s
</code></pre></div></div> <p>Explaination of <code class="language-plaintext highlighter-rouge">k</code> Certainly! In the line <code class="language-plaintext highlighter-rouge">k = s[C].op.reduce_axis[0]</code>, we are accessing the first reduction axis of the operation that computes <code class="language-plaintext highlighter-rouge">C</code>.</p> <p>Here’s a breakdown:</p> <ul> <li><code class="language-plaintext highlighter-rouge">s[C]</code>: This refers to the schedule for the computation of <code class="language-plaintext highlighter-rouge">C</code>.</li> <li><code class="language-plaintext highlighter-rouge">s[C].op</code>: This accesses the operation that computes <code class="language-plaintext highlighter-rouge">C</code>.</li> <li><code class="language-plaintext highlighter-rouge">s[C].op.reduce_axis</code>: This is a list of reduction axes used in the computation of <code class="language-plaintext highlighter-rouge">C</code>. Reduction axes are the dimensions over which we perform a reduction operation, such as summing elements.</li> <li><code class="language-plaintext highlighter-rouge">[0]</code>: This accesses the first reduction axis in the list.</li> </ul> <p>In the context of matrix multiplication, the reduction axis <code class="language-plaintext highlighter-rouge">k</code> represents the dimension over which we sum the products of elements from matrices <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>. For example, if <code class="language-plaintext highlighter-rouge">A</code> is of shape <code class="language-plaintext highlighter-rouge">(m, n)</code> and <code class="language-plaintext highlighter-rouge">B</code> is of shape <code class="language-plaintext highlighter-rouge">(n, p)</code>, the reduction axis <code class="language-plaintext highlighter-rouge">k</code> would range from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">n-1</code>, summing the products of corresponding elements from <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> to compute each element of the resulting matrix <code class="language-plaintext highlighter-rouge">C</code>.</p> <p>Let me know if you need any further clarification!</p> <h4 id="problem-setup">Problem Setup</h4> <p>Suppose <code class="language-plaintext highlighter-rouge">C</code> computes the result of multiplying two matrices ( A ) (size ( 64 \times 64 )) and ( B ) (size ( 64 \times 64 )):</p> <p>[ C[i, j] = \sum_{k=0}^{63} A[i, k] \cdot B[k, j] ]</p> <p>Here:</p> <ul> <li><code class="language-plaintext highlighter-rouge">x</code> corresponds to <code class="language-plaintext highlighter-rouge">i</code> (rows of ( C )),</li> <li><code class="language-plaintext highlighter-rouge">y</code> corresponds to <code class="language-plaintext highlighter-rouge">j</code> (columns of ( C )),</li> <li><code class="language-plaintext highlighter-rouge">k</code> is the reduction axis (over ( k )).</li> </ul> <h4 id="steps-in-context">Steps in Context</h4> <ol> <li><strong>Splitting Axes</strong> <ul> <li><code class="language-plaintext highlighter-rouge">x</code> (rows) and <code class="language-plaintext highlighter-rouge">y</code> (columns) are split into blocks of size <code class="language-plaintext highlighter-rouge">32</code>. This creates a <strong>tile-based computation</strong>: <ul> <li><code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> iterate over ( 2 \times 2 ) blocks (as ( 64 / 32 = 2 )).</li> <li><code class="language-plaintext highlighter-rouge">xi</code> and <code class="language-plaintext highlighter-rouge">yi</code> handle elements within each ( 32 \times 32 ) block.</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">k</code> is split into chunks of size <code class="language-plaintext highlighter-rouge">4</code>: <ul> <li><code class="language-plaintext highlighter-rouge">ko</code> iterates over 16 chunks (as ( 64 / 4 = 16 )).</li> <li><code class="language-plaintext highlighter-rouge">ki</code> handles individual reduction operations within each chunk.</li> </ul> </li> </ul> </li> <li><strong>Reordering</strong><br/> The computation is reordered to maximize: <ul> <li><strong>Data locality</strong>: Processing elements in nearby memory locations together.</li> <li><strong>Parallelism</strong>: Outer loops (<code class="language-plaintext highlighter-rouge">xo</code>, <code class="language-plaintext highlighter-rouge">yo</code>) can often run in parallel.</li> </ul> </li> </ol> <h4 id="execution-order">Execution Order</h4> <p>The reordered iteration could look like this (pseudocode):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">xo</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>         <span class="c1"># Iterate over 32-row blocks
</span>    <span class="k">for</span> <span class="n">yo</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>     <span class="c1"># Iterate over 32-column blocks
</span>        <span class="k">for</span> <span class="n">ko</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span> <span class="c1"># Iterate over reduction chunks (k-axis)
</span>            <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">32</span><span class="p">):</span>  <span class="c1"># Process rows within a block
</span>                <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">32</span><span class="p">):</span>  <span class="c1"># Process columns within a block
</span>                    <span class="k">for</span> <span class="n">ki</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>  <span class="c1"># Perform reduction within the chunk
</span>                        <span class="n">C</span><span class="p">[</span><span class="n">xo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">yi</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">xo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">xi</span><span class="p">,</span> <span class="n">ko</span><span class="o">*</span><span class="mi">4</span> <span class="o">+</span> <span class="n">ki</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">ko</span><span class="o">*</span><span class="mi">4</span> <span class="o">+</span> <span class="n">ki</span><span class="p">,</span> <span class="n">yo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">yi</span><span class="p">]</span>
</code></pre></div></div> <hr/> <h3 id="optimization-insight">Optimization Insight</h3> <p>This approach of splitting and reordering:</p> <ul> <li><strong>Improves memory access patterns</strong>: Data is processed in small blocks, reducing cache misses.</li> <li><strong>Enables parallel execution</strong>: Larger outer loops (<code class="language-plaintext highlighter-rouge">xo</code>, <code class="language-plaintext highlighter-rouge">yo</code>, <code class="language-plaintext highlighter-rouge">ko</code>) can be distributed across threads or cores.</li> <li><strong>Reduces computation overhead</strong>: By carefully controlling inner loops, computation can be streamlined for specific hardware.</li> </ul> <p>This code snippet is an example of how to define a computation schedule in <strong>TVM</strong>, a machine learning compiler framework used for optimizing tensor computations. Let’s break it down step by step:</p> <hr/> <h3 id="1-s--tecreate_schedulecop">1. <strong><code class="language-plaintext highlighter-rouge">s = te.create_schedule(C.op)</code></strong></h3> <ul> <li>This creates a schedule for the operation (<code class="language-plaintext highlighter-rouge">C.op</code>) that needs to be optimized.</li> <li>A schedule defines how the computation will be organized in terms of loops, parallelism, vectorization, etc.</li> </ul> <hr/> <h3 id="2-x-y--scopaxis">2. <strong><code class="language-plaintext highlighter-rouge">x, y = s[C].op.axis</code></strong></h3> <ul> <li><code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> are the primary loop axes of the computation. These are typically the dimensions of the tensor being computed.</li> <li>For instance, if <code class="language-plaintext highlighter-rouge">C</code> is a 2D tensor, <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> might represent its row and column dimensions.</li> </ul> <hr/> <h3 id="3-k--scopreduce_axis0">3. <strong><code class="language-plaintext highlighter-rouge">k = s[C].op.reduce_axis[0]</code></strong></h3> <ul> <li><code class="language-plaintext highlighter-rouge">k</code> is a reduction axis, commonly used in operations like matrix multiplication or summation over an axis.</li> <li>If <code class="language-plaintext highlighter-rouge">C</code> is the result of a matrix multiplication ( A \times B ), then <code class="language-plaintext highlighter-rouge">k</code> would be the summation axis.</li> </ul> <hr/> <h3 id="4-xo-xi--scsplitx-factor32">4. <strong><code class="language-plaintext highlighter-rouge">xo, xi = s[C].split(x, factor=32)</code></strong></h3> <ul> <li>Splits the <code class="language-plaintext highlighter-rouge">x</code> axis into two parts: <ul> <li><code class="language-plaintext highlighter-rouge">xo</code>: Outer loop, handling chunks of 32 iterations.</li> <li><code class="language-plaintext highlighter-rouge">xi</code>: Inner loop, handling the remaining iterations within each chunk.</li> </ul> </li> <li>This is a technique for tiling the computation, which improves cache efficiency.</li> </ul> <hr/> <h3 id="5-yo-yi--scsplity-factor32">5. <strong><code class="language-plaintext highlighter-rouge">yo, yi = s[C].split(y, factor=32)</code></strong></h3> <ul> <li>Similar to <code class="language-plaintext highlighter-rouge">x</code>, this splits the <code class="language-plaintext highlighter-rouge">y</code> axis into outer (<code class="language-plaintext highlighter-rouge">yo</code>) and inner (<code class="language-plaintext highlighter-rouge">yi</code>) loops, with a tile size of 32.</li> </ul> <hr/> <h3 id="6-ko-ki--scsplitk-factor4">6. <strong><code class="language-plaintext highlighter-rouge">ko, ki = s[C].split(k, factor=4)</code></strong></h3> <ul> <li>Splits the reduction axis <code class="language-plaintext highlighter-rouge">k</code> into: <ul> <li><code class="language-plaintext highlighter-rouge">ko</code>: Outer loop for reduction.</li> <li><code class="language-plaintext highlighter-rouge">ki</code>: Inner loop for reduction, with a tile size of 4.</li> </ul> </li> </ul> <hr/> <h3 id="7-screorderxo-yo-ko-xi-yi-ki">7. <strong><code class="language-plaintext highlighter-rouge">s[C].reorder(xo, yo, ko, xi, yi, ki)</code></strong></h3> <ul> <li>Rearranges the loop order to optimize computation.</li> <li>The new order ensures that the outer loops (<code class="language-plaintext highlighter-rouge">xo</code>, <code class="language-plaintext highlighter-rouge">yo</code>, <code class="language-plaintext highlighter-rouge">ko</code>) iterate first, followed by the inner loops (<code class="language-plaintext highlighter-rouge">xi</code>, <code class="language-plaintext highlighter-rouge">yi</code>, <code class="language-plaintext highlighter-rouge">ki</code>).</li> <li>This can improve locality of reference and allow efficient use of the hardware.</li> </ul> <hr/> <h3 id="8-scvectorizeyi">8. <strong><code class="language-plaintext highlighter-rouge">s[C].vectorize(yi)</code></strong></h3> <ul> <li>Applies vectorization to the <code class="language-plaintext highlighter-rouge">yi</code> loop.</li> <li>This instructs the compiler to use SIMD (Single Instruction, Multiple Data) instructions to compute multiple iterations of the <code class="language-plaintext highlighter-rouge">yi</code> loop simultaneously, boosting performance.</li> </ul> <hr/> <h3 id="9-scparallelxo">9. <strong><code class="language-plaintext highlighter-rouge">s[C].parallel(xo)</code></strong></h3> <ul> <li>Parallelizes the <code class="language-plaintext highlighter-rouge">xo</code> loop, enabling multiple threads to compute different chunks of the <code class="language-plaintext highlighter-rouge">x</code> axis concurrently.</li> <li>This is particularly useful on multi-core processors.</li> </ul> <hr/> <h3 id="example"><strong>Example</strong></h3> <p>Suppose <code class="language-plaintext highlighter-rouge">C</code> is the result of a matrix multiplication ( A[m, k] \times B[k, n] ), and we are optimizing the computation.</p> <ul> <li><strong>Matrix Dimensions</strong>: ( A ) is ( 64 \times 128 ), ( B ) is ( 128 \times 64 ).</li> <li>The schedule: <ul> <li>Splits the computation into tiles of ( 32 \times 32 ) for <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> dimensions (row and column).</li> <li>Reduces over <code class="language-plaintext highlighter-rouge">k</code> in chunks of 4.</li> <li>Reorders loops to optimize memory access patterns.</li> <li>Vectorizes operations within a 32-element column tile (<code class="language-plaintext highlighter-rouge">yi</code>).</li> <li>Parallelizes over outermost rows (<code class="language-plaintext highlighter-rouge">xo</code>), enabling multi-threading.</li> </ul> </li> </ul> <p>This structured approach ensures better utilization of the CPU/GPU by leveraging parallelism, vectorization, and efficient memory access.</p> <h2 id="we-can-parallelize-xo-and-yo-at-the-same-time">We can parallelize xo and yo at the same time</h2> <p>Parallelizing <code class="language-plaintext highlighter-rouge">xo</code> distributes the workload of the outer <code class="language-plaintext highlighter-rouge">x</code> axis (rows of tiles) across multiple threads, allowing each thread to compute one or more row tiles simultaneously. This is often chosen because:</p> <ol> <li> <p><strong>Independent Workload</strong>: Each tile along <code class="language-plaintext highlighter-rouge">xo</code> is independent, meaning computations for different tiles don’t depend on each other. This independence is a prerequisite for parallelization.</p> </li> <li> <p><strong>Balanced Workload</strong>: If the number of row tiles (determined by the size of <code class="language-plaintext highlighter-rouge">xo</code>) matches the number of available threads or cores, it results in a balanced workload.</p> </li> <li> <p><strong>Memory Access</strong>: For row-major memory layouts, accessing data row-wise tends to be more cache-friendly. Parallelizing <code class="language-plaintext highlighter-rouge">xo</code> aligns well with this layout.</p> </li> </ol> <hr/> <h3 id="can-we-parallelize-yo-at-the-same-time">Can We Parallelize <code class="language-plaintext highlighter-rouge">yo</code> at the Same Time?</h3> <p>Yes, it’s possible to parallelize <code class="language-plaintext highlighter-rouge">yo</code> as well, but there are important considerations:</p> <ol> <li><strong>Nested Parallelism</strong>: <ul> <li>If <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> are both parallelized, this results in nested parallelism (threads inside threads).</li> <li>This is often inefficient because hardware threads can’t dynamically spawn or manage sub-threads efficiently, and most hardware supports a flat parallel structure.</li> </ul> </li> <li><strong>Overhead</strong>: <ul> <li>Parallelizing both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> can introduce thread management overhead.</li> <li>The system must divide and coordinate work across all threads, which can diminish performance if not carefully managed.</li> </ul> </li> <li><strong>Thread Count</strong>: <ul> <li>Hardware has a limited number of threads or cores. If you parallelize both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code>, you risk oversubscribing threads, causing context-switching overhead.</li> </ul> </li> </ol> <hr/> <h3 id="when-parallelizing-both-could-work"><strong>When Parallelizing Both Could Work</strong></h3> <p>Parallelizing both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> is feasible under specific conditions:</p> <ol> <li><strong>Low Tile Counts in <code class="language-plaintext highlighter-rouge">xo</code> or <code class="language-plaintext highlighter-rouge">yo</code></strong>: <ul> <li>If <code class="language-plaintext highlighter-rouge">xo</code> alone doesn’t provide enough parallelism (e.g., when the matrix is small), you might also parallelize <code class="language-plaintext highlighter-rouge">yo</code>.</li> </ul> </li> <li><strong>Hierarchical Parallelism</strong>: <ul> <li>You can map <code class="language-plaintext highlighter-rouge">xo</code> to thread blocks and <code class="language-plaintext highlighter-rouge">yo</code> to threads within each block on GPUs or similar architectures.</li> </ul> </li> <li><strong>Custom Scheduling</strong>: <ul> <li>For some specialized cases, you can explicitly balance thread allocation across <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> to avoid over-parallelization.</li> </ul> </li> </ol> <hr/> <h3 id="example-scenario">Example Scenario</h3> <p>If you’re working with a 2D matrix computation and your target machine has 16 cores:</p> <ul> <li><strong>Parallelize only <code class="language-plaintext highlighter-rouge">xo</code></strong>: Efficient if the number of row tiles ( \frac{\text{rows}}{\text{tile_size}} ) is ≥ 16.</li> <li><strong>Parallelize both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code></strong>: Useful when row and column tiles combined can provide enough parallelism (e.g., ( 4 \times 4 = 16 ) tiles).</li> </ul> <p>To summarize, while it’s theoretically possible to parallelize both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code>, in practice, parallelizing only one is simpler and avoids excessive thread management overhead on most CPUs. For GPUs, however, hierarchical or hybrid parallelization across <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> is more common.</p>]]></content><author><name></name></author><category term="ml"/><category term="framework"/><category term="ml"/><category term="framework"/><summary type="html"><![CDATA[This is fun to implement]]></summary></entry></feed>