<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bilyz98.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bilyz98.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-23T02:17:53+00:00</updated><id>https://bilyz98.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Topo Sort</title><link href="https://bilyz98.github.io/blog/2024/topo-sort/" rel="alternate" type="text/html" title="Topo Sort"/><published>2024-12-07T00:00:00+00:00</published><updated>2024-12-07T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/topo-sort</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/topo-sort/"><![CDATA[<h2 id="topo-sort-starts-from-end-node">Topo sort starts from end node</h2> <p>This is used in autodiff graph build algorithm</p> <h2 id="topo-sort-given-graph">Topo sort given graph</h2> <p>This is used in leetcode problem.</p> <p>Given a graph we use <code class="language-plaintext highlighter-rouge">in_degree</code> to track in degree for each node.</p> <p>Each node whose in degree is 0 is put to output array which means that this node depends on no other nodes.</p> <p>This node is also put to queue to get adjacent nodes to this <code class="language-plaintext highlighter-rouge">in_degree=0</code> node.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;queue&gt;</span><span class="cp">
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">num_node</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">edge_count</span><span class="p">;</span>
    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">num_node</span> <span class="o">&gt;&gt;</span> <span class="n">edge_count</span><span class="p">;</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">graph</span><span class="p">(</span><span class="n">num_node</span><span class="p">);</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">in_degree</span><span class="p">(</span><span class="n">num_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">edge_count</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">in</span><span class="p">,</span> <span class="n">out</span><span class="p">;</span>
        <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">in</span> <span class="o">&gt;&gt;</span> <span class="n">out</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">node_idx</span> <span class="o">=</span> <span class="n">in</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">out_node_idx</span> <span class="o">=</span> <span class="n">out</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span>
        <span class="n">graph</span><span class="p">[</span><span class="n">node_idx</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="n">out_node_idx</span><span class="p">);</span>
        <span class="n">in_degree</span><span class="p">[</span><span class="n">out_node_idx</span><span class="p">]</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">queue</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">node_q</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_node</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span><span class="p">(</span><span class="n">in_degree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">node_q</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">topo_sort</span><span class="p">;</span>
    <span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">node_q</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span>  <span class="p">{</span>
        <span class="kt">int</span> <span class="n">front_node</span> <span class="o">=</span> <span class="n">node_q</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
        <span class="n">node_q</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
        <span class="n">topo_sort</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">front_node</span><span class="p">);</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">out_node</span><span class="o">:</span><span class="n">graph</span><span class="p">[</span><span class="n">front_node</span><span class="p">])</span> <span class="p">{</span>
            <span class="n">in_degree</span><span class="p">[</span><span class="n">out_node</span><span class="p">]</span><span class="o">--</span><span class="p">;</span>
            <span class="k">if</span><span class="p">(</span><span class="n">in_degree</span><span class="p">[</span><span class="n">out_node</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">node_q</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">out_node</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">if</span><span class="p">(</span><span class="n">topo_sort</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">num_node</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_node</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">topo_sort</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">topo_sort</span><span class="p">.</span><span class="n">back</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>



<span class="p">}</span>
<span class="c1">// 64 位输出请用 printf("%lld")</span>

</code></pre></div></div> <h2 id="difference-between-tensorflow-and-pytorch">Difference between tensorflow and pytorch?</h2>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Rdma</title><link href="https://bilyz98.github.io/blog/2024/rdma/" rel="alternate" type="text/html" title="Rdma"/><published>2024-12-06T00:00:00+00:00</published><updated>2024-12-06T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/rdma</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/rdma/"><![CDATA[<p>[Article talks about difference between one sided and two sided rdma]https://forums.developer.nvidia.com/t/comparasion-of-one-sided-vs-two-sided-operations/207183</p> <h2 id="what-is-single-sided-rdma">What is single-sided RDMA</h2> <p>Single-sided RDMA (Remote Direct Memory Access) refers to a method where one machine can directly access the memory of another machine without involving the remote CPU. This is achieved through RDMA’s one-sided verbs, which allow for operations like read and write to be performed directly on the remote memory.</p> <h3 id="key-features-of-single-sided-rdma">Key Features of Single-Sided RDMA:</h3> <ul> <li><strong>Low Latency</strong>: Provides ultra-low latency network communication.</li> <li><strong>CPU Bypass</strong>: Bypasses the remote CPU, reducing overhead and improving efficiency.</li> <li><strong>Direct Memory Access</strong>: Enables direct access to remote memory, which is particularly useful in distributed systems and databases.</li> </ul> <h3 id="applications">Applications:</h3> <ul> <li><strong>Distributed Databases</strong>: Used in disaggregated database management systems to improve performance and scalability.</li> <li><strong>Key-Value Stores</strong>: Utilized in high-performance key-value stores to achieve low latency and high throughput.</li> </ul> <p>Single-sided RDMA is a powerful tool for building efficient and scalable distributed systems by leveraging direct memory access and minimizing CPU involvement.</p> <h2 id="what-is-two-sided-rdma">What is two-sided RDMA</h2> <p>Yes, there is a concept known as two-sided RDMA operations. While single-sided RDMA operations involve one machine directly accessing the memory of another without involving the remote CPU, two-sided RDMA operations require both the sender and receiver to participate in the communication.</p> <h3 id="key-features-of-two-sided-rdma">Key Features of Two-Sided RDMA:</h3> <ul> <li><strong>Send/Receive Operations</strong>: In two-sided RDMA, the sender posts a send work request, and the receiver posts a receive work request. The data transfer occurs when the send request matches the receive request.</li> <li><strong>CPU Involvement</strong>: Both the sender’s and receiver’s CPUs are involved in the communication process, which can add some overhead compared to single-sided operations.</li> <li><strong>Use Cases</strong>: Two-sided RDMA is often used in scenarios where synchronization between the sender and receiver is necessary, such as in certain types of distributed transactions and message passing.</li> </ul> <h3 id="comparison-with-single-sided-rdma">Comparison with Single-Sided RDMA:</h3> <ul> <li><strong>Performance</strong>: Single-sided RDMA generally offers lower latency and higher throughput because it bypasses the remote CPU. However, two-sided RDMA can be more suitable for applications requiring tight synchronization and coordination between nodes.</li> <li><strong>Complexity</strong>: Two-sided RDMA operations can be more complex to implement due to the need for both sides to coordinate their actions.</li> </ul> <p>You can find more detailed information and comparisons between one-sided and two-sided RDMA operations <a href="https://forums.developer.nvidia.com/t/comparasion-of-one-sided-vs-two-sided-operations/207183">here</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[[Article talks about difference between one sided and two sided rdma]https://forums.developer.nvidia.com/t/comparasion-of-one-sided-vs-two-sided-operations/207183]]></summary></entry><entry><title type="html">Autodiff implementation - kernel and memory management</title><link href="https://bilyz98.github.io/blog/2024/uw-sysml-assign2/" rel="alternate" type="text/html" title="Autodiff implementation - kernel and memory management"/><published>2024-12-04T11:59:00+00:00</published><updated>2024-12-04T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/uw-sysml-assign2</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/uw-sysml-assign2/"><![CDATA[<p>This is fun to implement</p> <p>Never aware that we can do management in python language like this.</p> <p>I always think that memory management in only done in cpp.</p> <p>Interesting, so interesting.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def memory_plan(self, feed_shapes):
        """Allocates tvm.nd.array for every node except feed_dict nodes.

        Implementation note:
        Easy Option: Alloc a tvm.nd.array per node that persists across run()

        Use self.node_to_arr_map to store node-&gt;tvm.nd.array mapping
        to allow mapping to persist across multiple executor.run().

        Parameters
        ----------
        feed_shapes: node-&gt;shapes mapping for feed_dict nodes.
        """
        """TODO: Your code here"""


</code></pre></div></div> <h2 id="what-is-placeholder-in-tvm-">What is placeholder in tvm ?</h2> <p>In TVM, a <code class="language-plaintext highlighter-rouge">placeholder</code> is used to define an input tensor for a computation. It essentially acts as a symbolic representation of the input data, specifying the shape and optionally the data type. This is useful for defining the structure of the computation graph before actual data is provided.</p> <p>Here’s a simple example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="c1"># Define the shape of the input tensor
</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="sh">"</span><span class="s">float32</span><span class="sh">"</span>

<span class="c1"># Create a placeholder tensor
</span><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Print the placeholder tensor
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Placeholder tensor A:</span><span class="sh">"</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</code></pre></div></div> <p>In this example:</p> <ul> <li><code class="language-plaintext highlighter-rouge">te.placeholder</code> creates a tensor <code class="language-plaintext highlighter-rouge">A</code> with the specified shape <code class="language-plaintext highlighter-rouge">(3, 4)</code> and data type <code class="language-plaintext highlighter-rouge">"float32"</code>.</li> <li>This tensor can then be used in further computations, such as defining operations that use <code class="language-plaintext highlighter-rouge">A</code> as input.</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">placeholder</code> is a fundamental concept in TVM’s Tensor Expression (TE) language, allowing you to build complex computation graphs by defining how data flows through various operations¹².</p> <p>¹: <a href="https://tvm.apache.org/docs/reference/api/python/te.html">TVM Documentation</a> ²: <a href="https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html">TVM Tutorial on Tensor Expressions</a></p> <p>Source: Conversation with Copilot, 12/5/2024 (1) tvm.te — tvm 0.19.dev0 documentation - The Apache Software Foundation. https://tvm.apache.org/docs/reference/api/python/te.html. (2) TVM 自底向上（三）：TE 的概念和编译原理 - 知乎. https://zhuanlan.zhihu.com/p/534313816. (3) Working with Operators Using Tensor Expression — tvm 0.19.dev0 …. https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html. (4) TVM学习（六）细读前端 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/346514871. (5) External Tensor Functions — tvm 0.19.dev0 documentation. https://tvm.apache.org/docs/how_to/work_with_schedules/extern_op.html.</p> <h2 id="tvm-create-ndarray-with-specified-shape">tvm create nd.array with specified shape</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="c1"># Specify the shape and data type
</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="sh">"</span><span class="s">float32</span><span class="sh">"</span>

<span class="c1"># Create an empty TVM NDArray with the specified shape
</span><span class="n">tvm_array</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Print the TVM NDArray
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TVM NDArray with specified shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tvm_array</span><span class="p">)</span>

</code></pre></div></div> <h2 id="difference-between-tvm-and-ncnn">Difference between tvm and ncnn</h2> <p>tvm is a open source deep learning compiler for cpus, gpus and specialized hardware.</p> <p>ncnn is a neural network inference framework optimized for mobile and embedded devices.</p> <p>We can assume input shape of mat_mul is 2d in this task.</p> <h2 id="matmul-kernel-for-unknown-input-dimension-shape">matmul kernel for unknown input dimension shape</h2> <p>This code is from gpt.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="k">def</span> <span class="nf">make_flexible_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="c1"># Determine the shapes of the input tensors
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">shapeA</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">shapeA</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">shapeA</span>
    
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">shapeB</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">shapeB</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">shapeB</span>
    
    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch</span> <span class="nf">else </span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch</span> <span class="nf">else </span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Create placeholders for the input tensors
</span>    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    
    <span class="c1"># Define the reduction axis
</span>    <span class="n">k_axis</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># Compute the matrix multiplication based on transpose flags and dimensionality
</span>    <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeA</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeA</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
    
    <span class="c1"># Create a schedule for the computation
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    
    <span class="c1"># Apply optimizations: split, reorder, vectorize, parallel
</span>    <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">bo</span><span class="p">,</span> <span class="n">bi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">io</span><span class="p">,</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">jo</span><span class="p">,</span> <span class="n">ji</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">bo</span><span class="p">,</span> <span class="n">io</span><span class="p">,</span> <span class="n">jo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">bi</span><span class="p">,</span> <span class="n">ii</span><span class="p">,</span> <span class="n">ji</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">bo</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">io</span><span class="p">,</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">jo</span><span class="p">,</span> <span class="n">ji</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">io</span><span class="p">,</span> <span class="n">jo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">ii</span><span class="p">,</span> <span class="n">ji</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">io</span><span class="p">)</span>
    
    <span class="c1"># Lower the schedule to generate the IR code
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">tvm</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    
    <span class="c1"># Build the function
</span>    <span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">func</span>

<span class="c1"># Example usage
</span><span class="n">tgt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">tgt_host</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">func_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">flexible_matrix_mul</span><span class="sh">"</span>
<span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># Batch size of 32
</span><span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>  <span class="c1"># Batch size of 32
</span><span class="n">transposeA</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">transposeB</span> <span class="o">=</span> <span class="bp">False</span>
<span class="nf">make_flexible_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">)</span>

</code></pre></div></div> <h2 id="conv2d-tvm-kernel">conv2d tvm kernel</h2> <p>Code is generated by gpt</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span><span class="p">,</span> <span class="n">topi</span>

<span class="k">def</span> <span class="nf">make_conv2d</span><span class="p">(</span><span class="n">shapeX</span><span class="p">,</span> <span class="n">shapeF</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">assert</span><span class="p">(</span><span class="n">shapeX</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeF</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">shapeX</span>
    <span class="n">M</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">shapeF</span>

    <span class="c1"># Create placeholders for the input tensor and filter
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">F</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># Define the reduction axes
</span>    <span class="n">rc</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rc</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">rr</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">R</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rr</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">rs</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rs</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Compute the convolution
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">H</span> <span class="o">-</span> <span class="n">R</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">S</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
        <span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="n">rr</span><span class="p">,</span> <span class="n">w</span> <span class="o">+</span> <span class="n">rs</span><span class="p">]</span> <span class="o">*</span> <span class="n">F</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">]),</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span>
    <span class="p">)</span>

    <span class="c1"># Create a schedule for the computation
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">Y</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># Apply optimizations: split, reorder, vectorize, parallel
</span>    <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
    <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span>
    <span class="n">ho</span><span class="p">,</span> <span class="n">hi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">wo</span><span class="p">,</span> <span class="n">wi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">ho</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">hi</span><span class="p">,</span> <span class="n">wi</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">ho</span><span class="p">)</span>

    <span class="c1"># Lower the schedule to generate the IR code
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">tvm</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="c1"># Build the function
</span>    <span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">func</span>

<span class="c1"># Example usage
</span><span class="n">tgt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">tgt_host</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">func_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">conv2d</span><span class="sh">"</span>
<span class="n">shapeX</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>  <span class="c1"># Example input shape (N, C, H, W)
</span><span class="n">shapeF</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   <span class="c1"># Example filter shape (M, C, R, S)
</span><span class="nf">make_conv2d</span><span class="p">(</span><span class="n">shapeX</span><span class="p">,</span> <span class="n">shapeF</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">)</span>


</code></pre></div></div> <h2 id="install-tvm-by-building-from-source">Install tvm by building from source</h2> <p>Can not install tvm through pip. Have to download source code and build it myself. I don’t know why.</p> <p>Folow steps in this <a href="https://github.com/apache/tvm/issues/13507">issue</a> to compile locally.</p> <p>Need to disable gtest inroder to pass cmake</p> <p><a href="https://tvm.apache.org/docs/install/from_source.html#install-from-source">Offical install document</a></p> <p>Finally finish installing tvm after building locally.</p> <p>Run this command to verify</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>uwsyml<span class="o">)</span> ➜  tvm git:<span class="o">(</span>main<span class="o">)</span> python <span class="nt">-c</span> <span class="s2">"import tvm; print(tvm.__file__)"</span>
/mnt/nvme1n1/zt/tvm/python/tvm/__init__.py
</code></pre></div></div> <p>nosetests does not use python in conda. It uses that in /usr/bin which is not what I want.</p> <p>It reports error that it can not find numpy which I have already installed in conda environment</p> <h2 id="update-code-to-use-latest-function-in-tvm-instead-of-old-function-in-tvm">Update code to use latest function in tvm instead of old function in tvm</h2> <p>New code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="k">def</span> <span class="nf">test_matrix_elementwise_mul</span><span class="p">():</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">arr_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_y</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_z</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">elemwise_mul</span> <span class="o">=</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_mul</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_add</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">elemwise_mul</span><span class="p">(</span><span class="n">arr_x</span><span class="p">,</span> <span class="n">arr_y</span><span class="p">,</span> <span class="n">arr_z</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">arr_z</span><span class="p">.</span><span class="nf">asnumpy</span><span class="p">()</span>
    <span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>


    
<span class="k">def</span> <span class="nf">make_elemwise_mul</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <p>Old code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">test_matrix_elementwise_mul_by_const</span><span class="p">():</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">const_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">arr_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_y</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">elemwise_mul_by_const</span> <span class="o">=</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_mul_by_const</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">const_val</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_mul_by_const</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">elemwise_mul_by_const</span><span class="p">(</span><span class="n">arr_x</span><span class="p">,</span> <span class="n">arr_y</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">arr_y</span><span class="p">.</span><span class="nf">asnumpy</span><span class="p">()</span>
    <span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">const_val</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_elemwise_mul_by_const</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">const_k</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">const_k</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="ml"/><category term="framework"/><category term="ml"/><category term="framework"/><summary type="html"><![CDATA[This is fun to implement]]></summary></entry><entry><title type="html">Autodiff implementation</title><link href="https://bilyz98.github.io/blog/2024/uw-sysml-assign/" rel="alternate" type="text/html" title="Autodiff implementation"/><published>2024-11-29T11:59:00+00:00</published><updated>2024-11-29T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/uw-sysml-assign</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/uw-sysml-assign/"><![CDATA[<p><a href="https://github.com/dlsys-course/assignment1-2018">Source assignment</a></p> <p>Wrong code I wrote:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    for node in reverse_topo_order:
        cur_node_grad = node_to_output_grads_list[node]
        sum_grad = np.sum(cur_node_grad, axis=0)
        node_grad = node.op.gradient(node, sum_grad)
        for input in node.inputs:
            input_grads = node.op.gradient(input, sum_grad)
            if input not in node_to_output_grads_list:
                node_to_output_grads_list[input] = []
            node_to_output_grads_list[input].append(input_grads)


</code></pre></div></div> <p><strong>Understanding the Code and Identifying the Issue</strong></p> <p>The provided code implements a backpropagation algorithm to compute gradients of an output node with respect to a list of input nodes in a computational graph. The core idea is to traverse the graph in reverse topological order, calculating gradients at each node and propagating them backward.</p> <p>However, the current implementation has a potential issue:</p> <ol> <li><strong>Incorrect Gradient Calculation:</strong> <ul> <li>The line <code class="language-plaintext highlighter-rouge">node_to_output_grad[node] = sum_grad</code> directly assigns the sum of gradients to the node’s gradient. This is incorrect, as it doesn’t account for the specific gradient contribution of each input to the node’s output.</li> <li>The subsequent gradient calculation for input nodes using <code class="language-plaintext highlighter-rouge">node.op.gradient</code> might also be affected by this incorrect sum.</li> </ul> </li> </ol> <p><strong>Corrected Code</strong></p> <p>Here’s a corrected implementation that addresses the issue:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">output_node</span><span class="p">,</span> <span class="n">node_list</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Take gradient of output node with respect to each node in node_list.

    Parameters
    ----------
    output_node: output node that we are taking derivative of.
    node_list: list of nodes that we are taking derivative wrt.

    Returns
    -------
    A list of gradient values, one for each node in node_list respectively.
    </span><span class="sh">"""</span>

    <span class="n">node_to_output_grads_list</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">node_to_output_grad</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Initialize gradient of output node as oneslike_op(output_node)
</span>    <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">output_node</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nf">oneslike_op</span><span class="p">(</span><span class="n">output_node</span><span class="p">)]</span>

    <span class="n">reverse_topo_order</span> <span class="o">=</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">find_topo_sort</span><span class="p">([</span><span class="n">output_node</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">reverse_topo_order</span><span class="p">:</span>
        <span class="n">cur_node_grads</span> <span class="o">=</span> <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
        <span class="n">sum_grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">cur_node_grads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Sum gradients across different output nodes
</span>
        <span class="c1"># Calculate gradient for the current node
</span>        <span class="n">node_grad</span> <span class="o">=</span> <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">sum_grad</span><span class="p">)</span>
        <span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_grad</span>

        <span class="c1"># Propagate gradients to input nodes
</span>        <span class="k">for</span> <span class="n">input_node</span><span class="p">,</span> <span class="n">input_grad</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">node_grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">input_node</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node_to_output_grads_list</span><span class="p">:</span>
                <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">input_node</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">input_node</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">input_grad</span><span class="p">)</span>

    <span class="c1"># Collect results for gradients requested
</span>    <span class="n">grad_node_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_list</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">grad_node_list</span>
</code></pre></div></div> <p><strong>Explanation of the Correction</strong></p> <ol> <li><strong>Correct Gradient Calculation:</strong> <ul> <li>The <code class="language-plaintext highlighter-rouge">sum_grad</code> is now calculated by summing the gradients across different output nodes, ensuring that the total gradient for the current node is correctly computed.</li> <li>The <code class="language-plaintext highlighter-rouge">node.op.gradient</code> method is used to calculate the gradient of the node with respect to its inputs, considering the current sum of gradients.</li> </ul> </li> <li><strong>Gradient Propagation:</strong> <ul> <li>The gradients are propagated to input nodes using <code class="language-plaintext highlighter-rouge">zip</code> to iterate over input nodes and their corresponding gradients, ensuring correct association.</li> </ul> </li> </ol> <p>By addressing the incorrect gradient calculation, this revised code provides a more accurate and robust implementation of the backpropagation algorithm.</p> <h2 id="nontype-not-iterable-error">nontype not iterable error</h2> <p>Code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def gradients(output_node, node_list):
    """Take gradient of output node with respect to each node in node_list.

    Parameters
    ----------
    output_node: output node that we are taking derivative of.
    node_list: list of nodes that we are taking derivative wrt.

    Returns
    -------
    A list of gradient values, one for each node in node_list respectively.

    """

    # a map from node to a list of gradient contributions from each output node
    node_to_output_grads_list = {}
    # Special note on initializing gradient of output_node as oneslike_op(output_node):
    # We are really taking a derivative of the scalar reduce_sum(output_node)
    # instead of the vector output_node. But this is the common case for loss function.
    node_to_output_grads_list[output_node] = [oneslike_op(output_node)]
    # a map from node to the gradient of that node
    node_to_output_grad = {}
    # Traverse graph in reverse topological order given the output_node that we are taking gradient wrt.
    reverse_topo_order = reversed(find_topo_sort([output_node]))

    """TODO: Your code here"""
    for node in reverse_topo_order:
        cur_node_grad = node_to_output_grads_list[node]
        sum_grad = np.sum(cur_node_grad, axis=0)
        # gradient for each input node with gradient from current node
        node_grad = node.op.gradient(node, sum_grad)
        node_to_output_grad[node] = node_grad

        if node.inputs is not None:
            print('type node.inputs', type(node.inputs), 'node name', node.name,  'type node_grad', type(node_grad))
            for input_node, input_grad in zip(node.inputs, node_grad):
                if input_node not in node_to_output_grads_list:
                    node_to_output_grads_list[input_node] = []
                # This is wrong
                node_to_output_grads_list[input_node] = input_grad

        # node_grad = node.op.gradient(node, sum_grad)
        # for input in node.inputs:
        #     input_grads = node.op.gradient(input, sum_grad)
        #     if input not in node_to_output_grads_list:
        #         node_to_output_grads_list[input] = []
        #     node_to_output_grads_list[input].append(input_grads)

        # for i in range(len(node.inputs)):
        #     input_node = node.inputs[i]
        #     if input_node not in node_to_output_grads_list:
        #         node_to_output_grads_list[input_node] = []
        #     input_node_grad = node_grad[i]
        #     node_to_output_grads_list[input_node].append(input_node_grad)

    # Collect results for gradients requested.
    grad_node_list = [node_to_output_grad[node] for node in node_list]
    return grad_node_list


</code></pre></div></div> <p>Error:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>======================================================================                                               ERROR: autodiff_test.test_add_mul_mix_1                                                                              ----------------------------------------------------------------------                                               Traceback (most recent call last):                                                                                     File "/home/zt/miniconda3/lib/python3.12/site-packages/nose/case.py", line 189, in runTest                             self.test(*self.arg)                                                                                               File "/mnt/nvme1n1/zt/assignment1-2018/autodiff_test.py", line 86, in test_add_mul_mix_1                               grad_x1, grad_x2, grad_x3 = ad.gradients(y, [x1, x2, x3])                                                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                          File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 353, in gradients                                            for input_node, input_grad in zip(node.inputs, node_grad):                                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                        TypeError: 'NoneType' object is not iterable                                                                         -------------------- &gt;&gt; begin captured stdout &lt;&lt; ---------------------
type node.inputs &lt;class 'list'&gt; node name (x1+((x2*x3)*x1)) type node_grad &lt;class 'list'&gt;
type node.inputs &lt;class 'list'&gt; node name ((x2*x3)*x1) type node_grad &lt;class 'list'&gt;
type node.inputs &lt;class 'list'&gt; node name (x2*x3) type node_grad &lt;class 'list'&gt;
type node.inputs &lt;class 'list'&gt; node name x3 type node_grad &lt;class 'NoneType'&gt;
                                                                                                                     --------------------- &gt;&gt; end captured stdout &lt;&lt; ----------------------
-------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------
--------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------

</code></pre></div></div> <p>I think it’s because I don’t add placeholder op to input x value.</p> <p>Get another error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ERROR: autodiff_test.test_add_by_const                                                                               ----------------------------------------------------------------------                                               Traceback (most recent call last):                                                                                     File "/home/zt/miniconda3/lib/python3.12/site-packages/nose/case.py", line 189, in runTest                             self.test(*self.arg)                                                                                               File "/mnt/nvme1n1/zt/assignment1-2018/autodiff_test.py", line 26, in test_add_by_const                                y_val, grad_x2_val= executor.run(feed_dict = {x2 : x2_val})                                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                        File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 306, in run                                                  topo_order = find_topo_sort(self.eval_node_list)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                   File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 392, in find_topo_sort                                       topo_sort_dfs(node, visited, topo_order)                                                                           File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 400, in topo_sort_dfs                                        for n in node.inputs:                                                                                                         ^^^^^^^^^^^                                                                                             AttributeError: 'NoneType' object has no attribute 'inputs'                                                          -------------------- &gt;&gt; begin captured stdout &lt;&lt; ---------------------                                               type node.inputs &lt;class 'list'&gt; node name (x2+5) type node_grad &lt;class 'list'&gt;                                                                                                                                                            --------------------- &gt;&gt; end captured stdout &lt;&lt; ----------------------                                               -------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------                                               --------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------
</code></pre></div></div> <p>Correct code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">output_node</span><span class="p">,</span> <span class="n">node_list</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Take gradient of output node with respect to each node in node_list.

    Parameters
    ----------
    output_node: output node that we are taking derivative of.
    node_list: list of nodes that we are taking derivative wrt.

    Returns
    -------
    A list of gradient values, one for each node in node_list respectively.

    </span><span class="sh">"""</span>

    <span class="c1"># a map from node to a list of gradient contributions from each output node
</span>    <span class="n">node_to_output_grads_list</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Special note on initializing gradient of output_node as oneslike_op(output_node):
</span>    <span class="c1"># We are really taking a derivative of the scalar reduce_sum(output_node)
</span>    <span class="c1"># instead of the vector output_node. But this is the common case for loss function.
</span>    <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">output_node</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nf">oneslike_op</span><span class="p">(</span><span class="n">output_node</span><span class="p">)]</span>
    <span class="c1"># a map from node to the gradient of that node
</span>    <span class="n">node_to_output_grad</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Traverse graph in reverse topological order given the output_node that we are taking gradient wrt.
</span>    <span class="n">reverse_topo_order</span> <span class="o">=</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">find_topo_sort</span><span class="p">([</span><span class="n">output_node</span><span class="p">]))</span>

    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">reverse_topo_order</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_to_output_grads_list</span><span class="p">:</span>
            <span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="nf">sum_node_list</span><span class="p">(</span><span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
        <span class="c1"># cur_node_grad = node_to_output_grads_list[node]
</span>        <span class="c1"># sum_grad = np.sum(cur_node_grad, axis=0)
</span>        <span class="c1"># gradient for each input node with gradient from current node
</span>        <span class="c1"># node_grad = node.op.gradient(node, sum_grad)
</span>        <span class="c1"># node_to_output_grad[node] = node_grad
</span>
        <span class="c1"># if node_grad is not None:
</span>        <span class="c1">#     print('type node.inputs', type(node.inputs), 'node name', node.name,  'type node_grad', type(node_grad))
</span>        <span class="c1">#     for input_node, input_grad in zip(node.inputs, node_grad):
</span>        <span class="c1">#         if input_node not in node_to_output_grads_list:
</span>        <span class="c1">#             node_to_output_grads_list[input_node] = []
</span>        <span class="c1">#         node_to_output_grads_list[input_node] = input_grad
</span>            <span class="k">for</span> <span class="nb">id</span> <span class="p">,</span> <span class="n">in_nodes</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">in_nodes</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node_to_output_grads_list</span><span class="p">:</span>
                    <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">in_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">in_nodes</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="nb">id</span><span class="p">])</span>

        <span class="c1"># node_grad = node.op.gradient(node, sum_grad)
</span>        <span class="c1"># for input in node.inputs:
</span>        <span class="c1">#     input_grads = node.op.gradient(input, sum_grad)
</span>        <span class="c1">#     if input not in node_to_output_grads_list:
</span>        <span class="c1">#         node_to_output_grads_list[input] = []
</span>        <span class="c1">#     node_to_output_grads_list[input].append(input_grads)
</span>
        <span class="c1"># for i in range(len(node.inputs)):
</span>        <span class="c1">#     input_node = node.inputs[i]
</span>        <span class="c1">#     if input_node not in node_to_output_grads_list:
</span>        <span class="c1">#         node_to_output_grads_list[input_node] = []
</span>        <span class="c1">#     input_node_grad = node_grad[i]
</span>        <span class="c1">#     node_to_output_grads_list[input_node].append(input_node_grad)
</span>
    <span class="c1"># Collect results for gradients requested.
</span>    <span class="n">grad_node_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_list</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">grad_node_list</span>


</code></pre></div></div> <p>I don’t know why this is the correct code.</p> <p>What is the difference between my previous code and this code ?</p> <p>This is very different from pytorch like auto gradient calculation.</p> <p><a href="https://dlsys.cs.washington.edu/pdf/lecture4.pdf">Autodiff algorithm lecture</a></p> <p><strong>Yes, PyTorch leverages automatic differentiation (autodiff) to calculate gradients automatically.</strong> This powerful feature significantly simplifies the process of training neural networks.</p> <p><strong>How it Works:</strong></p> <ol> <li><strong>Computational Graph:</strong> When you perform operations on PyTorch tensors, a computational graph is created. This graph represents the sequence of operations involved in calculating the output.</li> <li><strong>Forward Pass:</strong> During the forward pass, the input data is fed into the graph, and calculations are performed to obtain the output.</li> <li><strong>Backward Pass:</strong> In the backward pass, the gradients of the output with respect to the input parameters are computed using the chain rule. PyTorch automatically traverses the computational graph in reverse order, calculating gradients for each node.</li> <li><strong>Gradient Update:</strong> The calculated gradients are then used to update the model’s parameters using an optimization algorithm like stochastic gradient descent (SGD).</li> </ol> <p><strong>Key Benefits of PyTorch’s Autodiff:</strong></p> <ul> <li><strong>Simplified Training:</strong> Autodiff eliminates the need for manual gradient calculation, making the training process more efficient.</li> <li><strong>Complex Models:</strong> It enables the training of complex deep learning models with numerous layers and parameters.</li> <li><strong>Flexibility:</strong> PyTorch’s dynamic computational graph allows for more flexibility in defining and training neural networks.</li> </ul> <p>By understanding how PyTorch’s autodiff works, you can effectively leverage this powerful tool to build and train sophisticated deep learning models.</p> <p><a href="https://github.com/yzh119/UW-dlsys-assignment1/blob/master/autodiff.py">reference repo</a></p> <p>This test code gives good example about how to use autodiff to do forward and gradient calculation.</p> <p>First we define variable in the graph.</p> <p>We do some calculations with the node in the graph. And then we first call <code class="language-plaintext highlighter-rouge">gradient()</code> to get gradient we are interested to get. And then we call <code class="language-plaintext highlighter-rouge">Excutor()</code> to create a instance of Executor to do forward pass to get the value want.</p> <p>In the constructor arg we give all the variables in graph we are interested to get value of when <code class="language-plaintext highlighter-rouge">run()</code> is called.</p> <p>To actually run and get values we need to call <code class="language-plaintext highlighter-rouge">executor.run()</code> by giving values for all inputs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_matmul_two_vars</span><span class="p">():</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">x3</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x3</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nf">matmul_op</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">)</span>

    <span class="n">grad_x2</span><span class="p">,</span> <span class="n">grad_x3</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">])</span>
    
    <span class="n">executor</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">Executor</span><span class="p">([</span><span class="n">y</span><span class="p">,</span> <span class="n">grad_x2</span><span class="p">,</span> <span class="n">grad_x3</span><span class="p">])</span>
    <span class="n">x2_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span> <span class="c1"># 3x2
</span>    <span class="n">x3_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]])</span> <span class="c1"># 2x3
</span>
    <span class="n">y_val</span><span class="p">,</span> <span class="n">grad_x2_val</span><span class="p">,</span> <span class="n">grad_x3_val</span> <span class="o">=</span> <span class="n">executor</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x2</span><span class="p">:</span> <span class="n">x2_val</span><span class="p">,</span> <span class="n">x3</span><span class="p">:</span> <span class="n">x3_val</span><span class="p">})</span>

    <span class="n">expected_yval</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x2_val</span><span class="p">,</span> <span class="n">x3_val</span><span class="p">)</span>
    <span class="n">expected_grad_x2_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">expected_yval</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">x3_val</span><span class="p">))</span>
    <span class="n">expected_grad_x3_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">x2_val</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">expected_yval</span><span class="p">))</span>

    <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ad</span><span class="p">.</span><span class="n">Node</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">expected_yval</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">grad_x2_val</span><span class="p">,</span> <span class="n">expected_grad_x2_val</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">grad_x3_val</span><span class="p">,</span> <span class="n">expected_grad_x3_val</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="ml"/><category term="framework"/><category term="ml"/><category term="framework"/><summary type="html"><![CDATA[Source assignment]]></summary></entry><entry><title type="html">palindrome substring partition</title><link href="https://bilyz98.github.io/blog/2024/palindrome-substring-partition/" rel="alternate" type="text/html" title="palindrome substring partition"/><published>2024-11-25T11:59:00+00:00</published><updated>2024-11-25T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/palindrome-substring-partition</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/palindrome-substring-partition/"><![CDATA[<p>Had an interview with the team that works on opeartor fustion.</p> <p>Failed this first round interview.</p> <p>I only gave very naive solution. Not really sad.</p> <p>I am well prepared for kinds of outcomes.</p> <p>Problem:</p> <p>given two strings a and b with same length n, you can partition two strings at the same place for two strings into a1, a2 and b1 b2, ie. length of a1 is equal to length of b1 and length of a2 is equal to the length of b2. please give a algorithm to test if there is a partition that can form a concat string such at a1 + b2 or b1 + a2 is palindrome string</p> <p>corresponding leetcode problem:</p> <p>https://leetcode.com/problems/split-two-strings-to-make-palindrome/solutions/888885/c-java-greedy-o-n-o-1/</p> <p>Certainly! Here’s a step-by-step algorithm to test if there is a partition that can form a palindrome by concatenating ( a1 + b2 ) or ( b1 + a2 ):</p> <h3 id="algorithm">Algorithm</h3> <ol> <li><strong>Input</strong>: Two strings <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> of length <code class="language-plaintext highlighter-rouge">n</code>.</li> <li><strong>Output</strong>: A boolean value indicating if there exists a partition point where ( a1 + b2 ) or ( b1 + a2 ) is a palindrome.</li> </ol> <h3 id="steps">Steps:</h3> <ol> <li><strong>Initialization</strong>: <ul> <li>Loop through each possible partition point <code class="language-plaintext highlighter-rouge">i</code> from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">n</code>.</li> </ul> </li> <li><strong>Check Partitions</strong>: <ul> <li>For each partition point <code class="language-plaintext highlighter-rouge">i</code>, split the strings into <code class="language-plaintext highlighter-rouge">a1</code>, <code class="language-plaintext highlighter-rouge">a2</code>, <code class="language-plaintext highlighter-rouge">b1</code>, and <code class="language-plaintext highlighter-rouge">b2</code>. <ul> <li>( a1 = a[0:i] )</li> <li>( a2 = a[i:n] )</li> <li>( b1 = b[0:i] )</li> <li>( b2 = b[i:n] )</li> </ul> </li> </ul> </li> <li><strong>Check Palindromes</strong>: <ul> <li>Concatenate <code class="language-plaintext highlighter-rouge">a1 + b2</code> and check if it’s a palindrome.</li> <li>Concatenate <code class="language-plaintext highlighter-rouge">b1 + a2</code> and check if it’s a palindrome.</li> <li>If either concatenation is a palindrome, return <code class="language-plaintext highlighter-rouge">True</code>.</li> </ul> </li> <li><strong>Return False</strong>: <ul> <li>If no valid palindrome is found after all partitions, return <code class="language-plaintext highlighter-rouge">False</code>.</li> </ul> </li> </ol> <h3 id="pseudocode">Pseudocode</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_palindrome</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">s</span> <span class="o">==</span> <span class="n">s</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">can_form_palindrome</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span> <span class="o">=</span> <span class="n">a</span><span class="p">[:</span><span class="n">i</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span>
        <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">b</span><span class="p">[:</span><span class="n">i</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span>
        <span class="k">if</span> <span class="nf">is_palindrome</span><span class="p">(</span><span class="n">a1</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span> <span class="ow">or</span> <span class="nf">is_palindrome</span><span class="p">(</span><span class="n">b1</span> <span class="o">+</span> <span class="n">a2</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="bp">False</span>

<span class="c1"># Example usage:
</span><span class="n">a</span> <span class="o">=</span> <span class="sh">"</span><span class="s">yourstringa</span><span class="sh">"</span>
<span class="n">b</span> <span class="o">=</span> <span class="sh">"</span><span class="s">yourstringb</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">can_form_palindrome</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</code></pre></div></div> <h3 id="explanation">Explanation</h3> <ul> <li><code class="language-plaintext highlighter-rouge">is_palindrome(s)</code>: This function checks if a given string <code class="language-plaintext highlighter-rouge">s</code> is a palindrome.</li> <li><code class="language-plaintext highlighter-rouge">can_form_palindrome(a, b)</code>: This function iterates through each possible partition point, creates the substrings <code class="language-plaintext highlighter-rouge">a1</code>, <code class="language-plaintext highlighter-rouge">a2</code>, <code class="language-plaintext highlighter-rouge">b1</code>, <code class="language-plaintext highlighter-rouge">b2</code>, and checks if either <code class="language-plaintext highlighter-rouge">a1 + b2</code> or <code class="language-plaintext highlighter-rouge">b1 + a2</code> forms a palindrome.</li> </ul> <p>This algorithm runs in ( O(n^2) ) time complexity because it performs a linear check for palindromes for each of the ( n + 1 ) partition points.</p> <p>Feel free to test this with your specific strings! If you need further refinements or have more questions, I’m here to help! 😊</p> <p>Understood! We can develop a more efficient algorithm to check for a palindrome partition by leveraging additional data structures and dynamic programming principles. Here’s a refined and more efficient approach:</p> <h3 id="efficient-algorithm">Efficient Algorithm</h3> <ol> <li><strong>Input</strong>: Two strings <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> of length <code class="language-plaintext highlighter-rouge">n</code>.</li> <li><strong>Output</strong>: A boolean value indicating if there exists a partition point where ( a1 + b2 ) or ( b1 + a2 ) is a palindrome.</li> </ol> <h3 id="steps-1">Steps:</h3> <ol> <li><strong>Initialization</strong>: <ul> <li>Precompute palindromic substrings for both <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> using a 2D boolean array <code class="language-plaintext highlighter-rouge">P[i][j]</code> which indicates if the substring from index <code class="language-plaintext highlighter-rouge">i</code> to <code class="language-plaintext highlighter-rouge">j</code> is a palindrome.</li> </ul> </li> <li><strong>Precompute Palindromic Substrings</strong>: <ul> <li>Use dynamic programming to fill in the 2D boolean arrays <code class="language-plaintext highlighter-rouge">Pa</code> and <code class="language-plaintext highlighter-rouge">Pb</code> for strings <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>, respectively.</li> </ul> </li> <li><strong>Check Possible Partitions</strong>: <ul> <li>For each partition point <code class="language-plaintext highlighter-rouge">i</code> from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">n</code>: <ul> <li>Check if <code class="language-plaintext highlighter-rouge">a[:i] + b[i:]</code> forms a palindrome by ensuring both substrings are palindromes.</li> <li>Check if <code class="language-plaintext highlighter-rouge">b[:i] + a[i:]</code> forms a palindrome by ensuring both substrings are palindromes.</li> </ul> </li> </ul> </li> <li><strong>Return Result</strong>: <ul> <li>If a valid palindrome partition is found, return <code class="language-plaintext highlighter-rouge">True</code>.</li> <li>If no valid palindrome partition is found after all partitions, return <code class="language-plaintext highlighter-rouge">False</code>.</li> </ul> </li> </ol> <h3 id="pseudocode-1">Pseudocode</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_palindrome</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="o">!=</span> <span class="n">s</span><span class="p">[</span><span class="n">end</span><span class="p">]:</span>
            <span class="k">return</span> <span class="bp">False</span>
        <span class="n">start</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">end</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="bp">True</span>

<span class="k">def</span> <span class="nf">can_form_palindrome</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    
    <span class="c1"># Precompute palindromes for string a
</span>    <span class="n">Pa</span> <span class="o">=</span> <span class="p">[[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">Pa</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">for</span> <span class="n">length</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">a</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="ow">and</span> <span class="p">(</span><span class="n">length</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">Pa</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]):</span>
                <span class="n">Pa</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="c1"># Precompute palindromes for string b
</span>    <span class="n">Pb</span> <span class="o">=</span> <span class="p">[[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">Pb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">for</span> <span class="n">length</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="ow">and</span> <span class="p">(</span><span class="n">length</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">Pb</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]):</span>
                <span class="n">Pb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="c1"># Check for possible partitions
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">Pa</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">n</span> <span class="ow">or</span> <span class="n">Pb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]):</span>
            <span class="k">return</span> <span class="bp">True</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">Pb</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">n</span> <span class="ow">or</span> <span class="n">Pa</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]):</span>
            <span class="k">return</span> <span class="bp">True</span>

    <span class="k">return</span> <span class="bp">False</span>

<span class="c1"># Example usage:
</span><span class="n">a</span> <span class="o">=</span> <span class="sh">"</span><span class="s">yourstringa</span><span class="sh">"</span>
<span class="n">b</span> <span class="o">=</span> <span class="sh">"</span><span class="s">yourstringb</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">can_form_palindrome</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</code></pre></div></div> <h3 id="explanation-1">Explanation</h3> <ul> <li><code class="language-plaintext highlighter-rouge">is_palindrome(s, start, end)</code>: Helper function to check if the substring <code class="language-plaintext highlighter-rouge">s[start:end]</code> is a palindrome.</li> <li><code class="language-plaintext highlighter-rouge">can_form_palindrome(a, b)</code>: Main function to precompute palindromic substrings and check for valid partitions. <ul> <li><strong>Precomputation</strong>: Use dynamic programming to fill in the 2D boolean arrays <code class="language-plaintext highlighter-rouge">Pa</code> and <code class="language-plaintext highlighter-rouge">Pb</code> indicating palindromic substrings for <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>.</li> <li><strong>Partition Check</strong>: For each partition point <code class="language-plaintext highlighter-rouge">i</code>, check if <code class="language-plaintext highlighter-rouge">a[:i] + b[i:]</code> or <code class="language-plaintext highlighter-rouge">b[:i] + a[i:]</code> forms a palindrome by ensuring both substrings are palindromic.</li> </ul> </li> </ul> <p>This approach reduces the time complexity by precomputing the palindromic substrings and performing efficient checks during partitioning.</p> <p>Feel free to test this refined algorithm and let me know if you need further assistance or refinements! 😊</p> <h1 id="simple-greedy-solution">Simple greedy solution</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Solution {
public:
    bool checkPalindromeFormation(string a, string b) {
        return validate(a, b) || validate(b, a);
    }

    bool validate(string &amp;a, string&amp; b) {
       int l=0;
        int r = a.size() -1;
        while(l &lt; r) {
            if(a[l] != b[r]) {
                return checkpalindrome(a, l, r) || checkpalindrome(b, l, r);
            }
            l++;
            r--;
        }
        return true;
    }

    bool checkpalindrome(string&amp;s, int l, int r) {
        while(l &lt; r) {
            if(s[l] != s[r]) return false;
            l++;
            r--;
        }
        return true;
    }
};
</code></pre></div></div> <p>Explanation: <a href="https://leetcode.com/problems/split-two-strings-to-make-palindrome/solutions/891646/java-greedy-explanation-to-o-n-solution/">https://leetcode.com/problems/split-two-strings-to-make-palindrome/solutions/891646/java-greedy-explanation-to-o-n-solution/</a></p>]]></content><author><name></name></author><category term="coding"/><category term="exam"/><category term="leetcode"/><category term="dynamic"/><category term="programming"/><summary type="html"><![CDATA[Had an interview with the team that works on opeartor fustion.]]></summary></entry><entry><title type="html">Speed up matrix multiplication 2</title><link href="https://bilyz98.github.io/blog/2024/matrix-mul-comparison/" rel="alternate" type="text/html" title="Speed up matrix multiplication 2"/><published>2024-11-12T11:59:00+00:00</published><updated>2024-11-12T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/matrix-mul-comparison</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/matrix-mul-comparison/"><![CDATA[<h3 id="benefits-of-tiling">Benefits of Tiling</h3> <p>Reduced Global Memory Accesses:</p> <p>By loading tiles into shared memory, we reduce the number of global memory accesses, which are slower compared to shared memory accesses.</p> <p>Improved Cache Efficiency:</p> <p>Tiling improves cache efficiency by ensuring that data is reused within the shared memory, reducing the need to fetch data from global memory multiple times.</p> <p>Better Utilization of GPU Resources:</p> <p>Tiling allows for better utilization of the GPU’s computational resources by dividing the work into smaller, manageable chunks that fit into the GPU’s shared memory.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kt">void</span> <span class="nf">matrixMulTile</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">width</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">dev_a</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_b</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_c</span><span class="p">;</span>

    <span class="c1">// Allocate device memory</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>

    <span class="c1">// Copy matrices to device memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="n">TILE_WIDTH</span><span class="p">);</span>
  <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">((</span><span class="n">width</span> <span class="o">+</span> <span class="n">TILE_WIDTH</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span> <span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="p">(</span><span class="n">width</span><span class="o">+</span><span class="n">TILE_WIDTH</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span> <span class="n">TILE_WIDTH</span><span class="p">);</span>
  <span class="n">matrixMulTileKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span> <span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>

  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_a</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_b</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_c</span><span class="p">);</span>


<span class="p">}</span>

<span class="cp">#define TILE_WIDTH 16
</span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">matrixMulTileKernel</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">da</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">db</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span><span class="n">dout</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">tile_A</span><span class="p">[</span><span class="n">TILE_WIDTH</span> <span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
  <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">tile_B</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>

  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">width</span><span class="o">+</span><span class="n">TILE_WIDTH</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>

      <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">da</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)];</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">if</span><span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">db</span><span class="p">[(</span><span class="n">i</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>

    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>

    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">value</span> <span class="o">+=</span> <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">tile_B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>


  <span class="p">}</span>

  <span class="n">dout</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>

<span class="p">}</span>
</code></pre></div></div> <p>This code means that we have one thread for each output element in output matrix.</p> <p>Each block of threads cooperatively loads chunks tiles of A and B into shared memory. This is done in a loop to cover all tiles that contribute to final result for the block.</p> <p><a href="https://penny-xu.github.io/blog/tiled-matrix-multiplication">Another post that talk about the tiling matrix multiplication</a></p> <h2 id="each-block-of-thread-would-load-matrix-elements-multiple-times">Each block of thread would load matrix elements multiple times</h2> <h3 id="how-it-works">How It Works</h3> <ol> <li><strong>Tile Loading</strong>: <ul> <li>For each iteration of the loop (<code class="language-plaintext highlighter-rouge">for (int i = 0; i &lt; width / TILE_WIDTH; ++i)</code>), every block loads a new set of tiles from global memory into shared memory.</li> <li>This means that each block loads a tile of matrix <code class="language-plaintext highlighter-rouge">A</code> and a corresponding tile of matrix <code class="language-plaintext highlighter-rouge">B</code> multiple times, once for each tile that contributes to the block’s portion of the output matrix <code class="language-plaintext highlighter-rouge">C</code>.</li> </ul> </li> <li><strong>Computing Partial Results</strong>: <ul> <li>Once the tiles are loaded into shared memory, the threads within the block use them to compute partial results.</li> <li>After computing the partial results, the tiles in shared memory are overwritten by the next set of tiles in the next iteration of the loop.</li> </ul> </li> <li><strong>Accumulating Results</strong>: <ul> <li>The partial results are accumulated across all iterations to compute the final value for each element in the output matrix <code class="language-plaintext highlighter-rouge">C</code>.</li> </ul> </li> </ol> <h3 id="why-this-is-necessary">Why This is Necessary</h3> <ul> <li><strong>Matrix Multiplication</strong>: Matrix multiplication requires summing the products of elements from entire rows of <code class="language-plaintext highlighter-rouge">A</code> and columns of <code class="language-plaintext highlighter-rouge">B</code>. Since the matrices are too large to fit entirely into shared memory, they must be processed in smaller chunks (tiles).</li> <li><strong>Efficiency</strong>: Using tiles and shared memory helps improve the efficiency of these operations by minimizing the slower global memory accesses.</li> </ul> <h3 id="example">Example</h3> <p>For a simplified example, let’s say <code class="language-plaintext highlighter-rouge">TILE_WIDTH</code> is 2 and our matrices are 4x4. Each block processes a 2x2 sub-matrix of the output matrix <code class="language-plaintext highlighter-rouge">C</code>. For each block, the loading process might look like this:</p> <ol> <li><strong>First Iteration</strong>: <ul> <li>Load the first 2x2 tile of <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>.</li> <li>Compute partial results using this tile.</li> </ul> </li> <li><strong>Second Iteration</strong>: <ul> <li>Load the next 2x2 tile of <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>.</li> <li>Add the products to the previously computed partial results.</li> </ul> </li> </ol> <p><img src="https://github.com/user-attachments/assets/a49597da-01cf-4ef3-ae0d-9d797b487b56" alt="tmm-59dd890f48435e692c47919d0df4a5e6"/></p> <p>This process repeats until the entire matrices have been processed. So, each block loads and processes multiple tiles to compute the final output elements.</p> <h2 id="the-whole-matrix-is-loaded-multiple-times-by-all-blocks-of-threads">The whole matrix is loaded multiple times by all blocks of threads</h2> <p>Yes, you’ve got it! The matrix elements are indeed loaded multiple times into shared memory by different blocks during the matrix multiplication process.</p> <h3 id="why-this-happens">Why This Happens</h3> <ul> <li><strong>Tiling</strong>: The entire matrix cannot fit into the limited shared memory available on the GPU. Thus, the matrix is divided into smaller tiles.</li> <li><strong>Multiple Blocks</strong>: Each block processes a portion of the matrix. Since the matrix multiplication operation requires data from different parts of the matrices, these tiles are loaded multiple times into shared memory by different blocks to complete the computation.</li> </ul> <h3 id="process-recap">Process Recap</h3> <ol> <li><strong>Loading Tiles</strong>: Each block loads a tile (a small sub-matrix) of the matrices <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> into shared memory.</li> <li><strong>Processing</strong>: The threads within the block process these tiles to compute partial results for the output matrix <code class="language-plaintext highlighter-rouge">C</code>.</li> <li><strong>Repetition</strong>: This loading and processing is repeated for every required tile until the entire matrix multiplication is complete.</li> </ol> <p>By using tiling and shared memory, the code minimizes the number of slow global memory accesses and maximizes the speed of computations by taking advantage of the faster shared memory.</p> <p>This approach ensures efficient parallel processing, leading to a significant performance boost on the GPU.</p> <h2 id="why-dont-use-threadidxx-as-row-idx-">why don’t use threadIdx.x as row idx ?</h2> <p>I see where you’re coming from, and it’s a valid question! Let’s delve deeper into why <code class="language-plaintext highlighter-rouge">threadIdx.y</code> is used as the row index and not <code class="language-plaintext highlighter-rouge">threadIdx.x</code>.</p> <h3 id="memory-coalescing-and-access-patterns">Memory Coalescing and Access Patterns</h3> <ol> <li> <p><strong>Memory Coalescing</strong>: On GPUs, memory access patterns significantly impact performance. Coalescing refers to combining multiple memory accesses into a single transaction. When threads within a warp (32 threads in modern GPUs) access contiguous memory locations, those accesses can be coalesced.</p> </li> <li> <p><strong>Row-major Order</strong>: In C/C++, arrays are stored in row-major order, meaning the elements of each row are stored in contiguous memory locations. For instance, in a 2D array <code class="language-plaintext highlighter-rouge">A</code>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">A[0][0], A[0][1], A[0][2], ..., A[0][N-1]</code> are stored consecutively in memory.</li> </ul> </li> </ol> <h3 id="access-efficiency">Access Efficiency</h3> <p>When we use <code class="language-plaintext highlighter-rouge">threadIdx.y</code> as the row index and <code class="language-plaintext highlighter-rouge">threadIdx.x</code> as the column index, it ensures that all threads in the same row of a block access contiguous memory locations in the global memory.</p> <h3 id="example-1">Example</h3> <p>Consider a tile size of 4x4 for simplicity. When loading a tile from global memory to shared memory:</p> <ul> <li><strong>Using <code class="language-plaintext highlighter-rouge">threadIdx.y</code> as row index</strong>: <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)];</span>
</code></pre></div> </div> <p>Each row in <code class="language-plaintext highlighter-rouge">tile_A</code> is loaded by threads in a row of the block, accessing contiguous elements in the global memory:</p> <ul> <li>Thread (0,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)]</code></li> <li>Thread (0,1) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 1)]</code></li> <li>Thread (0,2) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 2)]</code></li> <li>Thread (0,3) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 3)]</code></li> </ul> <p>All these accesses can be coalesced into a single transaction.</p> </li> <li><strong>Using <code class="language-plaintext highlighter-rouge">threadIdx.x</code> as row index</strong>: <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)];</span>
</code></pre></div> </div> <p>Now, each column in <code class="language-plaintext highlighter-rouge">tile_A</code> is loaded by threads in a column of the block, accessing non-contiguous elements in the global memory:</p> <ul> <li>Thread (0,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)]</code></li> <li>Thread (1,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)] + width</code></li> <li>Thread (2,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)] + 2 * width</code></li> <li>Thread (3,0) accesses <code class="language-plaintext highlighter-rouge">d_A[row * width + (i * TILE_WIDTH + 0)] + 3 * width</code></li> </ul> <p>These accesses are not contiguous and cannot be efficiently coalesced.</p> </li> </ul> <h3 id="summary">Summary</h3> <p>Using <code class="language-plaintext highlighter-rouge">threadIdx.y</code> as the row index and <code class="language-plaintext highlighter-rouge">threadIdx.x</code> as the column index ensures that threads within the same row of a block access contiguous memory locations. This pattern optimizes memory coalescing and significantly improves performance on GPUs.</p> <p>I hope this clarifies why the indices are arranged this way! If you have more questions or need further explanations, feel free to ask. 😊</p> <p>cuda has a limit on the number of threads per block, typically 1024. If width is larger than 32, kernel launch will fail. And we get this error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@ln102%tianhe2-K matmul]$ yhrun -p gpu_v100 ./matmul_cuda
CUDA error: invalid configuration argument
cuda Duration: 345.838 ms
cuda tile Duration: 25.2786 ms
check failed: 0, cuda naive: 0, cuda tile: 2000
yhrun: error: gpu29: task 0: Exited with exit code 1
</code></pre></div></div> <p>Comparison between cpu, cuda native and cuda matrix multiplication with tiling.</p> <p>Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="cp">#define N 2 // Matrix size
#define BLOCK_SIZE 16 // Block size
</span>
<span class="kt">void</span> <span class="nf">matmul_cpu</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">input_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">input_row_idx</span><span class="o">&lt;</span> <span class="n">width</span><span class="p">;</span> <span class="n">input_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>  <span class="n">output_col_idx</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">;</span> <span class="n">output_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">element_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">element_idx</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">;</span> <span class="n">element_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="c1">// value += a[input_row_idx][element_idx] * b[element_idx][output_col_idx];</span>
          <span class="n">value</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">input_row_idx</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">element_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">element_idx</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">output_col_idx</span><span class="p">];</span>

      <span class="p">}</span>
      <span class="n">c</span><span class="p">[</span><span class="n">input_row_idx</span><span class="o">*</span><span class="n">width</span><span class="o">+</span><span class="n">output_col_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>
      <span class="c1">// c[input_row_idx][output_col_idx] = value;</span>
    <span class="p">}</span>
  <span class="p">}</span>

<span class="p">}</span>

<span class="c1">// CUDA Kernel for Matrix Multiplication</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">MatrixMul</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">val</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">c</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span> <span class="nf">matmul_cuda_naive</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">width</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">dev_a</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_b</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_c</span><span class="p">;</span>

    <span class="c1">// Allocate device memory</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>

    <span class="c1">// Copy matrices to device memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

    <span class="c1">// Launch kernel</span>

    <span class="n">dim3</span> <span class="n">threadsPerBlock</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">numBlocks</span><span class="p">((</span><span class="n">width</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="p">(</span><span class="n">width</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>
    <span class="c1">// dim3 dimBlock(width, width);</span>
    <span class="c1">// dim3 dimGrid(1, 1);</span>

    <span class="n">MatrixMul</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
    <span class="c1">// MatrixMul&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(dev_a, dev_b, dev_c, width);</span>

    <span class="c1">// Synchronize CPU and GPU</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>

    <span class="c1">// Check for errors</span>
    <span class="n">cudaError_t</span> <span class="n">error</span> <span class="o">=</span> <span class="n">cudaGetLastError</span><span class="p">();</span>
    <span class="k">if</span><span class="p">(</span><span class="n">error</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"CUDA error: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">error</span><span class="p">));</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Copy result back to host memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

    <span class="c1">// Print the result</span>
    <span class="c1">// for (int y = 0; y &lt; N; y++) {</span>
    <span class="c1">//     for (int x = 0; x &lt; N; x++) {</span>
    <span class="c1">//         printf("%d ", c[y][x]);</span>
    <span class="c1">//     }</span>
    <span class="c1">//     printf("\n");</span>
    <span class="c1">// }</span>

    <span class="c1">// Free device memory</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_a</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_b</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_c</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>


<span class="p">}</span>


<span class="kt">void</span> <span class="nf">matrixMulTile</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">width</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">dev_a</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_b</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_c</span><span class="p">;</span>

    <span class="c1">// Allocate device memory</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>

    <span class="c1">// Copy matrices to device memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="n">TILE_WIDTH</span><span class="p">);</span>
  <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">((</span><span class="n">width</span> <span class="o">+</span> <span class="n">TILE_WIDTH</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span> <span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="p">(</span><span class="n">width</span><span class="o">+</span><span class="n">TILE_WIDTH</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span> <span class="n">TILE_WIDTH</span><span class="p">);</span>
  <span class="n">matrixMulTileKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span> <span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>

  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_a</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_b</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">dev_c</span><span class="p">);</span>


<span class="p">}</span>

<span class="cp">#define TILE_WIDTH 16
</span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">matrixMulTileKernel</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">da</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">db</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span><span class="n">dout</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">tile_A</span><span class="p">[</span><span class="n">TILE_WIDTH</span> <span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
  <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">tile_B</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>

  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">width</span><span class="o">+</span><span class="n">TILE_WIDTH</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span><span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>

      <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">da</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)];</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">if</span><span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">db</span><span class="p">[(</span><span class="n">i</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>

    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>

    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">value</span> <span class="o">+=</span> <span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">tile_B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>


  <span class="p">}</span>

  <span class="n">dout</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>

<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">width</span><span class="p">;</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">h_A</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">h_B</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">h_C</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

  <span class="kt">int</span> <span class="o">*</span><span class="n">hc2</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">size</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="n">h_A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">h_B</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>


      <span class="k">auto</span> <span class="n">start</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
  <span class="n">matmul_cuda_naive</span><span class="p">(</span><span class="n">h_A</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">h_C</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
      <span class="k">auto</span> <span class="n">end</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
      <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">duration</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">milli</span><span class="o">&gt;</span> <span class="n">duration</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">;</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"cuda Duration: "</span> <span class="o">&lt;&lt;</span> <span class="n">duration</span><span class="p">.</span><span class="n">count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">" ms"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>


       <span class="n">start</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
  <span class="n">matrixMulTile</span><span class="p">(</span><span class="n">h_A</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">hc2</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
       <span class="n">end</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
      <span class="n">duration</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">;</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"cuda tile Duration: "</span> <span class="o">&lt;&lt;</span> <span class="n">duration</span><span class="p">.</span><span class="n">count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">" ms"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>


  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span><span class="n">size</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">hc2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">h_C</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">{</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">"check failed: %d, cuda naive: %d, cuda tile: %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">i</span> <span class="p">,</span> <span class="n">h_C</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hc2</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
      <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="n">printf</span><span class="p">(</span><span class="s">"correctness check pass</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>


       <span class="n">start</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
      <span class="n">matmul_cpu</span><span class="p">(</span><span class="n">h_A</span><span class="p">,</span> <span class="n">h_B</span> <span class="p">,</span> <span class="n">hc2</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
       <span class="n">end</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
       <span class="n">duration</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">;</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"cpu Duration: "</span> <span class="o">&lt;&lt;</span> <span class="n">duration</span><span class="p">.</span><span class="n">count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">" ms"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>



  <span class="n">free</span><span class="p">(</span><span class="n">h_A</span><span class="p">);</span> <span class="n">free</span><span class="p">(</span><span class="n">h_B</span><span class="p">);</span> <span class="n">free</span><span class="p">(</span><span class="n">h_C</span><span class="p">);</span>
<span class="p">}</span>

</code></pre></div></div> <p>indexing for da and db in cuda kernal explanations</p> <p>The indexing of matrix elements in the <code class="language-plaintext highlighter-rouge">matrixMulTileKernel</code> function is designed to optimize memory access patterns and improve performance by leveraging shared memory. Let’s break down the indexing for <code class="language-plaintext highlighter-rouge">da</code> and <code class="language-plaintext highlighter-rouge">db</code>:</p> <p>Simply speaking, <code class="language-plaintext highlighter-rouge">i</code> has different meaning for <code class="language-plaintext highlighter-rouge">da</code> and <code class="language-plaintext highlighter-rouge">db</code>, for <code class="language-plaintext highlighter-rouge">da</code> i loop for all columns in the same row. for <code class="language-plaintext highlighter-rouge">db</code> i loops all rows in the same column.</p> <h3 id="indexing-for-da-matrix-a">Indexing for <code class="language-plaintext highlighter-rouge">da</code> (Matrix A)</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tile_A</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">da</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)];</span>
</code></pre></div></div> <ul> <li><strong>Row Calculation</strong>: <code class="language-plaintext highlighter-rouge">row = blockIdx.y * TILE_WIDTH + threadIdx.y</code> <ul> <li><code class="language-plaintext highlighter-rouge">blockIdx.y * TILE_WIDTH</code> gives the starting row index for the block.</li> <li><code class="language-plaintext highlighter-rouge">threadIdx.y</code> gives the row index within the block.</li> </ul> </li> <li><strong>Column Calculation</strong>: <code class="language-plaintext highlighter-rouge">(i * TILE_WIDTH + threadIdx.x)</code> <ul> <li><code class="language-plaintext highlighter-rouge">i * TILE_WIDTH</code> shifts the column index by the tile width for each iteration.</li> <li><code class="language-plaintext highlighter-rouge">threadIdx.x</code> gives the column index within the block.</li> </ul> </li> </ul> <h3 id="indexing-for-db-matrix-b">Indexing for <code class="language-plaintext highlighter-rouge">db</code> (Matrix B)</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tile_B</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">db</span><span class="p">[(</span><span class="n">i</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
</code></pre></div></div> <ul> <li><strong>Row Calculation</strong>: <code class="language-plaintext highlighter-rouge">(i * TILE_WIDTH + threadIdx.y)</code> <ul> <li><code class="language-plaintext highlighter-rouge">i * TILE_WIDTH</code> shifts the row index by the tile width for each iteration.</li> <li><code class="language-plaintext highlighter-rouge">threadIdx.y</code> gives the row index within the block.</li> </ul> </li> <li><strong>Column Calculation</strong>: <code class="language-plaintext highlighter-rouge">col = blockIdx.x * TILE_WIDTH + threadIdx.x</code> <ul> <li><code class="language-plaintext highlighter-rouge">blockIdx.x * TILE_WIDTH</code> gives the starting column index for the block.</li> <li><code class="language-plaintext highlighter-rouge">threadIdx.x</code> gives the column index within the block.</li> </ul> </li> </ul> <h3 id="why-this-indexing">Why This Indexing?</h3> <ul> <li><strong>Memory Coalescing</strong>: Using <code class="language-plaintext highlighter-rouge">threadIdx.x</code> for columns and <code class="language-plaintext highlighter-rouge">threadIdx.y</code> for rows ensures that threads within a warp access contiguous memory locations. This pattern optimizes memory coalescing, which combines multiple memory accesses into a single transaction, significantly improving performance.</li> <li><strong>Efficient Tiling</strong>: By dividing the matrices into smaller tiles that fit into shared memory, the kernel reduces the number of global memory accesses. Each tile is loaded multiple times, but the overall number of accesses to global memory is minimized, leading to better performance.</li> </ul> <p>Test matrix size is (2000, 2000) cuda naive reduce run time by 54x compared to cpu. cuda tiling reduce run time by 17x compared to cuda naive.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@ln102%tianhe2-K matmul]$ yhrun -p gpu_v100 ./matmul_cuda
cuda Duration: 432.435 ms
cuda tile Duration: 25.7345 ms
cpu Duration: 23042.4 ms
correctness check pass
</code></pre></div></div> <p>The benefit of tiling is that it reduces number of access to global shared memory.</p> <blockquote> <p>No, tiling not just replaces random accesses with sequential ones. It actually saves tons of bandwidth to global memory.</p> </blockquote> <blockquote> <p>Let’s say we multiply two large square matrices of size S×S, where S is a multiple of 32. Obviously, the result is also a square matrix of size S×S.</p> </blockquote> <blockquote> <p>With naïve algorithm, to compute each element of the result, we gonna need to fetch S elements from both matrices. The output matrix has S^2 elements, therefore the total count of loaded elements is 2*S^3.</p> </blockquote> <blockquote> <p>With 32×32 tiling, to compute each 32×32 tile of the result, we gonna need to fetch S/32 tiles from both matrices. The output size in tiles is (S/32)^2, the total count of loaded tiles is 2<em>(S/32)^3. Each 32×32 tile contains 32^2 elements, the total count of loaded elements is therefore (32^2)</em>2<em>(S/32)^3 = (2/32)</em>S^3. Therefore, the tiling reduced global memory bandwidth by the factor of 32, which is a huge performance win.</p> </blockquote>]]></content><author><name></name></author><category term="ml"/><category term="ai"/><category term="ml"/><category term="matrix"/><category term="multiplication"/><summary type="html"><![CDATA[Benefits of Tiling Reduced Global Memory Accesses:]]></summary></entry><entry><title type="html">Stf CS149 flash attention</title><link href="https://bilyz98.github.io/blog/2024/cs149gpt/" rel="alternate" type="text/html" title="Stf CS149 flash attention"/><published>2024-11-12T11:59:00+00:00</published><updated>2024-11-12T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cs149gpt</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cs149gpt/"><![CDATA[<p><a href="https://github.com/stanford-cs149/cs149gpt">Stf CS149 flash attention lab assignment repo</a></p> <h2 id="install-library-to-compile-code">Install library to compile code</h2> <p>It shows error about error loading shared object.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(cs149gpt) ➜  cs149gpt git:(main) ✗ python3 gpt149.py 4Daccess
/home/zt/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)                                                                                        return torch._C._cuda_getDeviceCount() &gt; 0
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 14, in &lt;module&gt;
    import module_ref as ms
ImportError: /home/zt/stf-cs149-pp/cs149gpt/module_ref.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE
</code></pre></div></div> <p>I tried to use conda to create new env and install low version of pytorch but conda always installs 2.3.x version of torch for me.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 cpuonly python=3.10 numpy=1.26 ninja tiktoken -c pytorch -c conda-forge
</code></pre></div></div> <p>What it is required is 2.1.x version of torch.</p> <p>Then I manually use pip to uninstall torch and reinstall 2.1.x version of torch</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> pip3 uninstall torch
pip3 install torch==2.1.2

</code></pre></div></div> <p>Got another issue saying that I should use numpy version &lt; 2.0.</p> <p>Then I uninstall numpy and reinstall it with version 1.2.x</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 uninstall numpy
 pip3 install numpy==1.26
</code></pre></div></div> <p>So now I can run the code successfully.</p> <h2 id="part-1-naive-attention">Part 1: naive attention</h2> <p>My code produce the value that is 0.0003 less than what solution produces for each element.</p> <p>I don’t know why.</p> <p>Should I use double ?</p> <p>First implementation:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myNaiveAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QK_tTensor</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>

    <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
    <span class="c1">//QK^t Intermediate Tensor has Shape (N, N)</span>
    
    <span class="c1">//Make O Tensor with Shape (B, H, N, d) </span>
    <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

    <span class="c1">//Format O, Q, K, and V tensors into 4D vectors</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>

    <span class="c1">//Format QK_t Tensor into a 2D vector.</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">QK_t</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QK_tTensor</span><span class="p">);</span>
    
    <span class="cm">/* Here is an example of how to read/write 0's to  Q (B, H, N, d) using the 4D accessors

        //loop over Batch Size
         for (int b = 0; b &lt; B; b++) {

             //loop over Heads
             for (int h = 0; h &lt; H; h++) {

                 //loop over Sequence Length
                 for (int i = 0; i &lt; N; i++) {

                     //loop over Embedding Dimensionality
                     for (int j = 0; j &lt; d; j++) {
                        float val = fourDimRead(Q, b, h, i, j, H, N, d);
                        val = 0.0;
                        fourDimWrite(Q, b, h, i, j, H, N, d, val);
                     }
                 }
             }
         }
    */</span>

    <span class="cm">/* Here is an example of how to read/write 0's to  QK_t (N, N) using the 2D accessors

           for (int i = 0; i &lt; N; i++) {
	       for (int j = 0; j &lt; N; j++) {
	           float val = twoDimRead(QK_t, i, j, N);
               val = 0.0;
	           twoDimWrite(QK_t, i, j, N, val);
             }
         }
    */</span>
    
    <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
       <span class="c1">//loop over Heads</span>
       <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
           <span class="c1">//loop over Sequence Length</span>
           <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">seq_i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">seq_i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">seq_i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
             <span class="c1">//loop over Embedding Dimensionality</span>
              <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
               <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">q_row</span>  <span class="o">=</span> <span class="n">i</span><span class="p">;</span> 
                  <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                  <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                  <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">seq_i</span><span class="p">;</span>
                  <span class="c1">// float val = fourDimRead(Q, b, h, i, j, H, N, d);</span>
          <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>


                  <span class="c1">// val = 0.0;</span>
                  <span class="c1">// fourDimWrite(Q, b, h, i, j, H, N, d, val);</span>
               <span class="p">}</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq_i</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">val</span> <span class="p">);</span>

            <span class="p">}</span>

           <span class="p">}</span>
          <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">);</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

            <span class="kt">float</span> <span class="n">row_sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span><span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
               <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
              <span class="kt">float</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
              <span class="n">row_sum</span> <span class="o">+=</span> <span class="n">exp_val</span><span class="p">;</span>
              <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp_val</span><span class="p">;</span>

            <span class="p">}</span>

            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
              <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">row_sum</span><span class="p">;</span>
              <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">prob</span><span class="p">);</span>
            <span class="p">}</span>
          <span class="p">}</span>


        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">qkt_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">qkt_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">qkt_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">output_d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">output_d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">m_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">m_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">m_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">qkt_val</span> <span class="o">=</span>  <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">m_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
            <span class="kt">int</span> <span class="n">v_row</span> <span class="o">=</span> <span class="n">m_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_col</span> <span class="o">=</span> <span class="n">output_d_idx</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row</span><span class="p">,</span> <span class="n">v_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">qkt_val</span> <span class="o">*</span> <span class="n">v_val</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">output_d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="p">,</span><span class="n">val</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="p">}</span>
       <span class="p">}</span>
   <span class="p">}</span>




    
    <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
    <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-17 21:15:18 207308:207308 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0768, 0.0776, 0.0784, 0.0792, 0.0800, 0.0808, 0.0816, 0.0824, 0.0832,
        0.0840, 0.0848, 0.0856, 0.0864, 0.0872, 0.0880, 0.0888, 0.0896, 0.0904,
        0.0912, 0.0920, 0.0928, 0.0936, 0.0944, 0.0952, 0.0960, 0.0968, 0.0976,
        0.0984, 0.0992, 0.1000, 0.1008, 0.1016])
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 311, in main
    part1Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 221, in part1Test
    testTemplate(attentionModuleReference.myUnfusedAttention, params, "STUDENT - NAIVE ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Try to use double to store itermediate result to fix this problem.</p> <p>I have two issues in previous version of code</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
       <span class="c1">//loop over Heads</span>
       <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="c1">//loop over Sequence Length</span>
          <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">seq_i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">seq_i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">seq_i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
             <span class="c1">//loop over Embedding Dimensionality</span>
              <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
               <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">q_row</span>  <span class="o">=</span> <span class="n">i</span><span class="p">;</span> 
                  <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>

                    <span class="c1">// this is the correct indexing for the second matrix.</span>
                    <span class="c1">// Since K is not transposed.</span>
                     <span class="c1">// K should be indexed with (seq_i, j) instead of (j, seq_i) like normal matrix multiplciation</span>
                  <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">seq_i</span><span class="p">;</span>
                  <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                  <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>
               <span class="p">}</span>
                <span class="c1">// This is the second place that is fixed. </span>
                 <span class="c1">// QK_t is two dimenional. </span>
                 <span class="c1">// Should use twoDimWrite</span>
              <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq_i</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">val</span> <span class="p">);</span>
            <span class="p">}</span>

           <span class="p">}</span>
 
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
REFERENCE - NAIVE ATTENTION statistics
cpu time:  293.585ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:10:17 1148267:1148267 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:10:17 1148267:1148267 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 13:10:17 1148267:1148267 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.2747969627380371

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::empty         0.01%      16.000us         0.01%      16.000us       2.000us       5.00 Mb       5.00 Mb             8
    STUDENT - NAIVE ATTENTION        99.49%     273.438ms        99.68%     273.946ms     273.946ms       4.50 Mb      -1.00 Mb             1
                  aten::zeros         0.01%      14.000us         0.05%     133.000us      66.500us       4.50 Mb           0 b             2
                  aten::clone         0.01%      20.000us         0.13%     346.000us     173.000us       1.00 Mb           0 b             2
          aten::empty_strided         0.00%      12.000us         0.00%      12.000us       2.400us     512.51 Kb     512.51 Kb             5
              model_inference         0.20%     549.000us       100.00%     274.838ms     274.838ms     512.00 Kb      -4.00 Mb             1
                aten::flatten         0.01%      19.000us         0.08%     209.000us      41.800us     512.00 Kb           0 b             5
             aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                     aten::to         0.00%       6.000us         0.01%      31.000us       5.167us         520 b           0 b             6
               aten::_to_copy         0.01%      14.000us         0.01%      25.000us       6.250us         520 b           0 b             4
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 274.838ms

STUDENT - NAIVE ATTENTION statistics
cpu time:  273.946ms
mem usage:  4718592 bytes
</code></pre></div></div> <h2 id="part2-blocked-matrix-multiplication">Part2: blocked matrix multiplication</h2> <p>Initial version</p> <p>Unlike that of code written in cuda, I have to manually iterate all rows and cols of input matrix.</p> <p>Don’t know if this is correct way to do that.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define TILE_SIZE 16
</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myUnfusedAttentionBlocked</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QK_tTensor</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>
  
  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
  <span class="c1">//QK^t Intermediate Tensor has Shape (N, N)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d) </span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

  <span class="c1">//Format O, Q, K, and V tensors into 4D vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>

  <span class="c1">//Format QK_t Tensor into a 2D vector.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">QK_t</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QK_tTensor</span><span class="p">);</span>

  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">q_row_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">q_row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">q_row_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// K is not transposed so we traverse k by row.</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_row_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k_row_tile_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_col_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_col_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">d_col_tile_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
              <span class="c1">// int out_row_idx = q_row_tile_idx * TILE_SIZE + tile_row_idx;</span>
              <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="c1">// int out_col_idx = k_row_tile_idx * TILE_SIZE + tile_col_idx;</span>
                <span class="kt">int</span> <span class="n">q_col_idx</span> <span class="o">=</span> <span class="n">d_col_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span><span class="n">q_row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span> 
                <span class="kt">int</span> <span class="n">k_row_idx</span> <span class="o">=</span> <span class="n">k_row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
                <span class="kt">int</span> <span class="n">k_col_idx</span> <span class="o">=</span> <span class="n">d_col_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="k">if</span><span class="p">(</span><span class="n">q_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">q_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span> <span class="o">&amp;&amp;</span> <span class="n">k_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">k_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">float</span> <span class="n">q_tile_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">q_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">k_tile_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row_idx</span><span class="p">,</span> <span class="n">k_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">orig_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">q_row_tile_idx</span><span class="p">,</span> <span class="n">k_row_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">q_tile_val</span> <span class="o">*</span> <span class="n">k_tile_val</span> <span class="o">+</span> <span class="n">orig_val</span><span class="p">;</span>
                  <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">q_row_tile_idx</span><span class="p">,</span> <span class="n">k_row_tile_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">val</span> <span class="p">);</span>
                <span class="p">}</span>
              <span class="p">}</span>
            <span class="p">}</span>
          <span class="p">}</span>

        <span class="p">}</span>
      <span class="p">}</span>

      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">);</span>
        <span class="kt">double</span> <span class="n">row_sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span><span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
           <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
          <span class="kt">double</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
          <span class="n">row_sum</span> <span class="o">+=</span> <span class="n">exp_val</span><span class="p">;</span>
          <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp_val</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">row_sum</span><span class="p">;</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">prob</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">qkt_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">qkt_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">qkt_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">output_d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">output_d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">m_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">m_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">m_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">qkt_val</span> <span class="o">=</span>  <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">m_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
            <span class="kt">int</span> <span class="n">v_row</span> <span class="o">=</span> <span class="n">m_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_col</span> <span class="o">=</span> <span class="n">output_d_idx</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row</span><span class="p">,</span> <span class="n">v_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">qkt_val</span> <span class="o">*</span> <span class="n">v_val</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">output_d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="p">,</span><span class="n">val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

    <span class="p">}</span>

  <span class="p">}</span>


  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <p>It’s not correct</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">-----</span><span class="n">RUNNING</span> <span class="n">STUDENT</span> <span class="n">IMPLEMENTATION</span><span class="o">-----</span>

<span class="n">first</span> <span class="n">row</span> <span class="n">value</span><span class="o">:</span>
 <span class="n">tensor</span><span class="p">([</span><span class="mf">0.0771</span><span class="p">,</span> <span class="mf">0.0779</span><span class="p">,</span> <span class="mf">0.0787</span><span class="p">,</span> <span class="mf">0.0795</span><span class="p">,</span> <span class="mf">0.0803</span><span class="p">,</span> <span class="mf">0.0811</span><span class="p">,</span> <span class="mf">0.0819</span><span class="p">,</span> <span class="mf">0.0827</span><span class="p">,</span> <span class="mf">0.0835</span><span class="p">,</span>
        <span class="mf">0.0843</span><span class="p">,</span> <span class="mf">0.0851</span><span class="p">,</span> <span class="mf">0.0859</span><span class="p">,</span> <span class="mf">0.0867</span><span class="p">,</span> <span class="mf">0.0875</span><span class="p">,</span> <span class="mf">0.0883</span><span class="p">,</span> <span class="mf">0.0891</span><span class="p">,</span> <span class="mf">0.0899</span><span class="p">,</span> <span class="mf">0.0907</span><span class="p">,</span>
        <span class="mf">0.0915</span><span class="p">,</span> <span class="mf">0.0923</span><span class="p">,</span> <span class="mf">0.0931</span><span class="p">,</span> <span class="mf">0.0939</span><span class="p">,</span> <span class="mf">0.0947</span><span class="p">,</span> <span class="mf">0.0955</span><span class="p">,</span> <span class="mf">0.0963</span><span class="p">,</span> <span class="mf">0.0971</span><span class="p">,</span> <span class="mf">0.0979</span><span class="p">,</span>
        <span class="mf">0.0987</span><span class="p">,</span> <span class="mf">0.0995</span><span class="p">,</span> <span class="mf">0.1003</span><span class="p">,</span> <span class="mf">0.1011</span><span class="p">,</span> <span class="mf">0.1019</span><span class="p">])</span>
<span class="n">STAGE</span><span class="o">:</span><span class="mi">2024</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">18</span> <span class="mi">11</span><span class="o">:</span><span class="mi">29</span><span class="o">:</span><span class="mi">24</span> <span class="mi">607233</span><span class="o">:</span><span class="mi">607233</span> <span class="n">ActivityProfilerController</span><span class="p">.</span><span class="n">cpp</span><span class="o">:</span><span class="mi">312</span><span class="p">]</span> <span class="n">Completed</span> <span class="n">Stage</span><span class="o">:</span> <span class="n">Warm</span> <span class="n">Up</span>
<span class="n">student</span> <span class="n">first</span> <span class="n">row</span><span class="o">:</span>
 <span class="n">tensor</span><span class="p">([</span><span class="mf">0.0767</span><span class="p">,</span> <span class="mf">0.0775</span><span class="p">,</span> <span class="mf">0.0783</span><span class="p">,</span> <span class="mf">0.0791</span><span class="p">,</span> <span class="mf">0.0799</span><span class="p">,</span> <span class="mf">0.0807</span><span class="p">,</span> <span class="mf">0.0815</span><span class="p">,</span> <span class="mf">0.0823</span><span class="p">,</span> <span class="mf">0.0831</span><span class="p">,</span>
        <span class="mf">0.0839</span><span class="p">,</span> <span class="mf">0.0847</span><span class="p">,</span> <span class="mf">0.0855</span><span class="p">,</span> <span class="mf">0.0863</span><span class="p">,</span> <span class="mf">0.0871</span><span class="p">,</span> <span class="mf">0.0879</span><span class="p">,</span> <span class="mf">0.0887</span><span class="p">,</span> <span class="mf">0.0895</span><span class="p">,</span> <span class="mf">0.0903</span><span class="p">,</span>
        <span class="mf">0.0911</span><span class="p">,</span> <span class="mf">0.0919</span><span class="p">,</span> <span class="mf">0.0927</span><span class="p">,</span> <span class="mf">0.0935</span><span class="p">,</span> <span class="mf">0.0943</span><span class="p">,</span> <span class="mf">0.0951</span><span class="p">,</span> <span class="mf">0.0959</span><span class="p">,</span> <span class="mf">0.0967</span><span class="p">,</span> <span class="mf">0.0975</span><span class="p">,</span>
        <span class="mf">0.0983</span><span class="p">,</span> <span class="mf">0.0991</span><span class="p">,</span> <span class="mf">0.0999</span><span class="p">,</span> <span class="mf">0.1007</span><span class="p">,</span> <span class="mf">0.1015</span><span class="p">])</span>
<span class="n">STAGE</span><span class="o">:</span><span class="mi">2024</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">18</span> <span class="mi">11</span><span class="o">:</span><span class="mi">29</span><span class="o">:</span><span class="mi">24</span> <span class="mi">607233</span><span class="o">:</span><span class="mi">607233</span> <span class="n">ActivityProfilerController</span><span class="p">.</span><span class="n">cpp</span><span class="o">:</span><span class="mi">318</span><span class="p">]</span> <span class="n">Completed</span> <span class="n">Stage</span><span class="o">:</span> <span class="n">Collection</span>
<span class="n">STAGE</span><span class="o">:</span><span class="mi">2024</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">18</span> <span class="mi">11</span><span class="o">:</span><span class="mi">29</span><span class="o">:</span><span class="mi">24</span> <span class="mi">607233</span><span class="o">:</span><span class="mi">607233</span> <span class="n">ActivityProfilerController</span><span class="p">.</span><span class="n">cpp</span><span class="o">:</span><span class="mi">322</span><span class="p">]</span> <span class="n">Completed</span> <span class="n">Stage</span><span class="o">:</span> <span class="n">Post</span> <span class="n">Processing</span>
<span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span><span class="o">:</span>
  <span class="n">File</span> <span class="s">"/home/zt/stf-cs149-pp/cs149gpt/gpt149.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">329</span><span class="p">,</span> <span class="n">in</span> <span class="o">&lt;</span><span class="k">module</span><span class="o">&gt;</span>
    <span class="n">main</span><span class="p">()</span>
  <span class="n">File</span> <span class="s">"/home/zt/stf-cs149-pp/cs149gpt/gpt149.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">313</span><span class="p">,</span> <span class="n">in</span> <span class="n">main</span>
    <span class="n">part2Test</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
  <span class="n">File</span> <span class="s">"/home/zt/stf-cs149-pp/cs149gpt/gpt149.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">233</span><span class="p">,</span> <span class="n">in</span> <span class="n">part2Test</span>
    <span class="n">testTemplate</span><span class="p">(</span><span class="n">attentionModuleReference</span><span class="p">.</span><span class="n">myUnfusedAttentionBlocked</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="s">"STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX"</span><span class="p">)</span>
  <span class="n">File</span> <span class="s">"/home/zt/stf-cs149-pp/cs149gpt/gpt149.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">182</span><span class="p">,</span> <span class="n">in</span> <span class="n">testTemplate</span>
    <span class="n">assert</span> <span class="n">torch</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">QKV</span><span class="p">,</span><span class="n">QKS1</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">),</span> <span class="n">correctness_error_message</span>
<span class="n">AssertionError</span><span class="o">:</span>
<span class="o">-------------------------------------------</span>
 <span class="n">YOUR</span> <span class="n">ATTENTION</span> <span class="n">PRODUCED</span> <span class="n">INCORRECT</span> <span class="n">RESULTS</span>
</code></pre></div></div> <p>Fixed the two indexing issue in original issue.</p> <p>This is the correct implementation given by gpt. However, it’s too slow</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      for (int q_row_tile_idx = 0; q_row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; q_row_tile_idx++) {
    for (int k_row_tile_idx = 0; k_row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; k_row_tile_idx++) {
        for (int d_col_tile_idx = 0; d_col_tile_idx &lt; (d + TILE_SIZE - 1) / TILE_SIZE; d_col_tile_idx++) {
            for (int tile_row_idx = 0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {
                for (int tile_col_idx = 0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {
                    int q_row_idx = q_row_tile_idx * TILE_SIZE + tile_row_idx;
                    int k_row_idx = k_row_tile_idx * TILE_SIZE + tile_col_idx; // Fix indexing
                    for (int d_idx = d_col_tile_idx * TILE_SIZE; d_idx &lt; (d_col_tile_idx + 1) * TILE_SIZE; d_idx++) {
                        if (q_row_idx &lt; N &amp;&amp; k_row_idx &lt; N &amp;&amp; d_idx &lt; d) {
                            float q_tile_val = fourDimRead(Q, b, h, q_row_idx, d_idx, H, N, d);
                            float k_tile_val = fourDimRead(K, b, h, k_row_idx, d_idx, H, N, d);
                            float orig_val = twoDimRead(QK_t, q_row_idx, k_row_idx, N);
                            float val = q_tile_val * k_tile_val + orig_val;
                            twoDimWrite(QK_t, q_row_idx, k_row_idx, N, val);
                        }
                    }
                }
            }
        }
    }
}
</code></pre></div></div> <p>Explanation from gpt</p> <p>You’re right to question the necessity of traversing the dimension ( d ) fully. Let’s clarify why it’s important in some cases and whether it applies here.</p> <h3 id="understanding-the-role-of-dimension--d-">Understanding the Role of Dimension ( d )</h3> <p>In block matrix multiplication:</p> <ol> <li><strong>Matrix Multiplication Basics</strong>: <ul> <li>To compute an element ((i, j)) of the resulting matrix ( QK_t ), you need to calculate the dot product of the (i)-th row of (Q) and the (j)-th row of (K).</li> <li>This involves summing the product of corresponding elements from both rows over the shared dimension ( d ):<br/> [ QK_t[i, j] = \sum_{k=0}^{d-1} Q[i, k] \times K[j, k] ]</li> </ul> </li> <li><strong>Tiling</strong>: <ul> <li>In tiled matrix multiplication, the computation of a block of ( QK_t ) (e.g., a ( \text{TILE_SIZE} \times \text{TILE_SIZE} ) block) is split into smaller sub-blocks to improve memory locality.</li> <li>Each sub-block computation iterates over the shared dimension ( d ) in chunks.</li> </ul> </li> </ol> <h3 id="why-traverse--d-">Why Traverse ( d )?</h3> <p>If you don’t traverse ( d ), the computation will only consider partial contributions to the dot product. This would leave the values in ( QK_t ) incomplete.</p> <p>For example, if ( d = 6 ), and your block size is ( \text{TILE_SIZE} = 3 ):</p> <ul> <li>You compute the dot product for ( d \in [0, 2] ), but ignore ( d \in [3, 5] ), leading to incorrect results.</li> </ul> <h3 id="why-the-code-needs-it">Why the Code Needs It</h3> <p>Your original code suggests you’re updating ( QK_t ) incrementally:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">q_tile_val</span> <span class="o">*</span> <span class="n">k_tile_val</span> <span class="o">+</span> <span class="n">orig_val</span><span class="p">;</span>
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">orig_val</code> accumulates partial results. Without iterating fully over ( d ), the result will be incomplete because only a subset of the dot product contributions is added.</p> <h3 id="when--d--traversal-isnt-necessary">When ( d ) Traversal Isn’t Necessary</h3> <p>In rare cases, you might not need to traverse ( d ) if:</p> <ol> <li><strong>Each block fully covers ( d )</strong>: <ul> <li>This is possible only when ( \text{TILE_SIZE} ) equals ( d ).</li> <li>In this case, the tiling logic aligns perfectly with the shared dimension, and a single traversal suffices.</li> </ul> </li> <li><strong>Precomputed Values</strong>: <ul> <li>If ( Q ) or ( K ) has been preprocessed (e.g., multiplied previously), the shared dimension ( d ) might not need iteration.</li> </ul> </li> </ol> <h3 id="conclusion">Conclusion</h3> <p>For the general case (and your code):</p> <ul> <li>Traversing ( d ) fully ensures the correctness of the dot product computation across the shared dimension.</li> <li>If you’re sure ( d ) traversal isn’t necessary, it usually means ( Q ) and ( K ) are already structured to encapsulate the full dot product for each tile.</li> </ul> <p>If you’re still unsure about why it’s needed in your case, we can examine your problem more closely to confirm!</p> <p>I can view the outside for loop from two kind of perspective.</p> <p>One is from input matrix.</p> <p>One is from output matrix. This one makes more sense after reading code generated by gpt</p> <p>Two outer for loop traverse row and column tiles of output matrix. The third for loop traverse the row in input matrix and the column in second input matrix The fourth and fifth for loop traverse the row and column elements in current tile. The sixth for loop does dot product for a single row in input tile and single column in output tile and accumulate the sum. And then write back the sum to output matrix.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for (int row_tile_idx = 0; row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; row_tile_idx++) {
    for (int col_tile_idx = 0; col_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; col_tile_idx++) {
        // Initialize the current block of the output matrix
        for (int i = 0; i &lt; TILE_SIZE; i++) {
            for (int j = 0; j &lt; TILE_SIZE; j++) {
                int row_idx = row_tile_idx * TILE_SIZE + i;
                int col_idx = col_tile_idx * TILE_SIZE + j;
                if (row_idx &lt; N &amp;&amp; col_idx &lt; N) {
                    twoDimWrite(C, row_idx, col_idx, N, 0.0); // Initialize to zero
                }
            }
        }

        for (int k_tile_idx = 0; k_tile_idx &lt; (d + TILE_SIZE - 1) / TILE_SIZE; k_tile_idx++) {
            for (int i = 0; i &lt; TILE_SIZE; i++) {
                for (int j = 0; j &lt; TILE_SIZE; j++) {
                    int row_idx = row_tile_idx * TILE_SIZE + i;
                    int col_idx = col_tile_idx * TILE_SIZE + j;
                    if (row_idx &gt;= N || col_idx &gt;= N) continue;

                    float sum = twoDimRead(C, row_idx, col_idx, N);

                    for (int k = 0; k &lt; TILE_SIZE; k++) {
                        int k_idx = k_tile_idx * TILE_SIZE + k;
                        if (k_idx &gt;= d) break;

                        float a_val = twoDimRead(A, row_idx, k_idx, d);
                        float b_val = twoDimRead(B, col_idx, k_idx, d); // Column index now indexes rows in B
                        sum += a_val * b_val;
                    }

                    twoDimWrite(C, row_idx, col_idx, N, sum);
                }
            }
        }
    }
}

</code></pre></div></div> <p>The code above use global buffer.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      for(int row_tile_idx=0; row_tile_idx &lt; (N+TILE_SIZE-1)/TILE_SIZE; row_tile_idx++) {
        for(int col_tile_idx=0; col_tile_idx &lt; (N+TILE_SIZE-1)/TILE_SIZE; col_tile_idx++) {
          for(int k_tile_idx=0; k_tile_idx &lt; (d+TILE_SIZE-1)/TILE_SIZE; k_tile_idx++  ) {
            for(int tile_row_idx=0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {
              for(int tile_col_idx=0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                if(row_idx &gt;= N || col_idx &gt;= N) {
                  continue;
                }
                float sum = twoDimRead(QK_t, row_idx, col_idx, N);

                for(int k=0; k &lt; TILE_SIZE; k++) {
                  int k_idx = k_tile_idx * TILE_SIZE + k;
                  if(k_idx &gt;= d) break;
                  float q_val =  fourDimRead(Q,b, h, row_idx, k_idx, H, N, d);
                  float k_val = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);
                  sum += q_val * k_val;
                }
                twoDimWrite(QK_t, row_idx, col_idx, N, sum);
              }

            }
          }
        }
      }
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  238.938ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:39:56 1318038:1318038 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:39:56 1318038:1318038 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 13:39:56 1318038:1318038 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.29826903343200684

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.01%      21.000us         0.01%      21.000us       2.625us       5.00 Mb       5.00 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.35%     296.386ms        99.70%     297.443ms     297.443ms       4.50 Mb      -1.00 Mb             1
                                   aten::zeros         0.01%      15.000us         0.24%     706.000us     353.000us       4.50 Mb           0 b             2
                                   aten::clone         0.01%      19.000us         0.11%     328.000us     164.000us       1.00 Mb           0 b             2
                           aten::empty_strided         0.00%      11.000us         0.00%      11.000us       2.200us     512.51 Kb     512.51 Kb             5
                               model_inference         0.18%     550.000us       100.00%     298.329ms     298.329ms     512.00 Kb      -4.00 Mb             1
                                 aten::flatten         0.01%      15.000us         0.07%     200.000us      40.000us     512.00 Kb           0 b             5
                              aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                                      aten::to         0.00%       6.000us         0.01%      31.000us       5.167us         520 b           0 b             6
                                aten::_to_copy         0.00%      14.000us         0.01%      25.000us       6.250us         520 b           0 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 298.329ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  297.443ms
mem usage:  4718592 bytes
</code></pre></div></div> <p>This code use local buffer that is allocated. A little bit faster but not a lot.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#pragma omp parallel for collapse(2) // Parallelize the two outermost loops
for (int row_tile_idx = 0; row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; row_tile_idx++) {
    for (int col_tile_idx = 0; col_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; col_tile_idx++) {
        for (int k_tile_idx = 0; k_tile_idx &lt; (d + TILE_SIZE - 1) / TILE_SIZE; k_tile_idx++) {

            // Buffers for tile data
            float Q_tile[TILE_SIZE][TILE_SIZE];
            float K_tile[TILE_SIZE][TILE_SIZE];

            // Preload Q and K tiles into local buffers
            for (int tile_row_idx = 0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                if (row_idx &gt;= N) continue; // Skip out-of-bound rows

                for (int k = 0; k &lt; TILE_SIZE; k++) {
                    int k_idx = k_tile_idx * TILE_SIZE + k;
                    if (k_idx &lt; d) {
                        Q_tile[tile_row_idx][k] = fourDimRead(Q, b, h, row_idx, k_idx, H, N, d);
                    } else {
                        Q_tile[tile_row_idx][k] = 0.0f; // Fill with zero if out-of-bounds
                    }
                }
            }

            for (int tile_col_idx = 0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {
                int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                if (col_idx &gt;= N) continue; // Skip out-of-bound columns

                for (int k = 0; k &lt; TILE_SIZE; k++) {
                    int k_idx = k_tile_idx * TILE_SIZE + k;
                    if (k_idx &lt; d) {
                        K_tile[tile_col_idx][k] = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);
                    } else {
                        K_tile[tile_col_idx][k] = 0.0f; // Fill with zero if out-of-bounds
                    }
                }
            }

            // Compute the dot product for the current tile
            for (int tile_row_idx = 0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                if (row_idx &gt;= N) continue; // Skip out-of-bound rows

                for (int tile_col_idx = 0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {
                    int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                    if (col_idx &gt;= N) continue; // Skip out-of-bound columns

                    float sum = twoDimRead(QK_t, row_idx, col_idx, N);

                    // Unrolled loop for vectorized dot product
                    for (int k = 0; k &lt; TILE_SIZE; k++) {
                        sum += Q_tile[tile_row_idx][k] * K_tile[tile_col_idx][k];
                    }

                    twoDimWrite(QK_t, row_idx, col_idx, N, sum);
                }
            }
        }
    }
}

</code></pre></div></div> <p>OUtput:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  226.667ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:44:02 1342423:1342423 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:44:02 1342423:1342423 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 13:44:02 1342423:1342423 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.2852001190185547

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.00%      13.000us         0.00%      13.000us       1.625us       5.00 Mb       5.00 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.52%     283.895ms        99.68%     284.350ms     284.350ms       4.50 Mb      -1.00 Mb             1
                                   aten::zeros         0.01%      17.000us         0.05%     131.000us      65.500us       4.50 Mb           0 b             2
                                   aten::clone         0.01%      23.000us         0.11%     300.000us     150.000us       1.00 Mb           0 b             2
                           aten::empty_strided         0.00%      12.000us         0.00%      12.000us       2.400us     512.26 Kb     512.26 Kb             5
                               model_inference         0.20%     565.000us       100.00%     285.254ms     285.254ms     512.00 Kb      -4.00 Mb             1
                                 aten::flatten         0.01%      16.000us         0.06%     159.000us      31.800us     512.00 Kb           0 b             5
                              aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                                      aten::to         0.00%      10.000us         0.01%      31.000us       5.167us         520 b         256 b             6
                                aten::_to_copy         0.01%      16.000us         0.01%      26.000us       6.500us         520 b         256 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 285.254ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  284.35ms
mem usage:  4718592 bytes
</code></pre></div></div> <p>Code that do block matrix multiplication for both (Q * K_t) and (softmax(QK_t ) * V):</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define TILE_SIZE 16
</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myUnfusedAttentionBlocked</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QK_tTensor</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>
  
  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
  <span class="c1">//QK^t Intermediate Tensor has Shape (N, N)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d) </span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

  <span class="c1">//Format O, Q, K, and V tensors into 4D vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>

  <span class="c1">//Format QK_t Tensor into a 2D vector.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">QK_t</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QK_tTensor</span><span class="p">);</span>

  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

      <span class="c1">//</span>
      <span class="c1">// correct</span>
      <span class="c1">// for(int row_tile_idx=0; row_tile_idx &lt; (N+TILE_SIZE-1)/TILE_SIZE; row_tile_idx++) {</span>
      <span class="c1">//   for(int col_tile_idx=0; col_tile_idx &lt; (N+TILE_SIZE-1)/TILE_SIZE; col_tile_idx++) {</span>
      <span class="c1">//     for(int k_tile_idx=0; k_tile_idx &lt; (d+TILE_SIZE-1)/TILE_SIZE; k_tile_idx++  ) {</span>
      <span class="c1">//       for(int tile_row_idx=0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {</span>
      <span class="c1">//         for(int tile_col_idx=0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {</span>
      <span class="c1">//           int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;</span>
      <span class="c1">//           int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;</span>
      <span class="c1">//           if(row_idx &gt;= N || col_idx &gt;= N) {</span>
      <span class="c1">//             continue;</span>
      <span class="c1">//           }</span>
      <span class="c1">//           float sum = twoDimRead(QK_t, row_idx, col_idx, N);</span>

      <span class="c1">//           for(int k=0; k &lt; TILE_SIZE; k++) {</span>
      <span class="c1">//             int k_idx = k_tile_idx * TILE_SIZE + k;</span>
      <span class="c1">//             if(k_idx &gt;= d) break;</span>
      <span class="c1">//             float q_val =  fourDimRead(Q,b, h, row_idx, k_idx, H, N, d);</span>
      <span class="c1">//             float k_val = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);</span>
      <span class="c1">//             sum += q_val * k_val;</span>
      <span class="c1">//           }</span>
      <span class="c1">//           twoDimWrite(QK_t, row_idx, col_idx, N, sum);</span>
      <span class="c1">//         }</span>

      <span class="c1">//       }</span>
      <span class="c1">//     }</span>
      <span class="c1">//   }</span>
      <span class="c1">// }</span>

      <span class="c1">// correct with local buffer</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">row_tile_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">TILE_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">row_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">col_tile_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">col_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">TILE_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">col_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k_tile_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="n">TILE_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

            <span class="c1">// Buffers for tile data</span>
            <span class="kt">float</span> <span class="n">Q_tile</span><span class="p">[</span><span class="n">TILE_SIZE</span><span class="p">][</span><span class="n">TILE_SIZE</span><span class="p">];</span>
            <span class="kt">float</span> <span class="n">K_tile</span><span class="p">[</span><span class="n">TILE_SIZE</span><span class="p">][</span><span class="n">TILE_SIZE</span><span class="p">];</span>

            <span class="c1">// Preload Q and K tiles into local buffers</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip out-of-bound rows</span>

                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                    <span class="kt">int</span> <span class="n">k_idx</span> <span class="o">=</span> <span class="n">k_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">k</span><span class="p">;</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">k_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">Q_tile</span><span class="p">[</span><span class="n">tile_row_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                        <span class="n">Q_tile</span><span class="p">[</span><span class="n">tile_row_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">// Fill with zero if out-of-bounds</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>

            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">col_idx</span> <span class="o">=</span> <span class="n">col_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">col_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip out-of-bound columns</span>

                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                    <span class="kt">int</span> <span class="n">k_idx</span> <span class="o">=</span> <span class="n">k_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">k</span><span class="p">;</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">k_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">K_tile</span><span class="p">[</span><span class="n">tile_col_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                        <span class="n">K_tile</span><span class="p">[</span><span class="n">tile_col_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">// Fill with zero if out-of-bounds</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>

            <span class="c1">// Compute the dot product for the current tile</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip out-of-bound rows</span>

                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                    <span class="kt">int</span> <span class="n">col_idx</span> <span class="o">=</span> <span class="n">col_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">col_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip out-of-bound columns</span>

                    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>

                    <span class="c1">// Unrolled loop for vectorized dot product</span>
                    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">sum</span> <span class="o">+=</span> <span class="n">Q_tile</span><span class="p">[</span><span class="n">tile_row_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">K_tile</span><span class="p">[</span><span class="n">tile_col_idx</span><span class="p">][</span><span class="n">k</span><span class="p">];</span>
                    <span class="p">}</span>

                    <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>



      <span class="c1">// also correct</span>
<span class="c1">//       for (int q_row_tile_idx = 0; q_row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; q_row_tile_idx++) {</span>
<span class="c1">//     for (int k_row_tile_idx = 0; k_row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; k_row_tile_idx++) {</span>
<span class="c1">//         for (int d_col_tile_idx = 0; d_col_tile_idx &lt; (d + TILE_SIZE - 1) / TILE_SIZE; d_col_tile_idx++) {</span>
<span class="c1">//             for (int tile_row_idx = 0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {</span>
<span class="c1">//                 for (int tile_col_idx = 0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {</span>
<span class="c1">//                     int q_row_idx = q_row_tile_idx * TILE_SIZE + tile_row_idx;</span>
<span class="c1">//                     int k_row_idx = k_row_tile_idx * TILE_SIZE + tile_col_idx; // Fix indexing</span>
<span class="c1">//                     for (int d_idx = d_col_tile_idx * TILE_SIZE; d_idx &lt; (d_col_tile_idx + 1) * TILE_SIZE; d_idx++) {</span>
<span class="c1">//                         if (q_row_idx &lt; N &amp;&amp; k_row_idx &lt; N &amp;&amp; d_idx &lt; d) {</span>
<span class="c1">//                             float q_tile_val = fourDimRead(Q, b, h, q_row_idx, d_idx, H, N, d);</span>
<span class="c1">//                             float k_tile_val = fourDimRead(K, b, h, k_row_idx, d_idx, H, N, d);</span>
<span class="c1">//                             float orig_val = twoDimRead(QK_t, q_row_idx, k_row_idx, N);</span>
<span class="c1">//                             float val = q_tile_val * k_tile_val + orig_val;</span>
<span class="c1">//                             twoDimWrite(QK_t, q_row_idx, k_row_idx, N, val);</span>
<span class="c1">//                         }</span>
<span class="c1">//                     }</span>
<span class="c1">//                 }</span>
<span class="c1">//             }</span>
<span class="c1">//         }</span>
<span class="c1">//     }</span>
<span class="c1">// }</span>




      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">);</span>
        <span class="kt">double</span> <span class="n">row_sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span><span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
           <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
          <span class="kt">double</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
          <span class="n">row_sum</span> <span class="o">+=</span> <span class="n">exp_val</span><span class="p">;</span>
          <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp_val</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">row_sum</span><span class="p">;</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">prob</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>


      
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">qkt_row_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">qkt_row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">qkt_row_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_d_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">output_d_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">output_d_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">out_row_idx</span> <span class="o">=</span> <span class="n">qkt_row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
              <span class="k">if</span><span class="p">(</span><span class="n">out_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
              <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">out_col_idx</span> <span class="o">=</span> <span class="n">output_d_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="k">if</span><span class="p">(</span> <span class="n">out_col_idx</span> <span class="o">&gt;=</span> <span class="n">d</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>

                <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="p">);</span>
                <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">k_idx</span> <span class="o">=</span> <span class="n">k_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">k</span><span class="p">;</span>
                  <span class="kt">float</span> <span class="n">qkt_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="n">sum</span> <span class="o">+=</span> <span class="n">qkt_val</span> <span class="o">*</span> <span class="n">v_val</span><span class="p">;</span> 
                <span class="p">}</span>
                <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
              <span class="p">}</span>
            <span class="p">}</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">}</span>


    <span class="p">}</span>

  <span class="p">}</span>


  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>

</code></pre></div></div> <p>Output:</p> <p>Achieve the same cpu time as ref solution.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Self CPU time total: 215.420ms

REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  214.494ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 20:06:22 3461555:3461555 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 20:06:23 3461555:3461555 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 20:06:23 3461555:3461555 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.2183218002319336

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.01%      12.000us         0.01%      12.000us       1.500us       5.00 Mb       5.00 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.38%     217.015ms        99.61%     217.504ms     217.504ms       4.50 Mb      -1.00 Mb             1
                                   aten::zeros         0.01%      17.000us         0.07%     142.000us      71.000us       4.50 Mb           0 b             2
                                   aten::clone         0.01%      19.000us         0.14%     313.000us     156.500us       1.00 Mb           0 b             2
                           aten::empty_strided         0.00%       9.000us         0.00%       9.000us       1.800us     512.25 Kb     512.25 Kb             5
                               model_inference         0.24%     532.000us       100.00%     218.359ms     218.359ms     512.00 Kb      -4.00 Mb             1
                                 aten::flatten         0.01%      25.000us         0.09%     189.000us      37.800us     512.00 Kb           0 b             5
                              aten::empty_like         0.00%       4.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                                      aten::to         0.00%       9.000us         0.01%      30.000us       5.000us         520 b           4 b             6
                                aten::_to_copy         0.01%      14.000us         0.01%      25.000us       6.250us         520 b         260 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 218.359ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  217.504ms
mem usage:  4718592 bytes
</code></pre></div></div> <p>Another issue of code above.</p> <p>It’s not correct when output matrix size is not power of 2</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(myenv) ➜  cs149gpt git:(main) python3 gpt149.py part2 -N 103
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0077, 0.0085, 0.0093, 0.0101, 0.0109, 0.0117, 0.0125, 0.0133, 0.0141,
        0.0149, 0.0157, 0.0165, 0.0173, 0.0181, 0.0189, 0.0197, 0.0205, 0.0213,
        0.0221, 0.0229, 0.0237, 0.0245, 0.0253, 0.0261, 0.0269, 0.0277, 0.0285,
        0.0293, 0.0301, 0.0309, 0.0317, 0.0325])
STAGE:2024-11-18 23:12:28 271968:271968 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0077, 0.0086, 0.0094, 0.0103, 0.0112, 0.0121, 0.0129, 0.0138, 0.0147,
        0.0155, 0.0164, 0.0173, 0.0181, 0.0190, 0.0199, 0.0208, 0.0216, 0.0225,
        0.0234, 0.0242, 0.0251, 0.0260, 0.0268, 0.0277, 0.0286, 0.0295, 0.0303,
        0.0312, 0.0321, 0.0329, 0.0338, 0.0347])
STAGE:2024-11-18 23:12:28 271968:271968 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 23:12:28 271968:271968 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 313, in main
    part2Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 233, in part2Test
    testTemplate(attentionModuleReference.myUnfusedAttentionBlocked, params, "STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Found the root cause. It’s because there is out of bound check for softmax(QK_t) * V</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">qkt_row_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">qkt_row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">qkt_row_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_d_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">output_d_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">output_d_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">out_row_idx</span> <span class="o">=</span> <span class="n">qkt_row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
              <span class="k">if</span><span class="p">(</span><span class="n">out_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
              <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">out_col_idx</span> <span class="o">=</span> <span class="n">output_d_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="k">if</span><span class="p">(</span> <span class="n">out_col_idx</span> <span class="o">&gt;=</span> <span class="n">d</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>

                <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="p">);</span>
                <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">k_idx</span> <span class="o">=</span> <span class="n">k_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">k</span><span class="p">;</span>
                <span class="c1">// Needto add this out of bound check</span>
                  <span class="k">if</span><span class="p">(</span><span class="n">k_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">break</span><span class="p">;</span>
                  <span class="kt">float</span> <span class="n">qkt_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="n">sum</span> <span class="o">+=</span> <span class="n">qkt_val</span> <span class="o">*</span> <span class="n">v_val</span><span class="p">;</span> 
                <span class="p">}</span>
                <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
              <span class="p">}</span>
            <span class="p">}</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">}</span>
</code></pre></div></div> <p>Now the result is correct when N is not power of 2. The cpu time is higher than aht of ref solution. I think this is because I add this out of bound check ?</p> <p>Output:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>myenv<span class="o">)</span> ➜  cs149gpt git:<span class="o">(</span>main<span class="o">)</span> ✗ python3 gpt149.py part2 <span class="nt">-N</span> 1023
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Self CPU time total: 223.154ms

REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  222.105ms
mem usage:  4709892 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0770, 0.0778, 0.0786, 0.0794, 0.0802, 0.0810, 0.0818, 0.0826, 0.0834,
        0.0842, 0.0850, 0.0858, 0.0866, 0.0874, 0.0882, 0.0890, 0.0898, 0.0906,
        0.0914, 0.0922, 0.0930, 0.0938, 0.0946, 0.0954, 0.0962, 0.0970, 0.0978,
        0.0986, 0.0994, 0.1002, 0.1010, 0.1018])
STAGE:2024-11-19 10:48:32 4016683:4016683 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0770, 0.0778, 0.0786, 0.0794, 0.0802, 0.0810, 0.0818, 0.0826, 0.0834,
        0.0842, 0.0850, 0.0858, 0.0866, 0.0874, 0.0882, 0.0890, 0.0898, 0.0906,
        0.0914, 0.0922, 0.0930, 0.0938, 0.0946, 0.0954, 0.0962, 0.0970, 0.0978,
        0.0986, 0.0994, 0.1002, 0.1010, 0.1018])
STAGE:2024-11-19 10:48:32 4016683:4016683 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-19 10:48:32 4016683:4016683 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.23152732849121094

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.01%      13.000us         0.01%      13.000us       1.625us       4.99 Mb       4.99 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.41%     230.208ms        99.62%     230.695ms     230.695ms       4.49 Mb   -1023.00 Kb             1
                                   aten::zeros         0.01%      32.000us         0.06%     145.000us      72.500us       4.49 Mb           0 b             2
                                   aten::clone         0.01%      18.000us         0.14%     318.000us     159.000us    1023.00 Kb           0 b             2
                           aten::empty_strided         0.00%       9.000us         0.00%       9.000us       1.800us     511.75 Kb     511.75 Kb             5
                               model_inference         0.24%     546.000us       100.00%     231.569ms     231.569ms     511.50 Kb      -3.99 Mb             1
                                 aten::flatten         0.01%      16.000us         0.07%     170.000us      34.000us     511.50 Kb           0 b             5
                              aten::empty_like         0.00%       4.000us         0.00%       6.000us       6.000us     511.50 Kb           0 b             1
                                      aten::to         0.00%       6.000us         0.01%      30.000us       5.000us         552 b          32 b             6
                                aten::_to_copy         0.01%      15.000us         0.01%      24.000us       6.000us         520 b         260 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 231.569ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  230.695ms
mem usage:  4709892 bytes
</code></pre></div></div> <h2 id="part3-fused-attention">Part3: Fused attention</h2> <p>Problematic implementation: Output is all zero. Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myFusedAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">temp</span><span class="p">,</span>
              <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>

  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d)</span>
  <span class="c1">//and O Row Tensor with Shape (N)</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">ORowTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">N</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

  <span class="c1">//Format Y, Q, K, and V tensors into 4D vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>
  
  <span class="c1">//Format ORow Tensor into a 1D vector</span>
  <span class="c1">// You can simply access this as ORow[i]</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">ORow</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">ORowTensor</span><span class="p">);</span>


  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="c1">// We give you a template of the first three loops for your convenience</span>
  <span class="c1">//loop over batch</span>

  <span class="cp">#pragma omp parallel for collapse(3)
</span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">){</span>
    <span class="c1">//loop over heads</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">){</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">q_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">q_row_idx</span><span class="o">++</span><span class="p">){</span>

  <span class="c1">// YRow is moved inside so each OpenMP thread gets a local copy.</span>
            <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">ORowTensor</span> <span class="o">=</span> <span class="n">temp</span><span class="p">.</span><span class="n">index</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">Slice</span><span class="p">(</span><span class="n">omp_get_thread_num</span><span class="p">(),</span> <span class="n">torch</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">None</span><span class="p">)});</span>      
            <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">ORow</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">ORowTensor</span><span class="p">);</span>
  <span class="c1">//YOUR CODE HERE</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">k_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span> <span class="p">;</span><span class="n">d_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">q_row</span> <span class="o">=</span> <span class="n">q_row_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">k_row_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="p">;</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">k_row_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>

        <span class="p">}</span>
        <span class="c1">// softmax</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">);</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">;</span>
          <span class="n">sum</span> <span class="o">+=</span> <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
          <span class="c1">// tmp_row_res[i] = exp_val;</span>
        <span class="p">}</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">/</span>  <span class="n">sum</span><span class="p">;</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">prob</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">v_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">v_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span> <span class="p">,</span><span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">v_val</span> <span class="o">*</span> <span class="n">ORow</span><span class="p">[</span><span class="n">v_row_idx</span><span class="p">];</span>
          <span class="p">}</span>
          <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"vcold_idx"</span> <span class="o">&lt;&lt;</span> <span class="n">v_col_idx</span> <span class="o">&lt;&lt;</span> <span class="s">"val: "</span> <span class="o">&lt;&lt;</span> <span class="n">sum</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
          <span class="c1">// tmp_row_res[v_col_idx] = sum;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
        <span class="p">}</span>
          
        
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
    

  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE - FUSED ATTENTION statistics
cpu time:  56.506ms
mem usage:  557056 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 11:53:42 171611:171611 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])
STAGE:2024-11-19 11:53:42 171611:171611 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-19 11:53:42 171611:171611 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 315, in main
    part3Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 245, in part3Test
    testTemplate(attentionModuleReference.myFusedAttention, params, "STUDENT - FUSED ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Found the root cause . It’s becaue of wrong type definition.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">v_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">v_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// This is the correct definition</span>
            <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span> <span class="p">,</span><span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">v_val</span> <span class="o">*</span> <span class="n">ORow</span><span class="p">[</span><span class="n">v_row_idx</span><span class="p">];</span>
          <span class="p">}</span>
          <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"vcold_idx"</span> <span class="o">&lt;&lt;</span> <span class="n">v_col_idx</span> <span class="o">&lt;&lt;</span> <span class="s">"val: "</span> <span class="o">&lt;&lt;</span> <span class="n">sum</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
        <span class="p">}</span>
</code></pre></div></div> <p>Output:</p> <p>It’s too slow. I don’t know why</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE - FUSED ATTENTION statistics
cpu time:  59.932ms
mem usage:  557056 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 13:07:43 578930:578930 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 13:07:43 578930:578930 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-19 13:07:43 578930:578930 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.3012988567352295

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::empty         0.01%      17.000us         0.01%      17.000us       1.889us       1.04 Mb       1.04 Mb             9
                  aten::clone         0.00%      15.000us         0.08%     247.000us     123.500us       1.00 Mb           0 b             2
                  aten::zeros         0.01%      19.000us         0.02%      65.000us      21.667us     548.00 Kb           0 b             3
    STUDENT - FUSED ATTENTION        95.37%     287.422ms        99.68%     300.382ms     300.382ms     544.00 Kb      -1.00 Mb             1
          aten::empty_strided         0.00%       9.000us         0.00%       9.000us       1.800us     512.00 Kb     512.00 Kb             5
              model_inference         0.21%     636.000us       100.00%     301.361ms     301.361ms     512.00 Kb     -32.63 Kb             1
                aten::flatten         1.42%       4.278ms         1.48%       4.467ms       1.089us     512.00 Kb           0 b          4101
             aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                     aten::to         0.00%       6.000us         0.01%      32.000us       5.333us         520 b           0 b             6
               aten::_to_copy         0.01%      18.000us         0.01%      26.000us       6.500us         520 b         516 b             4
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 301.361ms

STUDENT - FUSED ATTENTION statistics
cpu time:  300.382ms
mem usage:  557056 bytes
</code></pre></div></div> <p>Found the root cause. I don’t enable openmp in original code</p> <p>The speed is comparable after enabling openmp Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myFusedAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">temp</span><span class="p">,</span>
              <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>

  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d)</span>
  <span class="c1">//and O Row Tensor with Shape (N)</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">ORowTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">N</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

  <span class="c1">//Format Y, Q, K, and V tensors into 4D vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>
  
  <span class="c1">//Format ORow Tensor into a 1D vector</span>
  <span class="c1">// You can simply access this as ORow[i]</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">ORow</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">ORowTensor</span><span class="p">);</span>


  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="c1">// We give you a template of the first three loops for your convenience</span>
  <span class="c1">//loop over batch</span>

<span class="c1">// Need to enable this openmp for parallel execution for all rows in batches and heads.</span>
  <span class="cp">#pragma omp parallel for collapse(3)
</span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">){</span>
    <span class="c1">//loop over heads</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">){</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">q_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">q_row_idx</span><span class="o">++</span><span class="p">){</span>

  <span class="c1">// YRow is moved inside so each OpenMP thread gets a local copy.</span>
            <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">ORowTensor</span> <span class="o">=</span> <span class="n">temp</span><span class="p">.</span><span class="n">index</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">Slice</span><span class="p">(</span><span class="n">omp_get_thread_num</span><span class="p">(),</span> <span class="n">torch</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">None</span><span class="p">)});</span>      
            <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">ORow</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">ORowTensor</span><span class="p">);</span>
  <span class="c1">//YOUR CODE HERE</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">k_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span> <span class="p">;</span><span class="n">d_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">q_row</span> <span class="o">=</span> <span class="n">q_row_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">k_row_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="p">;</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">k_row_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
          <span class="c1">// std::cout &lt;&lt; "krowidx: " &lt;&lt; k_row_idx &lt;&lt; " val: " &lt;&lt; ORow[k_row_idx] &lt;&lt; std::endl;</span>

        <span class="p">}</span>
        <span class="c1">// softmax</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">);</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">;</span>
          <span class="n">sum</span> <span class="o">+=</span> <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
          <span class="c1">// tmp_row_res[i] = exp_val;</span>
        <span class="p">}</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">/</span>  <span class="n">sum</span><span class="p">;</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">prob</span><span class="p">;</span>
          <span class="c1">// std::cout &lt;&lt; "softmax col: "  &lt;&lt; i &lt;&lt; " val: " &lt;&lt; ORow[i] &lt;&lt; std::endl;</span>
        <span class="p">}</span>

        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">v_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">v_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span> <span class="p">,</span><span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">v_val</span> <span class="o">*</span> <span class="n">ORow</span><span class="p">[</span><span class="n">v_row_idx</span><span class="p">];</span>
          <span class="p">}</span>
          <span class="c1">// std::cout &lt;&lt; "vcold_idx" &lt;&lt; v_col_idx &lt;&lt; "val: " &lt;&lt; sum &lt;&lt; std::endl;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
        <span class="p">}</span>
          
        
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
    

  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE - FUSED ATTENTION statistics
cpu time:  56.526ms
mem usage:  557056 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 17:17:41 1945272:1945272 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 17:17:41 1945272:1945272 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-19 17:17:41 1945272:1945272 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.04986262321472168

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::empty         0.02%      12.000us         0.02%      12.000us       1.333us       1.04 Mb       1.04 Mb             9
                  aten::clone         0.04%      19.000us         0.42%     208.000us     104.000us       1.00 Mb           0 b             2
                  aten::zeros         0.04%      19.000us         0.14%      68.000us      22.667us     548.00 Kb           0 b             3
    STUDENT - FUSED ATTENTION        94.35%      47.083ms        98.27%      49.043ms      49.043ms     544.00 Kb      -1.00 Mb             1
          aten::empty_strided         0.02%       8.000us         0.02%       8.000us       1.600us     512.00 Kb     512.00 Kb             5
              model_inference         1.11%     555.000us       100.00%      49.904ms      49.904ms     512.00 Kb     -32.63 Kb             1
                aten::flatten         1.10%     549.000us         1.50%     748.000us       1.447us     512.00 Kb           0 b           517
             aten::empty_like         0.01%       3.000us         0.01%       4.000us       4.000us     512.00 Kb           0 b             1
                     aten::to         0.01%       6.000us         0.06%      29.000us       4.833us         520 b           0 b             6
               aten::_to_copy         0.03%      14.000us         0.05%      23.000us       5.750us         520 b         260 b             4
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 49.904ms

STUDENT - FUSED ATTENTION statistics
cpu time:  49.043ms
mem usage:  557056 bytes
</code></pre></div></div> <h2 id="part4-flash-attention">Part4: Flash attention</h2> <p>gpt gives good explanation about how Oi forumala is obtained.</p> <p>Basically previous O_i is divided by l_i . So we need to multiply O_i by l_i when calculating new O_i with new l_i according to this expression.</p> <p>[ O_i = \frac{\sum_{\text{old } j} \alpha_{ij} V_j + \sum_{\text{new } j} \alpha_{ij} V_j}{\sum_{\text{old } j} \alpha_{ij} + \sum_{\text{new } j} \alpha_{ij}}. ]</p> <p>The formula</p> <p>[ O_i = \frac{l_i O_i + P_{ij} V_j}{l_{\text{new}}} ]</p> <p>and its update mechanism for the cumulative sum ( l_i O_i ) and normalization factor ( l_i ) works because of the <strong>linearity of the softmax-weighted sum</strong> and the <strong>incremental construction of softmax normalization</strong>. Let’s derive and explain why this works step-by-step.</p> <hr/> <h3 id="1-standard-attention-mechanism">1. <strong>Standard Attention Mechanism</strong></h3> <p>In the standard attention mechanism, the output ( O_i ) for query ( Q_i ) is defined as:</p> <p>[ O_i = \frac{\sum_{j=1}^N \alpha_{ij} V_j}{\sum_{j=1}^N \alpha_{ij}}, ]</p> <p>where:</p> <ul> <li>( \alpha_{ij} = \exp(Q_i K_j^T) ): the unnormalized attention score between query ( Q_i ) and key ( K_j ).</li> <li>( P_{ij} = \frac{\alpha_{ij}}{\sum_{k=1}^N \alpha_{ik}} ): the normalized attention score (softmax).</li> <li>( V_j ): the value vector associated with key ( K_j ).</li> </ul> <p>Rewriting ( O_i ) explicitly:</p> <p>[ O_i = \frac{\sum_{j=1}^N \alpha_{ij} V_j}{Z_i}, ]</p> <p>where ( Z_i = \sum_{j=1}^N \alpha_{ij} ) is the normalization constant for row ( i ).</p> <hr/> <h3 id="2-chunked-computation-in-flash-attention">2. <strong>Chunked Computation in Flash Attention</strong></h3> <p>To handle memory constraints, Flash Attention processes the attention computation in chunks of ( j ). Suppose we process a chunk of keys/values indexed by ( j ) in each step. Let:</p> <ul> <li>( l_i^{\text{old}} ) be the normalization constant ( Z_i ) for the previously processed chunks.</li> <li>( O_i^{\text{old}} ) be the partial output for the previously processed chunks.</li> </ul> <p>The goal is to update ( O_i ) and ( l_i ) incrementally when a new chunk is processed.</p> <p>For the new chunk, compute:</p> <ul> <li>( \alpha_{ij} ) for the current chunk.</li> <li>The sum of unnormalized scores for the chunk: ( l_{\text{chunk}} = \sum_{j \in \text{chunk}} \alpha_{ij} ).</li> <li>The weighted value contribution: ( O_{\text{chunk}} = \sum_{j \in \text{chunk}} \alpha_{ij} V_j ).</li> </ul> <p>The updated values are:</p> <p>[ l_i^{\text{new}} = l_i^{\text{old}} + l_{\text{chunk}} ] [ O_i^{\text{new}} = \frac{l_i^{\text{old}} O_i^{\text{old}} + O_{\text{chunk}}}{l_i^{\text{new}}}. ]</p> <hr/> <h3 id="3-why-this-works-derivation">3. <strong>Why This Works: Derivation</strong></h3> <h4 id="a-weighted-combination-of-outputs"><strong>a. Weighted Combination of Outputs</strong></h4> <p>The full output ( O_i ) over all chunks can be expressed as a weighted combination of the contributions from the old and new chunks:</p> <p>[ O_i = \frac{\sum_{\text{all } j} \alpha_{ij} V_j}{\sum_{\text{all } j} \alpha_{ij}}. ]</p> <p>Breaking this into previously processed chunks (old) and the current chunk:</p> <p>[ O_i = \frac{\sum_{\text{old } j} \alpha_{ij} V_j + \sum_{\text{new } j} \alpha_{ij} V_j}{\sum_{\text{old } j} \alpha_{ij} + \sum_{\text{new } j} \alpha_{ij}}. ]</p> <p>Let:</p> <ul> <li>( l_i^{\text{old}} = \sum_{\text{old } j} \alpha_{ij} ),</li> <li>( l_{\text{chunk}} = \sum_{\text{new } j} \alpha_{ij} ),</li> <li>( O_i^{\text{old}} = \frac{\sum_{\text{old } j} \alpha_{ij} V_j}{l_i^{\text{old}}} ),</li> <li>( O_{\text{chunk}} = \sum_{\text{new } j} \alpha_{ij} V_j ).</li> </ul> <p>Substituting:</p> <p>[ O_i = \frac{l_i^{\text{old}} O_i^{\text{old}} + O_{\text{chunk}}}{l_i^{\text{old}} + l_{\text{chunk}}}. ]</p> <p>This matches the Flash Attention formula:</p> <p>[ O_i = \frac{l_i O_i + P_{ij} V_j}{l_{\text{new}}}. ]</p> <hr/> <h4 id="b-incremental-normalization"><strong>b. Incremental Normalization</strong></h4> <p>The normalization factor ( l_i ) (equivalent to ( Z_i ) in standard attention) is computed incrementally:</p> <p>[ l_i^{\text{new}} = l_i^{\text{old}} + \sum_{j \in \text{chunk}} \alpha_{ij}. ]</p> <p>This ensures that the normalization is consistent across all processed chunks, allowing the final ( O_i ) to represent the output for the full sequence.</p> <hr/> <h3 id="4-benefits-of-this-approach">4. <strong>Benefits of This Approach</strong></h3> <ol> <li><strong>Memory Efficiency</strong>: <ul> <li>By processing chunks incrementally, there is no need to store the full ( N \times N ) attention matrix or intermediate results.</li> </ul> </li> <li><strong>Numerical Stability</strong>: <ul> <li>The incremental updates prevent large sums from growing uncontrollably, which could cause overflow/underflow in standard softmax computations.</li> </ul> </li> <li><strong>Linearity of Weighted Sum</strong>: <ul> <li>The linearity of summation ensures that partial results (( l_i^{\text{old}} O_i^{\text{old}} )) can be combined directly with new contributions without recomputation.</li> </ul> </li> </ol> <hr/> <h3 id="5-conclusion">5. <strong>Conclusion</strong></h3> <p>The Flash Attention update formula works because:</p> <ul> <li>It incrementally combines contributions from each chunk using the linearity of the softmax-weighted sum.</li> <li>The normalization factor ( l_i ) ensures consistency across chunks, so the final result is equivalent to the full softmax computation.</li> <li>This approach trades memory for compute, making it feasible to handle large inputs while maintaining mathematical equivalence to standard attention.</li> </ul> <p>Incorrect implementation:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myFlashAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QiTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KjTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VjTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">SijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">PijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">PVTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OiTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LTensor</span><span class="p">,</span>  <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LiTensor</span><span class="p">,</span> 
	       <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LnewTensor</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Bc</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Br</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
      
  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
  <span class="c1">// Sij, Pij are passed in with Shape: (Br, Bc)</span>
  <span class="c1">// Kj, Vj are passed in with Shape: (Bc, d)</span>
  <span class="c1">// Qi, Oi, and PV  are passed in with Shape: (Br, d)</span>
  <span class="c1">// L in passed in with Shape: (N)</span>
  <span class="c1">// Li, Lij, and Lnew are passed in with shape (Br)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d)</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
 
  <span class="c1">//Format All Tensors into Vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Sij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">SijTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Pij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">PijTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Kj</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KjTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Vj</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VjTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Qi</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QiTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Oi</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OiTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">l</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">PV</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">PVTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">li</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LiTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LijTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lnew</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LnewTensor</span><span class="p">);</span>

  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_block_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_block_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">Bc</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">Bc</span><span class="p">;</span> <span class="n">k_block_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// load Kj, Vj into local memory blocks.</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">j_row</span> <span class="o">=</span> <span class="n">k_block_idx</span> <span class="o">*</span> <span class="n">Bc</span> <span class="o">+</span> <span class="n">j</span><span class="p">;</span>
      <span class="k">if</span><span class="p">(</span><span class="n">j_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">j_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
        <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">j_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
        <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Kj</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">k_val</span><span class="p">);</span>
        <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Vj</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">v_val</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">q_block_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">q_block_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">Br</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">Br</span><span class="p">;</span> <span class="n">q_block_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">// load Qi, Oi, li into local memory blocks</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span> 
        <span class="k">if</span><span class="p">(</span><span class="n">q_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span> <span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">o_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span> <span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Qi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">q_val</span><span class="p">);</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">o_val</span><span class="p">);</span>

        <span class="p">}</span>
        <span class="kt">float</span> <span class="n">l_val</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="n">q_row_idx</span><span class="p">];</span>
        <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">l_val</span><span class="p">;</span>

      <span class="p">}</span>

      <span class="c1">// compute Sij  = Qi * Kj_T (Br x Bc) </span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Qi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Kj</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>

          <span class="p">}</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Compute Pij = exp(Sij) of size (Br x Bc)</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">twoDimRead</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">));</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">exp_val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Compute lij = rowsum(Pij) of size (Br)</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>

      <span class="p">}</span>

      <span class="c1">// compute lnew = li + lij</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>
      <span class="p">}</span>

      <span class="c1">// Compute Oi = (liOi + PijVj)/ lnew</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">pv_sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">p_row</span> <span class="o">=</span> <span class="n">br_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">p_col</span> <span class="o">=</span> <span class="n">bc_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_row</span> <span class="o">=</span> <span class="n">bc_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="n">pv_sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">p_row</span><span class="p">,</span> <span class="n">p_col</span><span class="p">,</span> <span class="n">Bc</span><span class="p">)</span> <span class="o">*</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Vj</span><span class="p">,</span> <span class="n">v_row</span><span class="p">,</span> <span class="n">v_col</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>

          <span class="p">}</span>

          <span class="kt">float</span> <span class="n">li_Oi_val</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">*</span> <span class="nf">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">new_sum</span> <span class="o">=</span> <span class="n">pv_sum</span> <span class="o">+</span> <span class="n">li_Oi_val</span><span class="p">;</span>
          <span class="kt">float</span> <span class="n">new_Oi_val</span> <span class="o">=</span> <span class="n">new_sum</span> <span class="o">/</span> <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">new_Oi_val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Write Oi and lnew back to O and l in main memory</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">O_row</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span>
        <span class="k">if</span><span class="p">(</span><span class="n">O_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">break</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">Oi_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">int</span> <span class="n">O_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">O_row</span><span class="p">,</span> <span class="n">O_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">Oi_val</span><span class="p">);</span>

        <span class="p">}</span>

        <span class="n">l</span><span class="p">[</span><span class="n">O_row</span><span class="p">]</span> <span class="o">=</span> <span class="n">lnew</span><span class="p">[</span><span class="n">O_row</span><span class="p">];</span>

      <span class="p">}</span>


    <span class="p">}</span>
  <span class="p">}</span>

    <span class="p">}</span>

  <span class="p">}</span>


  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Self CPU time total: 589.890ms

REFERENCE - FLASH ATTENTION statistics
cpu time:  588.591ms
mem usage:  524288 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-20 14:25:23 442505:442505 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0773, 0.0781, 0.0789, 0.0797, 0.0805, 0.0813, 0.0821, 0.0829, 0.0837,
        0.0845, 0.0853, 0.0861, 0.0869, 0.0877, 0.0885, 0.0893, 0.0901, 0.0909,
        0.0917, 0.0925, 0.0933, 0.0941, 0.0949, 0.0957, 0.0965, 0.0973, 0.0981,
        0.0989, 0.0997, 0.1005, 0.1013, 0.1021])
STAGE:2024-11-20 14:25:23 442505:442505 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-20 14:25:23 442505:442505 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 317, in main
    part4Test(N, d, B, H, int(args.bc), int(args.br))
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 257, in part4Test
    testTemplate(attentionModuleReference.myFlashAttention, params, "STUDENT - FLASH ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Asked gpt to find bug in my previous code</p> <p>Some are corret , some are not.</p> <p>There are several potential bugs and inefficiencies in the provided implementation of Flash Attention. Below is an analysis of common issues based on the code structure.</p> <hr/> <h3 id="1-bug-in-writing-lnew-to-global-memory">1. <strong>Bug in Writing <code class="language-plaintext highlighter-rouge">lnew</code> to Global Memory</strong></h3> <p>At the end of the block:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span><span class="p">[</span><span class="n">O_row</span><span class="p">]</span> <span class="o">=</span> <span class="n">lnew</span><span class="p">[</span><span class="n">O_row</span><span class="p">];</span>
</code></pre></div></div> <p>This line is problematic because <code class="language-plaintext highlighter-rouge">lnew[O_row]</code> is incorrect. You are trying to write from <code class="language-plaintext highlighter-rouge">lnew</code> (a local array for the current query block) to the global <code class="language-plaintext highlighter-rouge">l</code>. The index <code class="language-plaintext highlighter-rouge">O_row</code> does not map directly to <code class="language-plaintext highlighter-rouge">lnew</code>, as <code class="language-plaintext highlighter-rouge">lnew</code> uses local indices within a chunk.</p> <p><strong>Fix</strong>:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span><span class="p">[</span><span class="n">O_row</span><span class="p">]</span> <span class="o">=</span> <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">br_idx</code> is the correct index for <code class="language-plaintext highlighter-rouge">lnew</code> in the current query block.</p> <hr/> <h3 id="2-mismatch-in-twodimread-and-twodimwrite-for-sij">2. <strong>Mismatch in <code class="language-plaintext highlighter-rouge">twoDimRead</code> and <code class="language-plaintext highlighter-rouge">twoDimWrite</code> for <code class="language-plaintext highlighter-rouge">Sij</code></strong></h3> <p>In the computation of ( S_{ij} ):</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
</code></pre></div></div> <p>You are using <code class="language-plaintext highlighter-rouge">d</code> as the stride instead of <code class="language-plaintext highlighter-rouge">Bc</code>. <code class="language-plaintext highlighter-rouge">Sij</code> is declared as having a shape <code class="language-plaintext highlighter-rouge">(Br, Bc)</code>, so the stride in the <code class="language-plaintext highlighter-rouge">twoDimWrite</code> call should be <code class="language-plaintext highlighter-rouge">Bc</code>.</p> <p><strong>Fix</strong>:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
</code></pre></div></div> <p>Similarly, update other uses of <code class="language-plaintext highlighter-rouge">Sij</code> to use the correct stride.</p> <hr/> <h3 id="3-incorrect-update-of-lioi">3. <strong>Incorrect Update of <code class="language-plaintext highlighter-rouge">liOi</code></strong></h3> <p>In the line:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">li_Oi_val</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">*</span> <span class="nf">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
</code></pre></div></div> <p>This reads the old <code class="language-plaintext highlighter-rouge">Oi</code> value from <code class="language-plaintext highlighter-rouge">Oi</code>, but <code class="language-plaintext highlighter-rouge">Oi</code> is already being updated in the loop. This introduces unintended feedback into the computation.</p> <p><strong>Fix</strong>: Make a copy of <code class="language-plaintext highlighter-rouge">Oi</code> at the start of each block before updates, and use that copy for the computation:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Oi_copy</span> <span class="o">=</span> <span class="n">Oi</span><span class="p">;</span> <span class="c1">// Create a copy of Oi</span>
<span class="kt">float</span> <span class="n">li_Oi_val</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">*</span> <span class="nf">twoDimRead</span><span class="p">(</span><span class="n">Oi_copy</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
</code></pre></div></div> <hr/> <h3 id="4-improper-memory-initialization">4. <strong>Improper Memory Initialization</strong></h3> <p>Variables like <code class="language-plaintext highlighter-rouge">Kj</code>, <code class="language-plaintext highlighter-rouge">Vj</code>, <code class="language-plaintext highlighter-rouge">Qi</code>, <code class="language-plaintext highlighter-rouge">Oi</code>, <code class="language-plaintext highlighter-rouge">Sij</code>, etc., are written to without ensuring their contents are initialized correctly. This might cause unexpected behavior if these tensors are not zeroed out.</p> <p><strong>Fix</strong>: Explicitly zero out these tensors before use:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Kj</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Kj</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Vj</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Vj</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Qi</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Qi</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Oi</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Oi</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Sij</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Sij</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
</code></pre></div></div> <hr/> <h3 id="5-incorrect-stride-in-twodimread-and-twodimwrite-for-block-matrices">5. <strong>Incorrect Stride in <code class="language-plaintext highlighter-rouge">twoDimRead</code> and <code class="language-plaintext highlighter-rouge">twoDimWrite</code> for Block Matrices</strong></h3> <p>Some block matrices (e.g., <code class="language-plaintext highlighter-rouge">Pij</code>, <code class="language-plaintext highlighter-rouge">Kj</code>, <code class="language-plaintext highlighter-rouge">Vj</code>, <code class="language-plaintext highlighter-rouge">Qi</code>) use strides inconsistently. Double-check the stride parameter in each <code class="language-plaintext highlighter-rouge">twoDimRead</code> and <code class="language-plaintext highlighter-rouge">twoDimWrite</code> call to match the matrix dimensions.</p> <hr/> <h3 id="6-inefficient-computation-of-lij-and-lnew">6. <strong>Inefficient Computation of <code class="language-plaintext highlighter-rouge">lij</code> and <code class="language-plaintext highlighter-rouge">lnew</code></strong></h3> <p>The computation of <code class="language-plaintext highlighter-rouge">lij</code> (row sum of <code class="language-plaintext highlighter-rouge">Pij</code>) and <code class="language-plaintext highlighter-rouge">lnew</code> involves multiple sequential loops that can be combined for better performance. For example:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
  <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>This eliminates one loop.</p> <hr/> <h3 id="7-numerical-stability-in-softmax">7. <strong>Numerical Stability in Softmax</strong></h3> <p>The computation of ( P_{ij} = \exp(S_{ij}) ) can suffer from numerical instability if ( S_{ij} ) contains large values. This can lead to overflow or inaccurate results.</p> <p><strong>Fix</strong>: Subtract the maximum value in each row of ( S_{ij} ) before applying the exponential function:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">float</span> <span class="n">max_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">std</span><span class="o">::</span><span class="n">numeric_limits</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;::</span><span class="n">infinity</span><span class="p">();</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">sij_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">);</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">sij_val</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">sij_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">)</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">;</span>
    <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">sij_val</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>This ensures the softmax computation is numerically stable.</p> <hr/> <h3 id="8-inefficient-loop-nesting">8. <strong>Inefficient Loop Nesting</strong></h3> <p>The loop order might lead to poor memory access patterns, especially for <code class="language-plaintext highlighter-rouge">Kj</code>, <code class="language-plaintext highlighter-rouge">Vj</code>, and <code class="language-plaintext highlighter-rouge">Qi</code>. Consider transposing these matrices in local memory to improve cache efficiency.</p> <hr/> <h3 id="9-potential-misalignment-in-block-indexing">9. <strong>Potential Misalignment in Block Indexing</strong></h3> <p>When processing blocks of size <code class="language-plaintext highlighter-rouge">Br</code> and <code class="language-plaintext highlighter-rouge">Bc</code>, ensure that the indexing properly handles edge cases where <code class="language-plaintext highlighter-rouge">N</code> is not evenly divisible by <code class="language-plaintext highlighter-rouge">Br</code> or <code class="language-plaintext highlighter-rouge">Bc</code>. Currently, edge handling appears inconsistent.</p> <p><strong>Fix</strong>: Use bounds checking consistently:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">q_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span> <span class="o">||</span> <span class="n">j_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
</code></pre></div></div> <hr/> <h3 id="10-output-tensor-not-updated-properly">10. <strong>Output Tensor Not Updated Properly</strong></h3> <p>The output tensor <code class="language-plaintext highlighter-rouge">OTensor</code> may not be updated properly due to indexing or memory mismanagement.</p> <p><strong>Fix</strong>: Ensure that updates to <code class="language-plaintext highlighter-rouge">O</code> directly correspond to <code class="language-plaintext highlighter-rouge">OTensor</code>:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">O_row</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">O_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">Oi_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
    <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">O_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">Oi_val</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/> <h3 id="summary-of-key-fixes">Summary of Key Fixes</h3> <ol> <li>Correct indexing when writing <code class="language-plaintext highlighter-rouge">lnew</code> to <code class="language-plaintext highlighter-rouge">l</code>.</li> <li>Fix strides in <code class="language-plaintext highlighter-rouge">twoDimRead</code> and <code class="language-plaintext highlighter-rouge">twoDimWrite</code> for block matrices.</li> <li>Zero-initialize temporary tensors.</li> <li>Apply numerical stabilization for the softmax computation.</li> <li>Combine loops for <code class="language-plaintext highlighter-rouge">lij</code> and <code class="language-plaintext highlighter-rouge">lnew</code>.</li> <li>Address potential misalignment when <code class="language-plaintext highlighter-rouge">N</code> is not a multiple of block sizes.</li> </ol> <p>These fixes ensure correctness, improve numerical stability, and enhance efficiency.</p> <p>Add initialization of 0.0 to Kj and Vj, to avoid impact of leftover numnber in last iteration</p> <p>I realize this problem from python diff check and found that value is not equal at the last row of matrix.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mismatch at index (0, 3, 1023, 22): QKV=0.13594280183315277, QKS1=0.03398570418357849
Mismatch at index (0, 3, 1023, 23): QKV=0.13674280047416687, QKS1=0.03418571129441261
Mismatch at index (0, 3, 1023, 24): QKV=0.13754287362098694, QKS1=0.03438570350408554
Mismatch at index (0, 3, 1023, 25): QKV=0.13834281265735626, QKS1=0.03458569571375847
Mismatch at index (0, 3, 1023, 26): QKV=0.13914281129837036, QKS1=0.03478570282459259
Mismatch at index (0, 3, 1023, 27): QKV=0.13994282484054565, QKS1=0.034985702484846115
Mismatch at index (0, 3, 1023, 28): QKV=0.14074280858039856, QKS1=0.03518570587038994
Mismatch at index (0, 3, 1023, 29): QKV=0.14154279232025146, QKS1=0.035385698080062866
Mismatch at index (0, 3, 1023, 30): QKV=0.14234280586242676, QKS1=0.03558569401502609
Mismatch at index (0, 3, 1023, 31): QKV=0.14314278960227966, QKS1=0.03578570485115051
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 339, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 327, in main
    part4Test(N, d, B, H, int(args.bc), int(args.br))
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 267, in part4Test
    testTemplate(attentionModuleReference.myFlashAttention, params, "STUDENT - FLASH ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 190, in testTemplate
    raise AssertionError(correctness_error_message)
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Found that result is incorrect starting from second head.</p> <p>I think it’s becase that some itermediate variable is not cleared before entering next head which leads to incorrect answer.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mismatch at index (0, 1, 0, 0): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 1): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 2): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 3): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 4): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 5): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 6): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 7): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 8): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 9): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 10): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 11): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 12): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 13): QKV=1.0, QKS1=0.5
M
</code></pre></div></div> <p>Found the root cause and fix it.</p> <p>I need to clear <code class="language-plaintext highlighter-rouge">l</code> for each head. This is because <code class="language-plaintext highlighter-rouge">l</code> is read to <code class="language-plaintext highlighter-rouge">li</code> for each <code class="language-plaintext highlighter-rouge">Q-</code> block</p> <p>Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myFlashAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QiTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KjTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VjTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">SijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">PijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">PVTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OiTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LTensor</span><span class="p">,</span>  <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LiTensor</span><span class="p">,</span> 
	       <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LnewTensor</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Bc</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Br</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
      
  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
  <span class="c1">// Sij, Pij are passed in with Shape: (Br, Bc)</span>
  <span class="c1">// Kj, Vj are passed in with Shape: (Bc, d)</span>
  <span class="c1">// Qi, Oi, and PV  are passed in with Shape: (Br, d)</span>
  <span class="c1">// L in passed in with Shape: (N)</span>
  <span class="c1">// Li, Lij, and Lnew are passed in with shape (Br)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d)</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
 
  <span class="c1">//Format All Tensors into Vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Sij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">SijTensor</span><span class="p">);</span> <span class="c1">//clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Pij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">PijTensor</span><span class="p">);</span> <span class="c1">//clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Kj</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KjTensor</span><span class="p">);</span> <span class="c1">// clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Vj</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VjTensor</span><span class="p">);</span> <span class="c1">// clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Qi</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QiTensor</span><span class="p">);</span> <span class="c1">// clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Oi</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OiTensor</span><span class="p">);</span> <span class="c1">//clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">l</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LTensor</span><span class="p">);</span> <span class="c1">// This should be cleared</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">PV</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">PVTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">li</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LiTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LijTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lnew</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LnewTensor</span><span class="p">);</span>

  <span class="c1">// std::cout &lt;&lt; "br:" &lt;&lt; Br &lt;&lt; " bc:" &lt;&lt; Bc &lt;&lt;std::endl;</span>
  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

    <span class="c1">// This line is essential to correctness.</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">l</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">lnew</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">lnew</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">lij</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">lij</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_block_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_block_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">Bc</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">Bc</span><span class="p">;</span> <span class="n">k_block_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Kj</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Kj</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Vj</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Vj</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="c1">// load Kj, Vj into local memory blocks.</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">j_row</span> <span class="o">=</span> <span class="n">k_block_idx</span> <span class="o">*</span> <span class="n">Bc</span> <span class="o">+</span> <span class="n">j</span><span class="p">;</span>
      <span class="k">if</span><span class="p">(</span><span class="n">j_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">j_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
        <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">j_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
        <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Kj</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">k_val</span><span class="p">);</span>
        <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Vj</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">v_val</span><span class="p">);</span>
          <span class="c1">// std::cout&lt;&lt; "j:" &lt;&lt; j_row &lt;&lt; " col:" &lt;&lt; d_idx &lt;&lt; "kj:" &lt;&lt; k_val &lt;&lt; " vj:" &lt;&lt; v_val &lt;&lt; std::endl;</span>
      <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">q_block_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">q_block_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">Br</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">Br</span><span class="p">;</span> <span class="n">q_block_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Qi</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Qi</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
      <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Oi</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Oi</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
      <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Sij</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Sij</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
      <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Pij</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Pij</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>


      <span class="c1">// load Qi, Oi, li into local memory blocks</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span> 
        <span class="k">if</span><span class="p">(</span><span class="n">q_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span> <span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">o_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span> <span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Qi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">q_val</span><span class="p">);</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">o_val</span><span class="p">);</span>
            <span class="c1">// std::cout &lt;&lt; "q_row_idx:" &lt;&lt; q_row_idx &lt;&lt; " d_idx:" &lt;&lt; d_idx &lt;&lt; " Qi:" &lt;&lt; q_val &lt;&lt; " Oi:" &lt;&lt; o_val &lt;&lt;std::endl;</span>

        <span class="p">}</span>
        <span class="kt">float</span> <span class="n">l_val</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="n">q_row_idx</span><span class="p">];</span>
        <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">l_val</span><span class="p">;</span>
            <span class="c1">// std::cout &lt;&lt; "li:" &lt;&lt; l_val &lt;&lt; std::endl;</span>

      <span class="p">}</span>

      <span class="c1">// compute Sij  = Qi * Kj_T (Br x Bc) </span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Qi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Kj</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>

          <span class="p">}</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
              <span class="c1">// std::cout &lt;&lt; "sij, br:" &lt;&lt; br_idx &lt;&lt; " bc:" &lt;&lt; bc_idx &lt;&lt; " val:" &lt;&lt; sum &lt;&lt; std::endl;</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Compute Pij = exp(Sij) of size (Br x Bc)</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">twoDimRead</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">));</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">exp_val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Compute lij = rowsum(Pij) of size (Br)</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
        <span class="c1">// compute lnew = li + lij</span>
        <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>

      <span class="p">}</span>



      <span class="c1">// Compute Oi = (liOi + PijVj)/ lnew</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">pv_sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">p_row</span> <span class="o">=</span> <span class="n">br_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">p_col</span> <span class="o">=</span> <span class="n">bc_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_row</span> <span class="o">=</span> <span class="n">bc_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="n">pv_sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">p_row</span><span class="p">,</span> <span class="n">p_col</span><span class="p">,</span> <span class="n">Bc</span><span class="p">)</span> <span class="o">*</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Vj</span><span class="p">,</span> <span class="n">v_row</span><span class="p">,</span> <span class="n">v_col</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>

          <span class="p">}</span>
          <span class="c1">// twoDimWrite(PV, br_idx, d_idx, d, pv_sum);</span>

          <span class="kt">float</span> <span class="n">li_Oi_val</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">*</span> <span class="nf">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">new_sum</span> <span class="o">=</span> <span class="n">pv_sum</span> <span class="o">+</span> <span class="n">li_Oi_val</span><span class="p">;</span>
          <span class="kt">float</span> <span class="n">new_Oi_val</span> <span class="o">=</span> <span class="n">new_sum</span> <span class="o">/</span> <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">new_Oi_val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Write Oi and lnew back to O and l in main memory</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">O_row</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span>
        <span class="k">if</span><span class="p">(</span><span class="n">O_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">Oi_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">int</span> <span class="n">O_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">O_row</span><span class="p">,</span> <span class="n">O_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">Oi_val</span><span class="p">);</span>

        <span class="p">}</span>

        <span class="n">l</span><span class="p">[</span><span class="n">O_row</span><span class="p">]</span> <span class="o">=</span> <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>

      <span class="p">}</span>


    <span class="p">}</span>
  <span class="p">}</span>

    <span class="p">}</span>

  <span class="p">}</span>


  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <p>OUtput:</p> <p>My implementation is 3x faster than ref implementation. I don’t know why. Is it because that I don’t materialize <code class="language-plaintext highlighter-rouge">PV</code>?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>               model_inference         0.11%     771.000us       100.00%     721.392ms     721.392ms     512.00 Kb     -10.53 Kb             1
    REFERENCE - FLASH ATTENTION        99.56%     718.217ms        99.83%     720.142ms     720.142ms     512.00 Kb      -8.00 Mb             1
                       aten::to         0.00%       5.000us         0.00%      33.000us       5.500us         520 b           0 b             6
                 aten::_to_copy         0.00%      16.000us         0.00%      28.000us       7.000us         520 b           8 b             4
                      aten::abs         0.00%      30.000us         0.01%      54.000us      13.500us         512 b         256 b             4
            aten::empty_strided         0.00%       3.000us         0.00%       3.000us       0.750us         512 b         512 b             4
                 aten::isfinite         0.00%      12.000us         0.01%      89.000us      89.000us         224 b           0 b             1
            aten::masked_select         0.00%      14.000us         0.01%      43.000us      43.000us         128 b         120 b             1
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 721.392ms

REFERENCE - FLASH ATTENTION statistics
cpu time:  720.142ms
mem usage:  524288 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
student first row:
 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
manual attention == pytorch attention True
Manual Execution Time:  0.21289777755737305

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::empty         0.01%      16.000us         0.01%      16.000us       0.889us       1.00 Mb       1.00 Mb            18
                  aten::clone         0.01%      25.000us         0.16%     332.000us     166.000us       1.00 Mb           0 b             2
                  aten::zeros         0.02%      36.000us         0.04%      83.000us       6.917us     521.59 Kb       5.00 Kb            12
          aten::empty_strided         0.00%      10.000us         0.00%      10.000us       2.000us     512.51 Kb     512.51 Kb             5
              model_inference         0.34%     722.000us       100.00%     212.937ms     212.937ms     512.00 Kb     -15.47 Kb             1
    STUDENT - FLASH ATTENTION        99.25%     211.341ms        99.44%     211.746ms     211.746ms     512.00 Kb      -1.00 Mb             1
                aten::flatten         0.01%      26.000us         0.10%     216.000us      14.400us     512.00 Kb           0 b            15
             aten::empty_like         0.00%       4.000us         0.00%       6.000us       6.000us     512.00 Kb           0 b             1
                  aten::zero_         0.00%       5.000us         0.02%      35.000us       2.917us       5.00 Kb       5.00 Kb            12
                     aten::to         0.01%      16.000us         0.01%      29.000us       4.833us         520 b           4 b             6
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 212.937ms

STUDENT - FLASH ATTENTION statistics
cpu time:  211.746ms
mem usage:  524288 bytes
</code></pre></div></div>]]></content><author><name></name></author><category term="ml"/><category term="ai"/><category term="ml"/><category term="transformer"/><summary type="html"><![CDATA[Stf CS149 flash attention lab assignment repo]]></summary></entry><entry><title type="html">Elf Loading</title><link href="https://bilyz98.github.io/blog/2024/elf-loading/" rel="alternate" type="text/html" title="Elf Loading"/><published>2024-11-07T00:00:00+00:00</published><updated>2024-11-07T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/elf-loading</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/elf-loading/"><![CDATA[<p><a href="https://www.youtube.com/watch?v=IY1FwkFu44E">ELF loading video</a></p> <ul> <li>Compile-time linking</li> <li>Runtime-time linking(dynamic linking)</li> </ul> <p>Output of compiler: object file.</p> <p>This object file includes code, data symbol table, and more</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Stf CS149 Parallel Programming - Assign3</title><link href="https://bilyz98.github.io/blog/2024/cs149-assign3/" rel="alternate" type="text/html" title="Stf CS149 Parallel Programming - Assign3"/><published>2024-11-03T11:59:00+00:00</published><updated>2024-11-03T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cs149-assign3</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cs149-assign3/"><![CDATA[<h2 id="part1">Part1</h2> <p>Code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code></code></pre></div></div> <p>saxpy serial cpu output from assign1 prog5:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog5_saxpy git:(master) ✗ ./saxpy
[saxpy serial]:         [20.605] ms     [14.464] GB/s   [1.941] GFLOPS
[saxpy ispc]:           [17.866] ms     [16.681] GB/s   [2.239] GFLOPS
[saxpy task ispc]:      [3.122] ms      [95.446] GB/s   [12.810] GFLOPS                                                                                                                                                           (5.72x speedup from use of tasks)
</code></pre></div></div> <p>saxpy gpu cuda output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Found 4 CUDA devices
Device 0: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 1: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 2: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 3: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
---------------------------------------------------------
Running 3 timing tests:
Effective BW by CUDA saxpy: 225.263 ms          [4.961 GB/s]
kernel execution time: 1.503ms
Effective BW by CUDA saxpy: 247.816 ms          [4.510 GB/s]
kernel execution time: 1.504ms
Effective BW by CUDA saxpy: 245.998 ms          [4.543 GB/s]
kernel execution time: 1.506ms
</code></pre></div></div> <p>Looks like gpu bandwidth is lower than cpu</p> <p>kernel execution time is super short and all the time is taken for memory copy.</p> <p>I am a little bit confused about the two bandwidths listed in this doc https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-datasheet-951643.pdf</p> <p>gpu memory bandwidth is 300GB/sec and interconnect bandwidth is 32 GB/sec.</p> <p>I guess gpu memory bandwidth is the bandwidth that is used in internal SMs in gpu</p> <p>And interconnect bandwidth is the bandwidth during transfer data between cpu and gp</p> <p>command to run when on A800</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./cudaSaxpy: error while loading shared libraries: libcudart.so.12: cannot open shared object file: No such file or directory

[nsccgz_qylin_1@gpu72%tianhe2-K saxpy]$ echo $LD_LIBRARY_PATH | grep dart
[nsccgz_qylin_1@gpu72%tianhe2-K saxpy]$ export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
[nsccgz_qylin_1@gpu72%tianhe2-K saxpy]$ export LD_LIBRARY_PATH=/usr/local/cuda-12.0/lib64:$LD_LIBRARY_PATH
</code></pre></div></div> <h2 id="part2-parallel-prefix-sum">Part2: parallel prefix sum</h2> <p>Get this libstd lib version issue when running execution binary</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@gpu72%tianhe2-K scan]$ ./cudaScan ./cudaScan: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by ./cudaScan) ./cudaScan: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by ./cudaScan) ./cudaScan: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by ./cudaScan)
</code></pre></div></div> <p>Solution: Use conda to install libstdcxx and include it in <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate myenv
conda install -c conda-forge libstdcxx-ng
find $CONDA_PREFIX -name "libstdc++.so.6"

 export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Found 4 CUDA devices
Device 0: NVIDIA A800 80GB PCIe
   SMs:        108
   Global mem: 81229 MB
   CUDA Cap:   8.0
Device 1: NVIDIA A800 80GB PCIe
   SMs:        108
   Global mem: 81229 MB
   CUDA Cap:   8.0
Device 2: NVIDIA A800 80GB PCIe
   SMs:        108
   Global mem: 81229 MB
   CUDA Cap:   8.0
Device 3: NVIDIA A800 80GB PCIe
   SMs:        108
   Global mem: 81229 MB
   CUDA Cap:   8.0
---------------------------------------------------------
Array size: 64
Student GPU time: 0.069 ms
Scan outputs are correct!
</code></pre></div></div> <h3 id="round-input-length-power-of-2-for-cudascan">Round input length power of 2 for cudaScan</h3> <h3 id="find-repeats">find repeats</h3> <p>I did not know why find repeats could be parallelized and how it can be done until I read code from others.</p> <p>The idea is that the return result of find repeats is the indices of <code class="language-plaintext highlighter-rouge">A[i]==A[i+1]</code></p> <p>This find repeats process can be parallelized with exclusive scan but we first need to generate a intermetidate representation of input arr.</p> <p>So basically we first generate a indices array which runs on cuda that can be parallelized. The output indices array is that <code class="language-plaintext highlighter-rouge">arr[i] = 1 if A[i]==A[i+1] else = 0</code>.</p> <p>This flags array is then passed to cudascan function for parallel exclusive scan to get a new array flags_sum_arr where <code class="language-plaintext highlighter-rouge">flags_sum_arr[i]</code> indicates how many repeated elements are accumulated so far.</p> <p>And then this <code class="language-plaintext highlighter-rouge">flags_sum_arr</code> is passed to another cuda kernel which is also parallelized to generated the final indices array <code class="language-plaintext highlighter-rouge">indices[i]</code>.</p> <p>This is the fun part that shows the core of parallel programming which is that each subtask has not dependency on each other. The output writing is totally independent.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// cuda kernal code</span>
<span class="k">if</span><span class="p">(</span><span class="n">flags_sum_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">flags_sum_arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="p">{</span>
    <span class="c1">// this indicates this is a repeated element in input array</span>
    <span class="c1">// The position of output value we need to write is flags_sum_arr[i]</span>
    <span class="c1">// The repeated element index is i.</span>
    <span class="n">indices</span><span class="p">[</span><span class="n">flags_sum_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span> 
<span class="p">}</span>
</code></pre></div></div> <p>Problem:</p> <p>Input array in cpu and input array in gpu is not the same</p> <p>Why is that?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input arr
0:1 1:1 2:1 3:1 4:1 5:1 6:1 7:1
input arr
0:1 1:1 2:337500088 3:10935 4:0 5:0 6:0 7:0
flags arr
0:1 1:1 2:337500088 3:10935 4:0 5:0 6:0 7:0
</code></pre></div></div> <p>This is because I did not copy all bytes of intput element</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> cudaMemcpy(arr, device_input, length*sizeof(int), cudaMemcpyDeviceToHost);
</code></pre></div></div> <p>and</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
cudaMemcpy(arr, device_input, length, cudaMemcpyDeviceToHost);
</code></pre></div></div> <p>Need to copy length of <code class="language-plaintext highlighter-rouge">length*sizeof(int)</code> instead of <code class="language-plaintext highlighter-rouge">length</code></p> <p>memcpy copies number of bytes specified.</p> <p>Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> 
<span class="nf">flag_repeats_kernel</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">index</span> <span class="o">&lt;</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">==</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="p">{</span>
    <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="p">}</span>


<span class="p">}</span>

<span class="n">__global__</span> <span class="kt">void</span> 
<span class="nf">flags_extract_indices</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">input</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span> <span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">index</span> <span class="o">&lt;</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="p">{</span>
    <span class="n">output</span><span class="p">[</span><span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span> <span class="o">=</span> <span class="n">index</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// find_repeats --</span>
<span class="c1">//</span>
<span class="c1">// Given an array of integers `device_input`, returns an array of all</span>
<span class="c1">// indices `i` for which `device_input[i] == device_input[i+1]`.</span>
<span class="c1">//</span>
<span class="c1">// Returns the total number of pairs found</span>
<span class="kt">int</span> <span class="nf">find_repeats</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">device_input</span><span class="p">,</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">device_output</span><span class="p">)</span> <span class="p">{</span>

    <span class="c1">// CS149 TODO:</span>
    <span class="c1">//</span>
    <span class="c1">// Implement this function. You will probably want to</span>
    <span class="c1">// make use of one or more calls to exclusive_scan(), as well as</span>
    <span class="c1">// additional CUDA kernel launches.</span>
    <span class="c1">//    </span>
    <span class="c1">// Note: As in the scan code, the calling code ensures that</span>
    <span class="c1">// allocated arrays are a power of 2 in size, so you can use your</span>
    <span class="c1">// exclusive_scan function with them. However, your implementation</span>
    <span class="c1">// must ensure that the results of find_repeats are correct given</span>
    <span class="c1">// the actual array length.</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">flags_arr</span><span class="p">;</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">flags_sum_arr</span><span class="p">;</span>

  <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">flags_arr</span><span class="p">,</span> <span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
  <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">flags_sum_arr</span><span class="p">,</span> <span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

  <span class="k">const</span> <span class="kt">int</span> <span class="n">threadsPerBlock</span> <span class="o">=</span> <span class="mi">512</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">length</span> <span class="o">+</span> <span class="n">threadsPerBlock</span> <span class="o">-</span><span class="mi">1</span> <span class="p">)</span> <span class="o">/</span> <span class="n">threadsPerBlock</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">repeat_indices_count</span><span class="p">;</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">arr</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
  <span class="c1">// cudaMemcpy(arr, device_input, length*sizeof(int), cudaMemcpyDeviceToHost);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
  <span class="c1">// printf("input arr2\n");</span>
  <span class="c1">// print_arr(arr, length);</span>
  <span class="n">flag_repeats_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">device_input</span><span class="p">,</span> <span class="n">flags_arr</span><span class="p">,</span> <span class="n">length</span><span class="p">);</span>
  <span class="c1">// cudaMemcpy(arr, flags_arr, length*sizeof(int), cudaMemcpyDeviceToHost);</span>
  <span class="c1">// printf("flags arr\n");</span>
  <span class="c1">// print_arr(arr, length);</span>
  <span class="n">cudaScan</span><span class="p">(</span><span class="n">flags_arr</span><span class="p">,</span> <span class="n">flags_arr</span><span class="o">+</span><span class="n">length</span><span class="p">,</span> <span class="n">flags_sum_arr</span><span class="p">)</span> <span class="p">;</span>
  <span class="n">flags_extract_indices</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">flags_sum_arr</span><span class="p">,</span> <span class="n">device_output</span><span class="p">,</span> <span class="n">length</span><span class="p">);</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">repeat_indices_count</span><span class="p">,</span> <span class="n">flags_sum_arr</span><span class="o">+</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
  <span class="n">free</span><span class="p">(</span><span class="n">arr</span><span class="p">);</span>

  <span class="k">return</span> <span class="n">repeat_indices_count</span><span class="p">;</span> 

<span class="p">}</span>


<span class="c1">//</span>
<span class="c1">// cudaFindRepeats --</span>
<span class="c1">//</span>
<span class="c1">// Timing wrapper around find_repeats. You should not modify this function.</span>
<span class="kt">double</span> <span class="nf">cudaFindRepeats</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">input</span><span class="p">,</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">output_length</span><span class="p">)</span> <span class="p">{</span>

    <span class="kt">int</span> <span class="o">*</span><span class="n">device_input</span><span class="p">;</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">device_output</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">rounded_length</span> <span class="o">=</span> <span class="n">nextPow2</span><span class="p">(</span><span class="n">length</span><span class="p">);</span>
    
    <span class="c1">// printf("input arr1\n");</span>
    <span class="c1">// print_arr(input, length);  </span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_input</span><span class="p">,</span> <span class="n">rounded_length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_output</span><span class="p">,</span> <span class="n">rounded_length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">device_input</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="kt">double</span> <span class="n">startTime</span> <span class="o">=</span> <span class="n">CycleTimer</span><span class="o">::</span><span class="n">currentSeconds</span><span class="p">();</span>
    
    <span class="kt">int</span> <span class="n">result</span> <span class="o">=</span> <span class="n">find_repeats</span><span class="p">(</span><span class="n">device_input</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">device_output</span><span class="p">);</span>

    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="kt">double</span> <span class="n">endTime</span> <span class="o">=</span> <span class="n">CycleTimer</span><span class="o">::</span><span class="n">currentSeconds</span><span class="p">();</span>

    <span class="c1">// set output count and results array</span>
    <span class="o">*</span><span class="n">output_length</span> <span class="o">=</span> <span class="n">result</span><span class="p">;</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">device_output</span><span class="p">,</span> <span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

    <span class="c1">// printf("output length:%d\n", *output_length);</span>
    <span class="c1">// printf("output indices\n");</span>
    <span class="c1">// print_arr(output, length);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_input</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_output</span><span class="p">);</span>

    <span class="kt">float</span> <span class="n">duration</span> <span class="o">=</span> <span class="n">endTime</span> <span class="o">-</span> <span class="n">startTime</span><span class="p">;</span> 
    <span class="k">return</span> <span class="n">duration</span><span class="p">;</span>
<span class="p">}</span>

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ yhrun -p gpu_v100 ./cudaScan -m find_repeats -n 8  -i ones
---------------------------------------------------------
Found 4 CUDA devices
Device 0: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 1: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 2: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 3: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
---------------------------------------------------------
Array size: 8
flags arr
0:1 1:1 2:1 3:1 4:1 5:1 6:1 7:0
output length:7
output indices
0:0 1:1 2:2 3:3 4:4 5:5 6:6 7:0
flags arr
0:1 1:1 2:1 3:1 4:1 5:1 6:1 7:0
output length:7
output indices
0:0 1:1 2:2 3:3 4:4 5:5 6:6 7:0
flags arr
0:1 1:1 2:1 3:1 4:1 5:1 6:1 7:0
output length:7
output indices
0:0 1:1 2:2 3:3 4:4 5:5 6:6 7:0
Student GPU time: 0.199 ms
Find_repeats outputs are correct!
</code></pre></div></div> <h2 id="part-3-renderer">Part 3: renderer</h2> <p>Install opengl library before compile the project</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install -c anaconda pyopengl
conda install -c anaconda freeglut
</code></pre></div></div> <p>Install opengl library using conda and then compile c code that use opengl library with Makefile</p> <p>The thing to note about is that we need to include conda library path in CFLAGS and LDFLAGS</p> <h3 id="step-by-step-guide">Step-by-Step Guide</h3> <ol> <li><strong>Install OpenGL using conda</strong>: <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> opengl_env
conda activate opengl_env
conda <span class="nb">install</span> <span class="nt">-c</span> anaconda pyopengl
conda <span class="nb">install</span> <span class="nt">-c</span> anaconda freeglut
</code></pre></div> </div> </li> <li><strong>Write Your C Code</strong>: <ul> <li>Create a file named <code class="language-plaintext highlighter-rouge">main.c</code> with the following content: <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;GL/glut.h&gt;</span><span class="cp">
</span>
<span class="kt">void</span> <span class="nf">display</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">glClear</span><span class="p">(</span><span class="n">GL_COLOR_BUFFER_BIT</span><span class="p">);</span>
    <span class="n">glBegin</span><span class="p">(</span><span class="n">GL_TRIANGLES</span><span class="p">);</span>
    <span class="n">glVertex2f</span><span class="p">(</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">);</span>
    <span class="n">glVertex2f</span><span class="p">(</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">);</span>
    <span class="n">glVertex2f</span><span class="p">(</span><span class="mi">0</span><span class="p">.</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">);</span>
    <span class="n">glEnd</span><span class="p">();</span>
    <span class="n">glFlush</span><span class="p">();</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">glutInit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">);</span>
    <span class="n">glutCreateWindow</span><span class="p">(</span><span class="s">"OpenGL Setup Test"</span><span class="p">);</span>
    <span class="n">glutDisplayFunc</span><span class="p">(</span><span class="n">display</span><span class="p">);</span>
    <span class="n">glutMainLoop</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> </div> </li> </ul> </li> <li><strong>Create a Makefile</strong>: <ul> <li>Create a file named <code class="language-plaintext highlighter-rouge">Makefile</code> with the following content: <div class="language-makefile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">CC</span> <span class="o">=</span> gcc
<span class="nv">CFLAGS</span> <span class="o">=</span> <span class="nt">-I</span><span class="p">$(</span>CONDA_PREFIX<span class="p">)</span>/include
<span class="nv">LDFLAGS</span> <span class="o">=</span> <span class="nt">-L</span><span class="p">$(</span>CONDA_PREFIX<span class="p">)</span>/lib <span class="nt">-lGL</span> <span class="nt">-lGLU</span> <span class="nt">-lglut</span>

<span class="nl">all</span><span class="o">:</span> <span class="nf">main</span>

<span class="nl">main</span><span class="o">:</span> <span class="nf">main.o</span>
    <span class="err">$(CC)</span> <span class="err">-o</span> <span class="err">main</span> <span class="err">main.o</span> <span class="err">$(LDFLAGS)</span>

<span class="nl">main.o</span><span class="o">:</span> <span class="nf">main.c</span>
    <span class="err">$(CC)</span> <span class="err">-c</span> <span class="err">main.c</span> <span class="err">$(CFLAGS)</span>

<span class="nl">clean</span><span class="o">:</span>
    <span class="err">rm</span> <span class="err">-f</span> <span class="err">main</span> <span class="err">main.o</span>
</code></pre></div> </div> </li> </ul> </li> <li><strong>Compile and Run Your Code</strong>: <ul> <li>Run the following commands in your terminal: <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make
./main
</code></pre></div> </div> </li> </ul> </li> </ol> <h3 id="explanation">Explanation</h3> <ul> <li><strong>CC</strong>: Specifies the compiler to use (gcc in this case).</li> <li><strong>CFLAGS</strong>: Specifies the include directory for the OpenGL headers.</li> <li><strong>LDFLAGS</strong>: Specifies the library directory and the libraries to link against (<code class="language-plaintext highlighter-rouge">-lGL</code>, <code class="language-plaintext highlighter-rouge">-lGLU</code>, <code class="language-plaintext highlighter-rouge">-lglut</code>).</li> <li><strong>all</strong>: The default target that builds the <code class="language-plaintext highlighter-rouge">main</code> executable.</li> <li><strong>main</strong>: The target that links the object file to create the executable.</li> <li><strong>main.o</strong>: The target that compiles the source file into an object file.</li> <li><strong>clean</strong>: A target to clean up the compiled files.</li> </ul> <p>This Makefile ensures that the compiler and linker use the correct paths for the OpenGL headers and libraries installed via conda.</p> <p>One problem is that how to get orders of drawing a same pixel when multiple circles overlap at the same pixel.</p> <p>The hints say that I can use prefix sum to help with this assignment but I don’t know how to do that.</p> <p>I know that once we have caculation order array for each pixel then we can parallize the image drawing for eall pixels in parallel.</p> <p>Take a look at shadePixel</p> <p>Run pixel rendering in parallel instead of rendering circles in parllel. <a href="https://github.com/ClaudiaRaffaelli/CUDA-Renderer/blob/master/cudaRenderer.cu">Ref repo</a></p> <p>The naive solution is slow because for each pixel thread it has to iterate all circles to see if each circle contributes to current pixel.</p> <p>The good news is that we don’t need to worry about the order issue and the correctness is guaranteed.</p> <p>I don’t know how prefix sum can be used to solve this problem yet.</p> <p><a href="https://github.com/MizukiCry/CS149/blob/c257598c102e438b9744ea133417e213188cd7ee/asst3/render/cudaRenderer.cu">Ref repo that use prefix sum to create unique offset for pixels of all circles so that each circles thread can draw pixels in correct order covered by all circles </a></p> <h3 id="first-naive-solution-render-all-pixels-in-parallel">First naive solution: render all pixels in parallel</h3> <p><a href="https://github.com/kykim0/asst3/blob/d0550cbd3a2037895f7a0bb7d1a52c3a40131fb1/render/cudaRenderer.cu">Ref implementation that uses similar idea to render pixel in parallel</a></p> <p>The naive solution is slow because for each pixel thread it has to iterate all circles to see if each circle contributes to current pixel.</p> <p>Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span>
<span class="n">CudaRenderer</span><span class="o">::</span><span class="n">render</span><span class="p">()</span> <span class="p">{</span>

    <span class="c1">// 256 threads per block is a healthy number</span>
    <span class="n">dim3</span> <span class="n">blockDim</span><span class="p">(</span><span class="n">THREADS_PER_BLOCK_X</span><span class="p">,</span> <span class="n">THREADS_PER_BLOCK_Y</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">gridDim</span><span class="p">((</span><span class="n">image</span><span class="o">-&gt;</span><span class="n">width</span><span class="o">+</span><span class="n">THREADS_PER_BLOCK_X</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">THREADS_PER_BLOCK_X</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="o">-&gt;</span><span class="n">height</span> <span class="o">+</span> <span class="n">THREADS_PER_BLOCK_Y</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">THREADS_PER_BLOCK_Y</span> <span class="p">);</span>

    <span class="n">kernelRenderPixels</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridDim</span><span class="p">,</span> <span class="n">blockDim</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>


<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernelRenderPixels</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">pixel_X</span> <span class="o">=</span>  <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">pixel_Y</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>


  <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">imageWidth</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">height</span> <span class="o">=</span> <span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">imageHeight</span><span class="p">;</span>

  <span class="c1">// float boxL = pixel_X -1;</span>
  <span class="c1">// float boxR = pixel_X +1;</span>
  <span class="c1">// float boxT = pixel_Y - 1;</span>
  <span class="c1">// float boxB = pixel_Y + 1;</span>


  <span class="k">if</span> <span class="p">(</span><span class="n">pixel_X</span> <span class="o">&gt;=</span> <span class="n">width</span> <span class="o">||</span> <span class="n">pixel_Y</span> <span class="o">&gt;=</span> <span class="n">height</span><span class="p">)</span>
      <span class="k">return</span><span class="p">;</span>

  <span class="n">float4</span><span class="o">*</span> <span class="n">imgPtr</span> <span class="o">=</span> <span class="p">(</span><span class="n">float4</span><span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">imageData</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">pixel_Y</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">pixel_X</span><span class="p">)]);</span>
  <span class="kt">int</span> <span class="n">num_circles</span> <span class="o">=</span> <span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">numCircles</span> <span class="p">;</span>

  <span class="kt">float</span> <span class="n">invWidth</span> <span class="o">=</span> <span class="mf">1.</span><span class="n">f</span> <span class="o">/</span> <span class="n">width</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">invHeight</span> <span class="o">=</span> <span class="mf">1.</span><span class="n">f</span> <span class="o">/</span> <span class="n">height</span><span class="p">;</span>

  <span class="n">float2</span> <span class="n">pixelCenterNorm</span> <span class="o">=</span> <span class="n">make_float2</span><span class="p">(</span><span class="n">invWidth</span> <span class="o">*</span> <span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">pixel_X</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="n">f</span><span class="p">),</span>
                                       <span class="n">invHeight</span> <span class="o">*</span> <span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">pixel_Y</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="n">f</span><span class="p">));</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">circle_index</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">circle_index</span><span class="o">&lt;</span> <span class="n">num_circles</span><span class="p">;</span> <span class="n">circle_index</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">circle_index3</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">circle_index</span><span class="p">;</span>  

    <span class="n">float3</span> <span class="n">p</span> <span class="o">=</span> <span class="o">*</span><span class="p">(</span><span class="n">float3</span><span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">position</span><span class="p">[</span><span class="n">circle_index3</span><span class="p">]);</span>
    <span class="c1">// float  rad = cuConstRendererParams.radius[circle_index];</span>
    <span class="n">shadePixel</span><span class="p">(</span><span class="n">circle_index</span><span class="p">,</span> <span class="n">pixelCenterNorm</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">imgPtr</span><span class="p">);</span>

  <span class="p">}</span>

<span class="p">}</span>
</code></pre></div></div> <p>Output:</p> <p>The output shows that render time reduces 17x with gpu.</p> <p>And it’s correct and passes the correctness check..</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@ln101%tianhe2-K render]$ yhrun -p gpu_v100 ./render -r cpuref rand10k
Rendering to 1024x1024 image
Loaded scene with 10000 circles

Running benchmark, 1 frames, beginning at frame 0 ...
Dumping frames to output_xxx.ppm
Wrote image file output_0000.ppm
Clear:    661.9973 ms
Advance:  0.0079 ms
Render:   390.4090 ms
Total:    1052.4143 ms
File IO:  61.6616 ms

Overall:  1.1314 sec (note units are seconds)



[nsccgz_qylin_1@ln101%tianhe2-K render]$ yhrun -p gpu_v100 ./render -r cuda rand10k
Rendering to 1024x1024 image
Loaded scene with 10000 circles
---------------------------------------------------------
Initializing CUDA for CudaRenderer
Found 4 CUDA devices
Device 0: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 1: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 2: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 3: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
---------------------------------------------------------

Running benchmark, 1 frames, beginning at frame 0 ...
Dumping frames to output_xxx.ppm
Copying image data from device
Wrote image file output_0000.ppm
Clear:    0.1329 ms
Advance:  0.0057 ms
Render:   23.3001 ms
Total:    23.4387 ms
File IO:  99.4054 ms

Overall:  0.1410 sec (note units are seconds)
</code></pre></div></div> <p>Correctness check</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@ln101%tianhe2-K render]$ yhrun -p gpu_v100 ./render -r cuda rand10k -c
Rendering to 1024x1024 image
Loaded scene with 10000 circles
Loaded scene with 10000 circles
---------------------------------------------------------
Initializing CUDA for CudaRenderer
Found 4 CUDA devices
Device 0: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 1: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 2: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 3: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
---------------------------------------------------------

Running benchmark, 1 frames, beginning at frame 0 ...
Dumping frames to output_xxx.ppm
Copying image data from device
Wrote image file output_0000.ppm
Copying image data from device
***************** Correctness check passed **************************
Clear:    0.1450 ms
Advance:  0.0058 ms
Render:   23.3115 ms
Total:    23.4624 ms
File IO:  69.5518 ms

Overall:  0.4857 sec (note units are seconds)
</code></pre></div></div>]]></content><author><name></name></author><category term="parallel"/><category term="programming"/><category term="parallel"/><category term="programming"/><summary type="html"><![CDATA[Part1 Code: saxpy serial cpu output from assign1 prog5: (base) ➜ prog5_saxpy git:(master) ✗ ./saxpy [saxpy serial]: [20.605] ms [14.464] GB/s [1.941] GFLOPS [saxpy ispc]: [17.866] ms [16.681] GB/s [2.239] GFLOPS [saxpy task ispc]: [3.122] ms [95.446] GB/s [12.810] GFLOPS (5.72x speedup from use of tasks)]]></summary></entry><entry><title type="html">Stf CS149 Parallel Programming - Lecture11 - Cache coherence</title><link href="https://bilyz98.github.io/blog/2024/cs149-lecture11-cache-coherence/" rel="alternate" type="text/html" title="Stf CS149 Parallel Programming - Lecture11 - Cache coherence"/><published>2024-11-01T11:59:00+00:00</published><updated>2024-11-01T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cs149-lecture11-cache-coherence</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cs149-lecture11-cache-coherence/"><![CDATA[<h2 id="lecture-11-cache-coherence">Lecture 11: Cache coherence</h2> <p>Cache takes large amount of area in the chip</p> <p>Cache bring performance boosting.</p> <p>Two locality of data access pattern so that cache can help with performance.</p> <ol> <li>Spatial locality</li> <li>Temporal locality</li> </ol> <p>Cache and cacheline in cpu. Usually a single cacheline contains multiple cache bytes(64bytes which can store 8 int for example ).</p> <p>What is the problem with a shared cache processor design?</p> <p>The scalability is the problem.</p> <p>The cache bus has bandwidth limit..</p> <p>A shared cache avoid the cache coherence problem but each write to a memory address will be broadcasted to other cores which waste bandwidth of bus.</p> <p>Shared cache contention example:</p> <p>Imagine you have a multi-core processor where two cores, Core 0 and Core 1, share the same Last Level Cache (LLC). If both cores are running different applications that frequently access and modify data stored in the shared cache, they will compete for the cache’s resources. This competition can lead to contention.</p> <p>For instance, consider two applications, App A and App B, running on Core 0 and Core 1, respectively. Both applications need to access large datasets that do not fit entirely in the cache. As App A accesses its data, it loads cache lines into the LLC, potentially evicting cache lines that App B needs. When App B tries to access its data, it may find that the required cache lines have been evicted by App A, causing cache misses and forcing App B to fetch the data from the slower main memory¹.</p> <p>This back-and-forth eviction and reloading of cache lines between the two applications degrade their performance compared to a scenario where each application has its own private cache¹.</p> <p>(2) 250P: Computer Systems Architecture Lecture 10: Caches - University of Utah. https://users.cs.utah.edu/~aburtsev/250P/2019fall/lectures/lecture10-caches/lecture10-caches.pdf.</p> <h3 id="constructive-and-destructive-interference">constructive and destructive interference</h3> <p>In the context of CPU caches, <strong>constructive interference</strong> and <strong>destructive interference</strong> refer to the effects of multiple processors accessing shared cache lines.</p> <h3 id="constructive-interference">Constructive Interference</h3> <p>Constructive interference occurs when multiple processors access the same data in a shared cache, leading to improved performance. For example, if one processor loads data into the shared cache that another processor also needs, the second processor can access this data quickly without having to fetch it from the slower main memory. This reduces cache misses and improves overall efficiency².</p> <h3 id="destructive-interference">Destructive Interference</h3> <p>Destructive interference, on the other hand, happens when multiple processors access different data that map to the same cache line, causing conflicts. This can lead to frequent cache line invalidations and reloads, increasing the number of cache misses and degrading performance. For instance, if two processors continuously overwrite each other’s data in the same cache line, they will experience higher latency due to the constant need to fetch data from the main memory².</p> <p>These concepts are crucial in designing efficient cache systems for multi-core processors, as they highlight the trade-offs between shared and private caches and the importance of cache coherence protocols.</p>]]></content><author><name></name></author><category term="parallel"/><category term="programming"/><category term="parallel"/><category term="programming"/><summary type="html"><![CDATA[Lecture 11: Cache coherence]]></summary></entry></feed>