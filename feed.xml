<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bilyz98.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bilyz98.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-24T06:10:49+00:00</updated><id>https://bilyz98.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Cs149 Lecture7 Cuda</title><link href="https://bilyz98.github.io/blog/2024/cs149-lecture7-cuda/" rel="alternate" type="text/html" title="Cs149 Lecture7 Cuda"/><published>2024-10-22T00:00:00+00:00</published><updated>2024-10-22T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cs149-lecture7-cuda</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cs149-lecture7-cuda/"><![CDATA[<p><a href="https://gfxcourses.stanford.edu/cs149/fall23/lecture/gpucuda/">Lecture 7 slides</a></p> <p><a href="https://www.youtube.com/watch?v=qQTDF0CBoxE&amp;list=PLoROMvodv4rMp7MTFr4hQsDEcX7Bx6Odp&amp;index=7&amp;pp=iAQB">Video lecture</a></p> <h2 id="cuda-programming-model--abstraction-">CUDA programming model ( abstraction )</h2> <p>Three execution unit and memory address</p> <ol> <li>thread</li> <li>thread block</li> <li>cuda kernel</li> </ol> <p>A thread block contains bunch of threads.</p> <p>A cuda kernal contains all the thread blocks.</p> <p>Memory address space</p> <ol> <li>Each thread has its own memory address space</li> <li>Each thread block has its own shared memory address space for all threads in the thread block</li> <li>All threads across all thread blocks share a process memory address space</li> </ol> <p><img src="https://github.com/user-attachments/assets/851fe0f2-52ec-4b7b-8a23-d3870982c520" alt="image"/></p> <p>Why this 3 level hierachy adress space ? For efficient memory access when threads in thread block are scheduled in the same core.</p> <h2 id="nvidia-gpu-implementation">Nvidia gpu (implementation)</h2> <p>A warp in nvidia gpu is a gropu of 32 threads in thread block. <img src="https://github.com/user-attachments/assets/e2f4aa55-103c-404b-828b-28d693b9c72b" alt="image"/></p> <p>Different CUDA thread has it own PC(Program counter) even though they are in the same warp.</p> <p>However, since all threads in the same warp is likely to execute the same code and same instructions it effectively looks like that there are only 4 unique PCs even though in reality there are 4 * 32 = 128 PCs.</p> <p>Difference between warp and thread block.</p> <p>A thread block is an programming model abstraction.</p> <p>A warp in hardware implementation.</p> <p>Both represent the concept of group of threads .</p> <p>sub-core has 4 warp in the diagram below.</p> <p>Each SM(streaming multi-processor) has 4 sub-core.</p> <p>V100 has 80 SMs in total.</p> <p>For V100, each SM(streaming multi-processor) has 4 sub-cores. <img src="https://github.com/user-attachments/assets/dbca936d-0da6-42fa-82d9-ceaf3d91596d" alt="image"/></p> <p>Instruction execution.</p> <p>Since we have more execution context than ALUs, each instructions is finished half of the work in one cycle and another half of the work in the next cycle.</p> <p><img src="https://github.com/user-attachments/assets/0531761c-6aef-437d-814a-095990d67950" alt="image"/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Lecture 7 slides]]></summary></entry><entry><title type="html">Cs149 Assign2</title><link href="https://bilyz98.github.io/blog/2024/cs149-assign2/" rel="alternate" type="text/html" title="Cs149 Assign2"/><published>2024-10-21T00:00:00+00:00</published><updated>2024-10-21T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cs149-assign2</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cs149-assign2/"><![CDATA[<h1 id="thread-pool-for-task-scheduling">Thread pool for task scheduling</h1> <h2 id="step1--parallel-thread-spawn">Step1 : parallel thread spawn</h2> <p>First implementaion that spawns new thread for each task:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">TaskSystemParallelSpawn</span><span class="o">::</span><span class="n">run</span><span class="p">(</span><span class="n">IRunnable</span><span class="o">*</span> <span class="n">runnable</span><span class="p">,</span> <span class="kt">int</span> <span class="n">num_total_tasks</span><span class="p">)</span> <span class="p">{</span>


    <span class="c1">//</span>
    <span class="c1">// TODO: CS149 students will modify the implementation of this</span>
    <span class="c1">// method in Part A.  The implementation provided below runs all</span>
    <span class="c1">// tasks sequentially on the calling thread.</span>
    <span class="c1">//</span>

    <span class="c1">// for (int i = 0; i &lt; num_total_tasks; i++) {</span>
    <span class="c1">//     runnable-&gt;runTask(i, num_total_tasks);</span>
    <span class="c1">// }</span>

  <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">workers</span><span class="p">[</span><span class="n">num_threads_</span><span class="p">];</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span> <span class="n">num_total_tasks</span><span class="p">;</span> <span class="n">i</span><span class="o">+=</span> <span class="n">num_threads_</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">cur_end</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="n">num_threads_</span> <span class="o">&lt;</span> <span class="n">num_total_tasks</span><span class="p">)</span> <span class="o">?</span> <span class="n">i</span><span class="o">+</span><span class="n">num_threads_</span> <span class="o">:</span> <span class="n">num_total_tasks</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">cur_begin</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">t_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="n">cur_begin</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">cur_end</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">,</span> <span class="n">t_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
       <span class="n">workers</span><span class="p">[</span><span class="n">t_idx</span><span class="p">]</span><span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span><span class="n">runTask</span><span class="p">,</span> <span class="n">runnable</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">num_total_tasks</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">t_idx</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="n">cur_begin</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">cur_end</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">,</span> <span class="n">t_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">workers</span><span class="p">[</span><span class="n">t_idx</span><span class="p">].</span><span class="n">join</span><span class="p">();</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>Result: Not good. Very slow. I think this is because of frequent thread spawn overhead.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  part_a git:(master) ✗ python3 ../tests/run_test_harness.py -n 8 -t  super_light super_super_light
runtasks_ref
Linux x86_64
================================================================================
Running task system grading harness... (2 total tests)
  - Detected CPU with 40 execution contexts
  - Task system configured to use at most 8 threads
================================================================================
================================================================================
Executing test: super_super_light...
Reference binary: ./runtasks_ref_linux
Results for: super_super_light
                                        STUDENT   REFERENCE   PERF?
[Serial]                                12.239    12.216      1.00  (OK)
[Parallel + Always Spawn]               444.19    53.469      8.31  (NOT OK)
[Parallel + Thread Pool + Spin]         12.121    25.723      0.47  (OK)
[Parallel + Thread Pool + Sleep]        12.106    24.976      0.48  (OK)
================================================================================
</code></pre></div></div> <p>Solution 2 : Created <code class="language-plaintext highlighter-rouge">num_threads</code> at the beginning of <code class="language-plaintext highlighter-rouge">run</code> call and reuse all of them. Use atomic counter to asggub unique id to each runnalbe call.</p> <p>Code :</p> <p>Notice that that we have to set <code class="language-plaintext highlighter-rouge">my_counter_=0</code> at the beginning of each run call.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">TaskSystemParallelSpawn</span><span class="o">::</span><span class="n">run</span><span class="p">(</span><span class="n">IRunnable</span><span class="o">*</span> <span class="n">runnable</span><span class="p">,</span> <span class="kt">int</span> <span class="n">num_total_tasks</span><span class="p">)</span> <span class="p">{</span> 
  <span class="n">my_counter_</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="o">&gt;</span> <span class="n">workers</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span> <span class="n">num_threads_</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">workers</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">([</span><span class="k">this</span><span class="p">,</span> <span class="n">runnable</span><span class="p">,</span> <span class="n">num_total_tasks</span><span class="p">]</span> <span class="p">{</span>
      <span class="k">this</span><span class="o">-&gt;</span><span class="n">threadTask</span><span class="p">(</span><span class="n">runnable</span><span class="p">,</span> <span class="n">num_total_tasks</span><span class="p">);</span>
    <span class="p">});</span>

  <span class="p">}</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_threads_</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">workers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">join</span><span class="p">();</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>===================================================================================
(base) ➜  part_a git:(master) ✗ python3 ../tests/run_test_harness.py -n 8 -t  super_light super_super_light
runtasks_ref
Linux x86_64
================================================================================
Running task system grading harness... (2 total tests)
  - Detected CPU with 40 execution contexts
  - Task system configured to use at most 8 threads
================================================================================
================================================================================
Executing test: super_super_light...
Reference binary: ./runtasks_ref_linux
Results for: super_super_light
                                        STUDENT   REFERENCE   PERF?
[Serial]                                8.562     12.451      0.69  (OK)
[Parallel + Always Spawn]               56.804    54.287      1.05  (OK)
[Parallel + Thread Pool + Spin]         8.532     25.417      0.34  (OK)
[Parallel + Thread Pool + Sleep]        8.456     24.996      0.34  (OK)
================================================================================
Executing test: super_light...
Reference binary: ./runtasks_ref_linux
Results for: super_light
                                        STUDENT   REFERENCE   PERF?
[Serial]                                81.044    95.194      0.85  (OK)
[Parallel + Always Spawn]               55.138    83.604      0.66  (OK)
[Parallel + Thread Pool + Spin]         80.753    27.388      2.95  (NOT OK)
[Parallel + Thread Pool + Sleep]        80.646    33.46       2.41  (NOT OK)
================================================================================
Overall performance results
[Serial]                                : All passed Perf
[Parallel + Always Spawn]               : All passed Perf
[Parallel + Thread Pool + Spin]         : Perf did not pass all tests
[Parallel + Thread Pool + Sleep]        : Perf did not pass all tests
</code></pre></div></div> <h2 id="step2--threadpool--spinning-waiting">Step2 : threadpool + spinning waiting</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>void TaskSystemParallelThreadPoolSpinning::threadTask() {
      // bool done = done_.load();
  while(!done_.load()) {
    // printf("id: %d\n", id);
    mutex_.lock();
    threadArg arg;
    if(!task_queue_.empty()) {
      arg = task_queue_.front();
      task_queue_.pop_front();
    }
    mutex_.unlock();
    if(arg.runnable) {
      arg.runnable-&gt;runTask( arg.task_id, arg.num_total_tasks);
      finished_tasks_.fetch_add(1);
    }
  }

}
TaskSystemParallelThreadPoolSpinning::TaskSystemParallelThreadPoolSpinning(int num_threads): ITaskSystem(num_threads) {
    //
    // TODO: CS149 student implementations may decide to perform setup
    // operations (such as thread pool construction) here.
    // Implementations are free to add new class member variables
    // (requiring changes to tasksys.h).
    //
  done_ = false;
  num_threads_ = num_threads;
  for(int i=0; i &lt; num_threads_; i++) {
    workers_.emplace_back([this ]{
      this-&gt;threadTask();
    });
  }

}

TaskSystemParallelThreadPoolSpinning::~TaskSystemParallelThreadPoolSpinning() {
  done_.store(true);
  for(int i=0; i&lt; num_threads_; i++) {
    workers_[i].join();
  }
}

void TaskSystemParallelThreadPoolSpinning::run(IRunnable* runnable, int num_total_tasks) {


    //
    // TODO: CS149 students will modify the implementation of this
    // method in Part A.  The implementation provided below runs all
    // tasks sequentially on the calling thread.
    //

  finished_tasks_ = 0;
  {
  const std::lock_guard&lt;std::mutex&gt; lock(mutex_);
  for(int i=0; i &lt; num_total_tasks; i++) {
      threadArg arg(i, num_total_tasks, runnable);
    task_queue_.emplace_back(arg);
  }
  }
  
  while(finished_tasks_.load() != num_total_tasks) {

  }

}
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  part_a git:(master) ✗ python3 ../tests/run_test_harness.py -n 8
runtasks_ref
Linux x86_64
================================================================================                              Running task system grading harness... (11 total tests)
- Detected CPU with 40 execution contexts
  - Task system configured to use at most 8 threads
================================================================================
================================================================================                                                                             Executing test: super_super_light...
Reference binary: ./runtasks_ref_linux
Results for: super_super_light
                                        STUDENT   REFERENCE   PERF?
[Serial]                                8.574     12.343      0.69  (OK)
[Parallel + Always Spawn]               56.938    54.949      1.04  (OK)
[Parallel + Thread Pool + Spin]         21.404    25.716      0.83  (OK)
[Parallel + Thread Pool + Sleep]        8.704     17.781      0.49  (OK)
================================================================================
Executing test: super_light...
Reference binary: ./runtasks_ref_linux
Results for: super_light
                                        STUDENT   REFERENCE   PERF?
[Serial]                                80.252    94.107      0.85  (OK)
[Parallel + Always Spawn]               73.338    83.35       0.88  (OK)
[Parallel + Thread Pool + Spin]         27.698    36.119      0.77  (OK)
[Parallel + Thread Pool + Sleep]        80.795    34.28       2.36  (NOT OK)
================================================================================
Executing test: ping_pong_equal...
Reference binary: ./runtasks_ref_linux
Results for: ping_pong_equal
                                        STUDENT   REFERENCE   PERF?
[Serial]                                1297.763  1553.482    0.84  (OK)
[Parallel + Always Spawn]               574.729   649.785     0.88  (OK)
[Parallel + Thread Pool + Spin]         187.65    232.88      0.81  (OK)
[Parallel + Thread Pool + Sleep]        1294.887  226.363     5.72  (NOT OK)
================================================================================
Executing test: ping_pong_unequal...
Reference binary: ./runtasks_ref_linux
Results for: ping_pong_unequal
                                        STUDENT   REFERENCE   PERF?
[Serial]                                2405.604  2401.768    1.00  (OK)
[Parallel + Always Spawn]               969.55    1021.323    0.95  (OK)
[Parallel + Thread Pool + Spin]         336.518   332.326     1.01  (OK)
[Parallel + Thread Pool + Sleep]        2396.002  338.008     7.09  (NOT OK)
================================================================================
Executing test: recursive_fibonacci...
Reference binary: ./runtasks_ref_linux
Results for: recursive_fibonacci
                                        STUDENT   REFERENCE   PERF?
[Serial]                                1578.621  1938.967    0.81  (OK)
[Parallel + Always Spawn]               482.274   548.912     0.88  (OK)
[Parallel + Thread Pool + Spin]         222.252   266.043     0.84  (OK)
[Parallel + Thread Pool + Sleep]        1583.164  266.454     5.94  (NOT OK)
================================================================================
Executing test: math_operations_in_tight_for_loop...
Reference binary: ./runtasks_ref_linux
Results for: math_operations_in_tight_for_loop
                                        STUDENT   REFERENCE   PERF?
[Serial]                                808.052   833.969     0.97  (OK)
[Parallel + Always Spawn]               651.691   705.552     0.92  (OK)
[Parallel + Thread Pool + Spin]         139.645   154.966     0.90  (OK)
[Parallel + Thread Pool + Sleep]        802.654   157.355     5.10  (NOT OK)
================================================================================
================================================================================
Executing test: math_operations_in_tight_for_loop_fewer_tasks...
Reference binary: ./runtasks_ref_linux
Results for: math_operations_in_tight_for_loop_fewer_tasks
                                        STUDENT   REFERENCE   PERF?
[Serial]                                806.914   839.226     0.96  (OK)
[Parallel + Always Spawn]               809.429   810.212     1.00  (OK)
[Parallel + Thread Pool + Spin]         215.631   238.903     0.90  (OK)
[Parallel + Thread Pool + Sleep]        805.115   244.607     3.29  (NOT OK)
================================================================================
Executing test: math_operations_in_tight_for_loop_fan_in...
Reference binary: ./runtasks_ref_linux
Results for: math_operations_in_tight_for_loop_fan_in
                                        STUDENT   REFERENCE   PERF?
[Serial]                                416.313   428.492     0.97  (OK)
[Parallel + Always Spawn]               133.057   160.116     0.83  (OK)
[Parallel + Thread Pool + Spin]         71.286    74.03       0.96  (OK)
[Parallel + Thread Pool + Sleep]        412.149   81.239      5.07  (NOT OK)
================================================================================
Executing test: math_operations_in_tight_for_loop_reduction_tree...
Reference binary: ./runtasks_ref_linux
Results for: math_operations_in_tight_for_loop_reduction_tree
                                        STUDENT   REFERENCE   PERF?
[Serial]                                415.842   428.485     0.97  (OK)
[Parallel + Always Spawn]               142.34    191.563     0.74  (OK)
[Parallel + Thread Pool + Spin]         70.203    65.066      1.08  (OK)
[Parallel + Thread Pool + Sleep]        411.849   67.231      6.13  (NOT OK)
================================================================================
Executing test: spin_between_run_calls...
Reference binary: ./runtasks_ref_linux
Results for: spin_between_run_calls
                                        STUDENT   REFERENCE   PERF?
[Serial]                                540.34    687.538     0.79  (OK)
[Parallel + Always Spawn]               290.111   364.247     0.80  (OK)
[Parallel + Thread Pool + Spin]         295.734   373.543     0.79  (OK)
[Parallel + Thread Pool + Sleep]        538.037   364.643     1.48  (NOT OK)
================================================================================
Executing test: mandelbrot_chunked...
Reference binary: ./runtasks_ref_linux
Results for: mandelbrot_chunked
                                        STUDENT   REFERENCE   PERF?
[Serial]                                526.695   537.581     0.98  (OK)
[Parallel + Always Spawn]               79.414    78.909      1.01  (OK)
[Parallel + Thread Pool + Spin]         83.16     77.74       1.07  (OK)
[Parallel + Thread Pool + Sleep]        530.464   85.143      6.23  (NOT OK)
================================================================================
Overall performance results
[Serial]                                : All passed Perf
[Parallel + Always Spawn]               : All passed Perf
[Parallel + Thread Pool + Spin]         : All passed Perf
[Parallel + Thread Pool + Sleep]        : Perf did not pass all tests
</code></pre></div></div> <h2 id="step3-threadpool--put-threads-to-sleep-when-there-is-nothing-to-do">Step3: Threadpool + put threads to sleep when there is nothing to do</h2> <p>Implementation :</p> <p>Notice that we have to test if each thread is joinable and call <code class="language-plaintext highlighter-rouge">join()</code> at the destructor of the class.</p> <p>Because each thread might have already exited when <code class="language-plaintext highlighter-rouge">done_</code> is set to <code class="language-plaintext highlighter-rouge">true</code></p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">TaskSystemParallelThreadPoolSleeping</span><span class="o">::</span><span class="n">threadTask</span><span class="p">(</span> <span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">num_task</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">done_</span><span class="p">.</span><span class="n">load</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex_</span><span class="p">);</span>
    <span class="n">threadArg</span> <span class="n">arg</span> <span class="p">;</span> 
    <span class="kt">bool</span> <span class="n">get_task</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
    <span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="n">task_queue_</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">arg</span> <span class="o">=</span> <span class="n">task_queue_</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
      <span class="n">task_queue_</span><span class="p">.</span><span class="n">pop_front</span><span class="p">();</span>
      <span class="n">get_task</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
      <span class="c1">// printf("get task\n");</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">cv_</span><span class="p">.</span><span class="n">wait</span><span class="p">(</span><span class="n">lock</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">lock</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
    <span class="k">if</span><span class="p">(</span><span class="n">get_task</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">arg</span><span class="p">.</span><span class="n">runnable</span><span class="o">-&gt;</span><span class="n">runTask</span><span class="p">(</span><span class="n">arg</span><span class="p">.</span><span class="n">task_id</span><span class="p">,</span> <span class="n">arg</span><span class="p">.</span><span class="n">num_total_tasks</span><span class="p">);</span>
      <span class="n">num_task</span> <span class="o">=</span> <span class="n">arg</span><span class="p">.</span><span class="n">num_total_tasks</span><span class="p">;</span>
      <span class="n">finished_tasks_</span><span class="p">.</span><span class="n">fetch_add</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
      <span class="k">if</span><span class="p">(</span><span class="n">finished_tasks_</span><span class="p">.</span><span class="n">load</span><span class="p">()</span> <span class="o">==</span> <span class="n">num_task</span><span class="p">)</span>  <span class="p">{</span>
        <span class="c1">// printf("send notify\n");</span>
        <span class="n">cv_</span><span class="p">.</span><span class="n">notify_all</span><span class="p">();</span>
      <span class="p">}</span>
    <span class="p">}</span>
 
  <span class="p">}</span>
<span class="p">}</span>
<span class="n">TaskSystemParallelThreadPoolSleeping</span><span class="o">::</span><span class="n">TaskSystemParallelThreadPoolSleeping</span><span class="p">(</span><span class="kt">int</span> <span class="n">num_threads</span><span class="p">)</span><span class="o">:</span> <span class="n">ITaskSystem</span><span class="p">(</span><span class="n">num_threads</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">//</span>
    <span class="c1">// TODO: CS149 student implementations may decide to perform setup</span>
    <span class="c1">// operations (such as thread pool construction) here.</span>
    <span class="c1">// Implementations are free to add new class member variables</span>
    <span class="c1">// (requiring changes to tasksys.h).</span>
    <span class="c1">//</span>
  <span class="n">num_threads_</span> <span class="o">=</span> <span class="n">num_threads</span><span class="p">;</span>
  <span class="n">done_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span> <span class="n">num_threads_</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">){</span>
    <span class="n">workers_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">([</span><span class="k">this</span><span class="p">]{</span>
      <span class="k">this</span><span class="o">-&gt;</span><span class="n">threadTask</span><span class="p">();</span>
    <span class="p">});</span>
  <span class="p">}</span>

<span class="p">}</span>

<span class="n">TaskSystemParallelThreadPoolSleeping</span><span class="o">::~</span><span class="n">TaskSystemParallelThreadPoolSleeping</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">//</span>
    <span class="c1">// TODO: CS149 student implementations may decide to perform cleanup</span>
    <span class="c1">// operations (such as thread pool shutdown construction) here.</span>
    <span class="c1">// Implementations are free to add new class member variables</span>
    <span class="c1">// (requiring changes to tasksys.h).</span>
    <span class="c1">//</span>
  <span class="n">done_</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
  <span class="n">cv_</span><span class="p">.</span><span class="n">notify_all</span><span class="p">();</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_threads_</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span><span class="p">(</span><span class="n">workers_</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">joinable</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">workers_</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">join</span><span class="p">();</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">TaskSystemParallelThreadPoolSleeping</span><span class="o">::</span><span class="n">run</span><span class="p">(</span><span class="n">IRunnable</span><span class="o">*</span> <span class="n">runnable</span><span class="p">,</span> <span class="kt">int</span> <span class="n">num_total_tasks</span><span class="p">)</span> <span class="p">{</span>


    <span class="c1">//</span>
    <span class="c1">// TODO: CS149 students will modify the implementation of this</span>
    <span class="c1">// method in Parts A and B.  The implementation provided below runs all</span>
    <span class="c1">// tasks sequentially on the calling thread.</span>
    <span class="c1">//</span>

  <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex_</span><span class="p">);</span>
  <span class="n">finished_tasks_</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_total_tasks</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">threadArg</span> <span class="n">arg</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">num_total_tasks</span><span class="p">,</span> <span class="n">runnable</span><span class="p">);</span>
    <span class="n">task_queue_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">arg</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="c1">// printf("before notify all\n");</span>
  <span class="n">lock</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
  <span class="n">cv_</span><span class="p">.</span><span class="n">notify_all</span><span class="p">();</span>
  <span class="n">lock</span><span class="p">.</span><span class="n">lock</span><span class="p">();</span>

  
  <span class="k">while</span><span class="p">(</span><span class="n">finished_tasks_</span><span class="p">.</span><span class="n">load</span><span class="p">()</span> <span class="o">!=</span> <span class="n">num_total_tasks</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">cv_</span><span class="p">.</span><span class="n">wait</span><span class="p">(</span><span class="n">lock</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="c1">// printf("finished_tasks_:%d\n", finished_tasks_.load());</span>

<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Cs149 Assign1 Prog3</title><link href="https://bilyz98.github.io/blog/2024/cs149-assign1-prog3/" rel="alternate" type="text/html" title="Cs149 Assign1 Prog3"/><published>2024-10-20T00:00:00+00:00</published><updated>2024-10-20T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cs149-assign1-prog3</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cs149-assign1-prog3/"><![CDATA[<h2 id="speedup-with-ispc">Speedup with ISPC</h2> <p>launching 80 tasks brings 62x speedup.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">export</span> <span class="kt">void</span> <span class="nf">mandelbrot_ispc_withtasks</span><span class="p">(</span><span class="n">uniform</span> <span class="kt">float</span> <span class="n">x0</span><span class="p">,</span> <span class="n">uniform</span> <span class="kt">float</span> <span class="n">y0</span><span class="p">,</span>
                                      <span class="n">uniform</span> <span class="kt">float</span> <span class="n">x1</span><span class="p">,</span> <span class="n">uniform</span> <span class="kt">float</span> <span class="n">y1</span><span class="p">,</span>
                                      <span class="n">uniform</span> <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="n">uniform</span> <span class="kt">int</span> <span class="n">height</span><span class="p">,</span>
                                      <span class="n">uniform</span> <span class="kt">int</span> <span class="n">maxIterations</span><span class="p">,</span>
                                      <span class="n">uniform</span> <span class="kt">int</span> <span class="n">output</span><span class="p">[])</span>
<span class="p">{</span>

    <span class="n">uniform</span> <span class="kt">int</span> <span class="n">rowsPerTask</span> <span class="o">=</span> <span class="n">height</span> <span class="o">/</span> <span class="mi">80</span><span class="p">;</span>

    <span class="c1">// create 2 tasks</span>
    <span class="n">launch</span><span class="p">[</span><span class="mi">80</span><span class="p">]</span> <span class="n">mandelbrot_ispc_task</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span>
                                     <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span>
                                     <span class="n">rowsPerTask</span><span class="p">,</span>
                                     <span class="n">maxIterations</span><span class="p">,</span>
                                     <span class="n">output</span><span class="p">);</span> 
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog3_mandelbrot_ispc git:(master) ✗ ./mandelbrot_ispc  --tasks
[mandelbrot serial]:            [268.624] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [55.141] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [4.328] ms
Wrote image file mandelbrot-task-ispc.ppm
                                (4.87x speedup from ISPC)
                                (62.07x speedup from task ISPC)
</code></pre></div></div> <p>Speedup for different image generation task with same task parallelism settings is different</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog3_mandelbrot_ispc git:(master) ✗ ./mandelbrot_ispc  --tasks  --view 0
[mandelbrot serial]:            [267.687] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [54.364] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [4.698] ms
Wrote image file mandelbrot-task-ispc.ppm                                                                                                                                                    (4.92x speedup from ISPC)                                                                                                                                    
(56.97x speedup from task ISPC)

(base) ➜  prog3_mandelbrot_ispc git:(master) ✗ ./mandelbrot_ispc  --tasks  --view 1
[mandelbrot serial]:            [266.777] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [53.877] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [5.287] ms
Wrote image file mandelbrot-task-ispc.ppm
                                (4.95x speedup from ISPC)
                                (50.46x speedup from task ISPC)

(base) ➜  prog3_mandelbrot_ispc git:(master) ✗ ./mandelbrot_ispc  --tasks  --view 2
[mandelbrot serial]:            [159.744] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [37.937] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [5.020] ms
Wrote image file mandelbrot-task-ispc.ppm                                                                                                                                                    (4.21x speedup from ISPC)                                                                                                                                    (31.82x speedup from task ISPC)
</code></pre></div></div> <h2 id="difference-between-launch-and-foreach-in-ispc">Difference between launch and foreach in ISPC</h2> <p>TLDR; both <code class="language-plaintext highlighter-rouge">launch</code> and <code class="language-plaintext highlighter-rouge">foreach</code> can do task parallelism. But <code class="language-plaintext highlighter-rouge">foreach</code> can use SIMD and <code class="language-plaintext highlighter-rouge">launch</code> can not do that itself.</p> <p>The difference between <code class="language-plaintext highlighter-rouge">launch</code> and <code class="language-plaintext highlighter-rouge">foreach</code> in ISPC lies in the type of parallelism they express and how they are used:</p> <ul> <li><strong><code class="language-plaintext highlighter-rouge">launch</code></strong>: This keyword is used to create a task that runs independently. When you use <code class="language-plaintext highlighter-rouge">launch</code>, you’re telling ISPC to execute a function asynchronously, potentially on a different core. This is useful for task parallelism, where you have separate tasks that can run concurrently. For example: <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">launch</span> <span class="nf">myFunction</span><span class="p">();</span>
</code></pre></div> </div> <p>This will execute <code class="language-plaintext highlighter-rouge">myFunction</code> as a separate task.</p> </li> <li><strong><code class="language-plaintext highlighter-rouge">foreach</code></strong>: This construct is used for data parallelism, where the same operation is applied to multiple data elements in parallel. The <code class="language-plaintext highlighter-rouge">foreach</code> loop allows ISPC to execute iterations in parallel, utilizing SIMD vector units. For example: <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">foreach</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">...</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// loop body</span>
<span class="p">}</span>
</code></pre></div> </div> <p>This will execute the loop body in parallel for each value of <code class="language-plaintext highlighter-rouge">i</code> from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">N-1</code>.</p> </li> </ul> <p>In summary, <code class="language-plaintext highlighter-rouge">launch</code> is used for task parallelism to run independent tasks concurrently, while <code class="language-plaintext highlighter-rouge">foreach</code> is used for data parallelism to apply the same operation to multiple data elements in parallel. Both constructs help you write parallel code, but they are suited for different types of parallelism. 😊</p> <p>The <code class="language-plaintext highlighter-rouge">launch</code> construct in ISPC is primarily designed for task parallelism, which means it can run tasks in parallel on multiple cores. However, <code class="language-plaintext highlighter-rouge">launch</code> itself does not directly utilize SIMD parallelism.</p> <p>Here’s a more detailed breakdown:</p> <ul> <li> <p><strong><code class="language-plaintext highlighter-rouge">launch</code></strong>: When you use <code class="language-plaintext highlighter-rouge">launch</code>, ISPC creates a new task that can run concurrently with other tasks. These tasks can be scheduled on different CPU cores, allowing for parallel execution. The focus of <code class="language-plaintext highlighter-rouge">launch</code> is on task-level parallelism, not on vectorization or SIMD parallelism.</p> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">foreach</code></strong>: On the other hand, <code class="language-plaintext highlighter-rouge">foreach</code> is designed for data parallelism and can utilize both SIMD and multi-core parallelism. It vectorizes the loop to use SIMD instructions and also distributes the iterations across multiple cores.</p> </li> </ul> <p>While <code class="language-plaintext highlighter-rouge">launch</code> does not directly use SIMD, the tasks that are launched can themselves contain <code class="language-plaintext highlighter-rouge">foreach</code> loops or other code that uses SIMD parallelism. So, you can combine <code class="language-plaintext highlighter-rouge">launch</code> and <code class="language-plaintext highlighter-rouge">foreach</code> to achieve a mix of task parallelism and data parallelism. For example:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">taskFunction</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">data</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">foreach</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">...</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">launch</span> <span class="nf">taskFunction</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</code></pre></div></div> <p>In this example, <code class="language-plaintext highlighter-rouge">taskFunction</code> uses <code class="language-plaintext highlighter-rouge">foreach</code> to process data in parallel using SIMD, and <code class="language-plaintext highlighter-rouge">launch</code> runs <code class="language-plaintext highlighter-rouge">taskFunction</code> as a separate task on a different core.</p> <p>Would you like to explore more about how to effectively combine <code class="language-plaintext highlighter-rouge">launch</code> and <code class="language-plaintext highlighter-rouge">foreach</code> in ISPC, or have any other questions? 😊</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Cs149 Assign1 Prog4</title><link href="https://bilyz98.github.io/blog/2024/cs149-assign1-prog4/" rel="alternate" type="text/html" title="Cs149 Assign1 Prog4"/><published>2024-10-20T00:00:00+00:00</published><updated>2024-10-20T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cs149-assign1-prog4</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cs149-assign1-prog4/"><![CDATA[<h2 id="change-values-array-to-change-speed-up-of-sqrt">Change values array to change speed up of sqrt</h2> <p>The best case is that all simd vectors handles the same value. And we would get max speed up if each SIMD vector takes long time to finish which reduce the percentage of sync and communication/ scheduling overhead brought by ispc.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog4_sqrt git:(master) ✗ ./sqrt;
[sqrt serial]:          [2062.501] ms
[sqrt ispc]:            [332.707] ms
[sqrt task ispc]:       [14.827] ms                                                                                                                                                          (6.20x speedup from ISPC)                                                                                                                                    (139.11x speedup from task ISPC)
</code></pre></div></div> <p>The worst case is that one value in vector takes extremely long time to finish and all other values in vector take shortest time to finish.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog4_sqrt git:(master) ✗ ./sqrt;
[sqrt serial]:          [286.506] ms
[sqrt ispc]:            [330.483] ms
[sqrt task ispc]:       [14.916] ms
                                (0.87x speedup from ISPC)
                                (19.21x speedup from task ISPC)
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    for (unsigned int i=0; i&lt;N; i++)
    {
        // TODO: CS149 students.  Attempt to change the values in the
        // array here to meet the instructions in the handout: we want
        // to you generate best and worse-case speedups
        
      // best case
    values[i] = 2.998f;

    // worst case
    // if(i%8 == 0) {
    //   values[i] = 2.998f;
    // } else {
    //   values[i] = 1.f;
    // }
    //
    //random:
        // starter code populates array with random input values
        // values[i] = .001f + 2.998f * static_cast&lt;float&gt;(rand()) / RAND_MAX;
    }


</code></pre></div></div> <p>Reference:</p> <p><a href="https://github.com/PKUFlyingPig/asst1/blob/master/prog4_sqrt/main.cpp">https://github.com/PKUFlyingPig/asst1/blob/master/prog4_sqrt/main.cpp</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Cs149 Assign1 Prog5</title><link href="https://bilyz98.github.io/blog/2024/cs149-assign1-prog5/" rel="alternate" type="text/html" title="Cs149 Assign1 Prog5"/><published>2024-10-20T00:00:00+00:00</published><updated>2024-10-20T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cs149-assign1-prog5</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cs149-assign1-prog5/"><![CDATA[<p>Extra Credit: (1 point) Note that the total memory bandwidth consumed computation in main.cpp is TOTAL_BYTES = 4 * N * sizeof(float);. Even though saxpy loads one element from X, one element from Y, and writes one element to result the multiplier by 4 is correct. Why is this the case? (Hint, think about how CPU caches work.)</p> <p>Answer: It’s because saxpy fetch 4 vector variables through the memory.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">saxpySerial</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span>
                       <span class="kt">float</span> <span class="n">scale</span><span class="p">,</span>
                       <span class="kt">float</span> <span class="n">X</span><span class="p">[],</span>
                       <span class="kt">float</span> <span class="n">Y</span><span class="p">[],</span>
                       <span class="kt">float</span> <span class="n">result</span><span class="p">[])</span>
<span class="p">{</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div> <p>Extra Credit: (points handled on a case-by-case basis) Improve the performance of saxpy. We’re looking for a significant speedup here, not just a few percentage points. If successful, describe how you did it and what a best-possible implementation on these systems might achieve. Also, if successful, come tell the staff, we’ll be interested. ;-)</p> <p>Answer: I don’t know how to do that.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Parallel programming - SIMD intrinsics</title><link href="https://bilyz98.github.io/blog/2024/cs149-assign1-prog2/" rel="alternate" type="text/html" title="Parallel programming - SIMD intrinsics"/><published>2024-10-18T11:59:00+00:00</published><updated>2024-10-18T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cs149-assign1-prog2</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cs149-assign1-prog2/"><![CDATA[<p><a href="https://ark.intel.com/content/www/us/en/ark/products/192450/intel-xeon-gold-6230n-processor-27-5m-cache-2-30-ghz.html">cpu xeon 6230n</a> Instruction set extensions: Intel® SSE4.2, Intel® AVX, Intel® AVX2, Intel® AVX-512</p> <p><a href="https://ark.intel.com/content/www/us/en/ark/products/97129/intel-core-i7-7700k-processor-8m-cache-up-to-4-50-ghz.html">cpu i7-7700k</a> Instruction set extensions: Intel® SSE4.1, Intel® SSE4.2, Intel® AVX2</p> <p>Both support hyper-threading technology which means each hardware core has two processing threads per physical core.</p> <h2 id="prog2-vectorizing--code-using-simd-intrinsics">Prog2: Vectorizing code using SIMD intrinsics</h2> <p><a href="https://github.com/stanford-cs149/asst1?tab=readme-ov-file#program-2-vectorizing-code-using-simd-intrinsics-20-points">Assignment link</a></p> <p>Solution idea:</p> <p>Just translate the <code class="language-plaintext highlighter-rouge">clampedExpSerial</code> the code to use SIMD. Refer to this <code class="language-plaintext highlighter-rouge">absVector</code> and <code class="language-plaintext highlighter-rouge">absSerial</code> to see how translation works.</p> <p>To deal with situation that total number of loop iterations is not a multiple of SIMD vector width we can set <code class="language-plaintext highlighter-rouge">maskAll</code> at the beginning of the function so that only valid values of input vector is used for SIMD computation.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">VECTOR_WIDTH</span> <span class="o">&gt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">maskAll</span> <span class="o">=</span> <span class="n">_cs149_init_ones</span><span class="p">(</span><span class="n">remain_count</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">maskAll</span> <span class="o">=</span> <span class="n">_cs149_init_ones</span><span class="p">();</span>
    <span class="p">}</span>

</code></pre></div></div> <p>code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">clampedExpVector</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">values</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">exponents</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>

  <span class="c1">//</span>
  <span class="c1">// CS149 STUDENTS TODO: Implement your vectorized version of</span>
  <span class="c1">// clampedExpSerial() here.</span>
  <span class="c1">//</span>
  <span class="c1">// Your solution should work for any value of</span>
  <span class="c1">// N and VECTOR_WIDTH, not just when VECTOR_WIDTH divides N</span>
  <span class="c1">//</span>
  
  <span class="n">__cs149_vec_float</span> <span class="n">x</span><span class="p">;</span>
  <span class="n">__cs149_vec_int</span>   <span class="n">y_exponents</span><span class="p">;</span>
  <span class="n">__cs149_vec_float</span> <span class="n">result</span><span class="p">;</span>
  <span class="n">__cs149_vec_int</span> <span class="n">zeros_int</span> <span class="o">=</span> <span class="n">_cs149_vset_int</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="n">__cs149_vec_int</span> <span class="n">ones_int</span> <span class="o">=</span> <span class="n">_cs149_vset_int</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
  <span class="n">__cs149_vec_float</span> <span class="n">ones_float</span> <span class="o">=</span> <span class="n">_cs149_vset_float</span><span class="p">(</span><span class="mf">1.0</span><span class="n">f</span><span class="p">);</span>
  <span class="n">__cs149_mask</span> <span class="n">maskAll</span><span class="p">,</span> <span class="n">maskExponentIsZero</span><span class="p">,</span> <span class="n">maskExponentNotZero</span><span class="p">;</span>  
  <span class="kt">int</span> <span class="n">remain_count</span> <span class="o">=</span> <span class="n">N</span> <span class="o">%</span> <span class="n">VECTOR_WIDTH</span><span class="p">;</span>
  <span class="c1">// int first_valid_count = remain_count &gt; 0 ? VECTOR_WIDTH - remain_count : VECTOR_WIDTH;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">+=</span> <span class="n">VECTOR_WIDTH</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">VECTOR_WIDTH</span> <span class="o">&gt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">maskAll</span> <span class="o">=</span> <span class="n">_cs149_init_ones</span><span class="p">(</span><span class="n">remain_count</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">maskAll</span> <span class="o">=</span> <span class="n">_cs149_init_ones</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="c1">// CS149Logger.addLog("initones", maskAll, VECTOR_WIDTH);</span>

    <span class="n">maskExponentIsZero</span> <span class="o">=</span> <span class="n">_cs149_init_ones</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

    <span class="n">_cs149_vload_float</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">values</span><span class="o">+</span><span class="n">i</span><span class="p">,</span> <span class="n">maskAll</span><span class="p">);</span>
    <span class="n">_cs149_vload_int</span><span class="p">(</span><span class="n">y_exponents</span><span class="p">,</span> <span class="n">exponents</span><span class="o">+</span><span class="n">i</span><span class="p">,</span> <span class="n">maskAll</span><span class="p">);</span>

    <span class="c1">// if y== 0</span>
    <span class="n">_cs149_veq_int</span><span class="p">(</span><span class="n">maskExponentIsZero</span><span class="p">,</span> <span class="n">y_exponents</span><span class="p">,</span> <span class="n">zeros_int</span><span class="p">,</span> <span class="n">maskAll</span><span class="p">);</span>
    <span class="n">maskExponentNotZero</span> <span class="o">=</span> <span class="n">_cs149_mask_not</span><span class="p">(</span><span class="n">maskExponentIsZero</span><span class="p">);</span>

    <span class="c1">// x == 1  if y_exponents == 0</span>
    <span class="c1">//</span>
    <span class="n">_cs149_vmove_float</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">ones_float</span><span class="p">,</span> <span class="n">maskExponentIsZero</span><span class="p">);</span>      

    <span class="c1">// else </span>
    <span class="c1">// result = x;</span>
    <span class="n">_cs149_vmove_float</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">maskExponentNotZero</span><span class="p">);</span>
    <span class="c1">// count = y -1;</span>
    <span class="n">_cs149_vsub_int</span><span class="p">(</span><span class="n">y_exponents</span><span class="p">,</span> <span class="n">y_exponents</span><span class="p">,</span> <span class="n">ones_int</span><span class="p">,</span> <span class="n">maskExponentNotZero</span><span class="p">);</span>
    <span class="n">__cs149_mask</span> <span class="n">maskExpNotZeroCnt</span><span class="p">;</span>
    <span class="n">_cs149_vgt_int</span><span class="p">(</span><span class="n">maskExpNotZeroCnt</span><span class="p">,</span> <span class="n">y_exponents</span><span class="p">,</span>  <span class="n">zeros_int</span><span class="p">,</span> <span class="n">maskAll</span><span class="p">);</span>
    <span class="c1">// while (count &gt; 0)</span>
    <span class="c1">// result *= x ;</span>
    <span class="c1">// count--;</span>
    <span class="k">while</span><span class="p">(</span><span class="n">_cs149_cntbits</span><span class="p">(</span><span class="n">maskExpNotZeroCnt</span><span class="p">))</span> <span class="p">{</span>
      <span class="n">_cs149_vmult_float</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">maskExpNotZeroCnt</span><span class="p">);</span>

      <span class="n">_cs149_vsub_int</span><span class="p">(</span><span class="n">y_exponents</span><span class="p">,</span> <span class="n">y_exponents</span><span class="p">,</span> <span class="n">ones_int</span><span class="p">,</span> <span class="n">maskExponentNotZero</span><span class="p">);</span>
      <span class="n">_cs149_vgt_int</span><span class="p">(</span><span class="n">maskExpNotZeroCnt</span><span class="p">,</span> <span class="n">y_exponents</span><span class="p">,</span>  <span class="n">zeros_int</span><span class="p">,</span> <span class="n">maskAll</span><span class="p">);</span>
    <span class="p">}</span>


    <span class="c1">// if( result &gt; 9.999999f) {</span>
    <span class="c1">// result = 9.999999f;</span>
    <span class="c1">// }</span>
    <span class="n">__cs149_mask</span> <span class="n">mask_gt_9</span><span class="p">;</span>
    <span class="n">__cs149_vec_float</span> <span class="n">nine_float</span> <span class="o">=</span> <span class="n">_cs149_vset_float</span><span class="p">(</span><span class="mf">9.999999</span><span class="n">f</span><span class="p">);</span>
    <span class="n">_cs149_vgt_float</span><span class="p">(</span><span class="n">mask_gt_9</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span>  <span class="n">nine_float</span><span class="p">,</span> <span class="n">maskAll</span><span class="p">);</span>
    <span class="n">_cs149_vmove_float</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">nine_float</span><span class="p">,</span> <span class="n">mask_gt_9</span><span class="p">);</span>

    <span class="c1">// output[i] = result;</span>
    <span class="n">_cs149_vstore_float</span><span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">maskAll</span><span class="p">);</span>

    
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>Run ./myexp -s 10000 and sweep the vector width from 2, 4, 8, to 16. Record the resulting vector utilization. You can do this by changing the #define VECTOR_WIDTH value in CS149intrin.h. Does the vector utilization increase, decrease or stay the same as VECTOR_WIDTH changes? Why?</p> <p>Answer: The vector utilization decrease as <code class="language-plaintext highlighter-rouge">VECTOR_WIDTH</code> increases. The reason I think it’s that not all values in vector is used for computation as <code class="language-plaintext highlighter-rouge">VECTOR_WIDTH</code> increases.</p> <p>So it’s not a very good idea to have very large <code class="language-plaintext highlighter-rouge">VECTOR_WIDTH</code>?</p> <p>vector width 2:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog2_vecintrin git:(master) ✗ ./myexp -s 10000
CLAMPED EXPONENT (required)
Results matched with answer!
****************** Printing Vector Unit Statistics *******************
Vector Width:              2
Total Vector Instructions: 167727
Vector Utilization:        88.7%
Utilized Vector Lanes:     297685
Total Vector Lanes:        335454
************************ Result Verification *************************
Passed!!!
</code></pre></div></div> <p>vector width 4:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog2_vecintrin git:(master) ✗ ./myexp -s 10000
CLAMPED EXPONENT (required)
Results matched with answer!
****************** Printing Vector Unit Statistics *******************
Vector Width:              4
Total Vector Instructions: 97075
Vector Utilization:        86.2%
Utilized Vector Lanes:     334817
Total Vector Lanes:        388300
************************ Result Verification *************************
Passed!!!
</code></pre></div></div> <p>vector width 8:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog2_vecintrin git:(master) ✗ ./myexp -s 10000
CLAMPED EXPONENT (required)
Results matched with answer!
****************** Printing Vector Unit Statistics *******************
Vector Width:              8
Total Vector Instructions: 52877
Vector Utilization:        85.0%
Utilized Vector Lanes:     359535
Total Vector Lanes:        423016
************************ Result Verification *************************
Passed!!!
</code></pre></div></div> <p>vector width 16:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog2_vecintrin git:(master) ✗ ./myexp -s 10000
CLAMPED EXPONENT (required)
Results matched with answer!
****************** Printing Vector Unit Statistics *******************
Vector Width:              16
Total Vector Instructions: 27592
Vector Utilization:        84.4%
Utilized Vector Lanes:     372781
Total Vector Lanes:        441472
************************ Result Verification *************************
Passed!!!
</code></pre></div></div>]]></content><author><name></name></author><category term="parallel"/><category term="programming"/><category term="parallel"/><category term="programming"/><summary type="html"><![CDATA[cpu xeon 6230n Instruction set extensions: Intel® SSE4.2, Intel® AVX, Intel® AVX2, Intel® AVX-512]]></summary></entry><entry><title type="html">Stf Cs149 Lecture Takeaway</title><link href="https://bilyz98.github.io/blog/2024/stf-cs149-lecture-takeaway/" rel="alternate" type="text/html" title="Stf Cs149 Lecture Takeaway"/><published>2024-10-18T00:00:00+00:00</published><updated>2024-10-18T00:00:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/stf-cs149-lecture-takeaway</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/stf-cs149-lecture-takeaway/"><![CDATA[<h2 id="lecture-5">Lecture 5</h2> <p><a href="https://youtu.be/mmO2Ri_dJkk?si=CCG3Tf9dDYZiExq6">Video</a> Deciding granularity is important for dynamic scheduling in parallel programming.</p> <p>Small granularity leads to better workload distribution but comes with higher synchronization overhead.</p> <h2 id="lecture-6">Lecture 6</h2> <p>Performance optimization: locality, communication and contention.</p> <p>Reduce costs of communication between:</p> <ol> <li>processors.</li> <li>between processors and memory.</li> </ol> <p>Shared memory communication. Numa: non-uniform memory access</p> <p>Message passing blocking send and non-blocking send</p> <p>Reduce communication is important to achieve max utilization of cpu. Just to keep cpu busy</p> <p>Roofline model: <img src="https://github.com/user-attachments/assets/da01cc6b-a009-4b49-a306-c72940b89eaf" alt="image"/></p> <p>To achieve maximum computation throughput GFLOPS/s of cpu or gpus one has to have algorithm that has high operation intensity -&gt; high flops/bytes.</p> <p>Need to has many computation per byte access unit.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">ssh display image on local server</title><link href="https://bilyz98.github.io/blog/2024/ssh-display-img/" rel="alternate" type="text/html" title="ssh display image on local server"/><published>2024-10-17T11:59:00+00:00</published><updated>2024-10-17T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/ssh-display-img</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/ssh-display-img/"><![CDATA[<p>To display an image on your local machine after using the <code class="language-plaintext highlighter-rouge">ssh -Y</code> command, you’ll need to have an image viewer installed on the remote server. Here’s a step-by-step example:</p> <ol> <li><strong>Connect to the remote server with X11 forwarding</strong>: <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-Y</span> user@remote-server
</code></pre></div> </div> </li> <li><strong>Check if an image viewer is installed</strong>: <ul> <li>Common image viewers include <code class="language-plaintext highlighter-rouge">eog</code> (Eye of GNOME), <code class="language-plaintext highlighter-rouge">display</code> (ImageMagick), and <code class="language-plaintext highlighter-rouge">xdg-open</code>.</li> <li>You can check if one of these is installed by running, for example: <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>which eog
</code></pre></div> </div> <p>If it’s installed, the command will return the path to the executable. If not, you might need to install it using the package manager for your distribution.</p> </li> </ul> </li> <li><strong>Display an image</strong>: <ul> <li>Assuming you have an image file named <code class="language-plaintext highlighter-rouge">example.png</code> in your home directory on the remote server, you can display it using: <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eog ~/example.png
</code></pre></div> </div> </li> <li>This command will open the image using the Eye of GNOME image viewer, and the image should be displayed on your local machine.</li> </ul> </li> </ol> <p>Remember, the exact command to display an image will depend on the image viewers available on the remote server. If you’re not sure which image viewers are installed, you can try running the command for a few common ones until you find one that works. Also, ensure that your local machine is running an X server (like XQuartz on macOS or Xming on Windows) to handle the forwarded X11 display.</p>]]></content><author><name></name></author><category term="ssh"/><category term="ssh"/><summary type="html"><![CDATA[To display an image on your local machine after using the ssh -Y command, you’ll need to have an image viewer installed on the remote server. Here’s a step-by-step example:]]></summary></entry><entry><title type="html">Parallel programming - round robin assignment</title><link href="https://bilyz98.github.io/blog/2024/stf-cs149-parallel-programming-assignment1/" rel="alternate" type="text/html" title="Parallel programming - round robin assignment"/><published>2024-10-17T11:59:00+00:00</published><updated>2024-10-17T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/stf-cs149-parallel-programming-assignment1</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/stf-cs149-parallel-programming-assignment1/"><![CDATA[<h2 id="program1-generate-image-with-multiple-threads">Program1: generate image with multiple threads.</h2> <p>Hardware: hyperthreads?</p> <p>Code: partition the image generation task.</p> <p>Plot speedup line with number of threads as x axis.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 2
[mandelbrot serial]:            [525.650] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot thread]:            [280.569] ms
Wrote image file mandelbrot-thread.ppm
                                (1.87x speedup from 2 threads)
(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 3
Wrote image file mandelbrot-serial.ppm
[mandelbrot thread]:            [341.063] ms
Wrote image file mandelbrot-thread.ppm                                                                                                                                                       (1.56x speedup from 3 threads)
(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 4
[mandelbrot serial]:            [531.202] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot thread]:            [237.027] ms
Wrote image file mandelbrot-thread.ppm                                                                                                                                                       (2.24x speedup from 4 threads)
(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 5
[mandelbrot serial]:            [532.980] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot thread]:            [213.517] ms
Wrote image file mandelbrot-thread.ppm                                                                                                                                                       (2.50x speedup from 5 threads)
(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 6
[mandelbrot serial]:            [531.480] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot thread]:            [182.457] ms
Wrote image file mandelbrot-thread.ppm                                                                                                                                                       (2.91x speedup from 6 threads)
(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 7
[mandelbrot serial]:            [530.595] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot thread]:            [173.007] ms
Wrote image file mandelbrot-thread.ppm
Mismatch : [1197][142], Expected : 1, Actual : 0
Error : Output from threads does not match serial output
(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 8
[mandelbrot serial]:            [534.651] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot thread]:            [150.080] ms
Wrote image file mandelbrot-thread.ppm                                                                                                                                                       (3.56x speedup from 8 threads)
</code></pre></div></div> <p>Time for each thread</p> <p>Not all threads run with same finish time.</p> <p>Why is that?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 2
[mandelbrot serial]:            [529.644] ms
Wrote image file mandelbrot-serial.ppm
exe time: 275.106996 ms
exe time: 294.014979 ms
exe time: 278.083589 ms
exe time: 280.188169 ms
exe time: 268.240355 ms
exe time: 288.978558 ms
exe time: 274.672702 ms
exe time: 285.212621 ms
exe time: 275.313959 ms
exe time: 291.359153 ms
[mandelbrot thread]:            [280.327] ms
Wrote image file mandelbrot-thread.ppm                                                                                                                                                       
(1.89x speedup from 2 threads)


(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 3
[mandelbrot serial]:            [532.551] ms
Wrote image file mandelbrot-serial.ppm
exe time: 120.048959 ms
exe time: 127.574176 ms
exe time: 346.292444 ms
exe time: 122.336693 ms
exe time: 122.885458 ms
exe time: 342.198521 ms
exe time: 123.949669 ms
exe time: 123.917334 ms
exe time: 343.334582 ms
exe time: 121.276554 ms
exe time: 121.796411 ms
exe time: 339.319866 ms
exe time: 122.690346 ms
exe time: 123.405423 ms
exe time: 341.921013 ms
[mandelbrot thread]:            [339.491] ms
Wrote image file mandelbrot-thread.ppm
                                (1.57x speedup from 3 threads)


(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 4
[mandelbrot serial]:            [532.573] ms
Wrote image file mandelbrot-serial.ppm
exe time: 66.314548 ms
exe time: 69.146506 ms
exe time: 236.007646 ms
exe time: 236.860119 ms
exe time: 67.293212 ms
exe time: 68.643764 ms
exe time: 235.531762 ms
exe time: 235.957604 ms
exe time: 67.872606 ms
exe time: 68.252590 ms
exe time: 231.048137 ms
exe time: 236.915503 ms
exe time: 68.757534 ms
exe time: 70.160590 ms
exe time: 219.524853 ms
exe time: 238.315675 ms
exe time: 66.293016 ms
exe time: 66.733379 ms
exe time: 233.316239 ms
exe time: 234.051295 ms
[mandelbrot thread]:            [234.236] ms
Wrote image file mandelbrot-thread.ppm
                                (2.27x speedup from 4 threads)


(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 8
[mandelbrot serial]:            [533.956] ms
Wrote image file mandelbrot-serial.ppm
exe time: 21.111727 ms
exe time: 21.316495 ms
exe time: 59.669584 ms
exe time: 59.972882 ms
exe time: 101.592321 ms
exe time: 104.972839 ms
exe time: 144.783191 ms
exe time: 145.647489 ms
[mandelbrot thread]:            [133.506] ms
Wrote image file mandelbrot-thread.ppm
                                (4.00x speedup from 8 threads)
</code></pre></div></div> <h2 id="more-efficient-implementation">More efficient implementation</h2> <p>cacheline aware ?</p> <p>In this code image is divided by <code class="language-plaintext highlighter-rouge">numThreads</code> blocks and each thread with <code class="language-plaintext highlighter-rouge">threadId</code> accesses idx:threadId of each block.</p> <p>This means that it’s highly likely that at the same moment each thread access the same memory block that is in cache.</p> <p>This is just my understanding.</p> <p>The reason for some threads taking much longer time to finish the job is that some adjacent rows need much more time to compute. If we use round-robin assignment strategy then we can distribute the computation job evenly and each thread can get equal amount of computation job.</p> <p>GPT give me following answer when I asked it why to use round-robin assignment:</p> <p>Thread 2 has one more row to process than the other threads, which can lead to a slight imbalance. However, the imbalance becomes more pronounced if the work done per row is not uniform. For example, if the computation for the Mandelbrot set is more complex for certain rows, the threads processing those rows will take longer to complete, leading to idle time for other threads.</p> <p>This imbalance can be avoided by using a better load balancing strategy, such as the round-robin assignment used in the original code, which distributes the rows more evenly across the threads. 😊</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">workerThreadStart</span><span class="p">(</span><span class="n">WorkerArgs</span> <span class="o">*</span> <span class="k">const</span> <span class="n">args</span><span class="p">)</span> <span class="p">{</span>

  <span class="c1">// TODO FOR CS149 STUDENTS: Implement the body of the worker</span>
  <span class="c1">// thread here. Each thread should make a call to mandelbrotSerial()</span>
  <span class="c1">// to compute a part of the output image.  For example, in a</span>
  <span class="c1">// program that uses two threads, thread 0 could compute the top</span>
  <span class="c1">// half of the image and thread 1 could compute the bottom half.</span>
  <span class="c1">//</span>
  <span class="kt">double</span> <span class="n">startTime</span> <span class="o">=</span> <span class="n">CycleTimer</span><span class="o">::</span><span class="n">currentSeconds</span><span class="p">();</span>
  <span class="kt">float</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">args</span><span class="o">-&gt;</span><span class="n">x0</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">y0</span> <span class="o">=</span> <span class="n">args</span><span class="o">-&gt;</span><span class="n">y0</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">args</span><span class="o">-&gt;</span><span class="n">x1</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">args</span><span class="o">-&gt;</span><span class="n">y1</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="n">args</span><span class="o">-&gt;</span><span class="n">width</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">height</span> <span class="o">=</span> <span class="n">args</span><span class="o">-&gt;</span><span class="n">height</span><span class="p">;</span>

  <span class="c1">// printf("cur start row: %d, cur total row:%d\n", cur_start_row, cur_total_rows);</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">height</span><span class="o">/</span><span class="n">args</span><span class="o">-&gt;</span><span class="n">numThreads</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">start_row</span> <span class="o">=</span> <span class="n">args</span><span class="o">-&gt;</span><span class="n">threadId</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">args</span><span class="o">-&gt;</span><span class="n">numThreads</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">num_rows</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">mandelbrotSerial</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span>
                     <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span>
                     <span class="n">start_row</span><span class="p">,</span> <span class="n">num_rows</span><span class="p">,</span>
                     <span class="n">args</span><span class="o">-&gt;</span><span class="n">maxIterations</span><span class="p">,</span> <span class="n">args</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">);</span>


  <span class="p">}</span>
  <span class="kt">double</span> <span class="n">endTime</span> <span class="o">=</span> <span class="n">CycleTimer</span><span class="o">::</span><span class="n">currentSeconds</span><span class="p">();</span>
  <span class="kt">double</span> <span class="n">exe_time</span> <span class="o">=</span> <span class="n">endTime</span> <span class="o">-</span> <span class="n">startTime</span><span class="p">;</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"exe time: %f ms</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">exe_time</span><span class="o">*</span><span class="mi">1000</span><span class="p">);</span>


  <span class="c1">// printf("Hello world from thread %d\n", args-&gt;threadId);</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --threads 4
[mandelbrot serial]:            [533.651] ms
Wrote image file mandelbrot-serial.ppm
exe time: 142.692391 ms
exe time: 157.330707 ms
exe time: 157.454997 ms
exe time: 157.499295 ms
exe time: 141.290620 ms
exe time: 150.249667 ms
exe time: 150.373559 ms
exe time: 150.334526 ms
exe time: 138.450164 ms
exe time: 151.096858 ms
exe time: 151.156478 ms
exe time: 151.210513 ms
exe time: 138.270788 ms
exe time: 150.668491 ms
exe time: 150.774766 ms
exe time: 150.800020 ms
exe time: 138.079636 ms
exe time: 150.737014 ms
exe time: 150.741972 ms
exe time: 150.848471 ms
[mandelbrot thread]:            [150.471] ms
Wrote image file mandelbrot-thread.ppm
                                (3.55x speedup from 4 threads)
</code></pre></div></div> <h3 id="comparison-between-inefficient-assignment-and-round-robin-assignment">Comparison between inefficient assignment and round-robin assignment</h3> <p>Naive sequential assignment:</p> <p>Thread running time for another image genration</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --view 2 --threads 4
[mandelbrot serial]:            [311.051] ms
Wrote image file mandelbrot-serial.ppm
exe time: 84.468454 ms
exe time: 86.912777 ms
exe time: 87.307919 ms
exe time: 133.808278 ms
[mandelbrot thread]:            [119.725] ms
Wrote image file mandelbrot-thread.ppm                                                                                                                                                       
(2.60x speedup from 4 threads)
</code></pre></div></div> <p>Round-robin assignment:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog1_mandelbrot_threads git:(master) ✗ ./mandelbrot --view 2 --threads 4
[mandelbrot serial]:            [310.842] ms
Wrote image file mandelbrot-serial.ppm
exe time: 83.830711 ms
exe time: 93.051653 ms
exe time: 93.096461 ms
exe time: 93.373701 ms
[mandelbrot thread]:            [93.562] ms
Wrote image file mandelbrot-thread.ppm                                                                                                                                                       (3.32x speedup from 4 threads)
</code></pre></div></div>]]></content><author><name></name></author><category term="parallel"/><category term="programming"/><category term="parallel"/><category term="programming"/><summary type="html"><![CDATA[Program1: generate image with multiple threads.]]></summary></entry><entry><title type="html">C compiler - single letter local variable</title><link href="https://bilyz98.github.io/blog/2024/chibicc-single-letter-ident/" rel="alternate" type="text/html" title="C compiler - single letter local variable"/><published>2024-10-15T11:59:00+00:00</published><updated>2024-10-15T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/chibicc-single-letter-ident</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/chibicc-single-letter-ident/"><![CDATA[<h2 id="add-single-letter-identity-in-c-compiler">Add single letter identity in c compiler</h2> <p>Need to allocate memory address for single letter variable.</p> <p>Use following two registers. <code class="language-plaintext highlighter-rouge">rsp</code> <code class="language-plaintext highlighter-rouge">rbp</code></p> <p>In x86 assembly language, <code class="language-plaintext highlighter-rouge">rsp</code> and <code class="language-plaintext highlighter-rouge">rbp</code> are registers that play a crucial role in stack management and function calls:</p> <ul> <li> <p><strong><code class="language-plaintext highlighter-rouge">rsp</code> (Stack Pointer)</strong>: This register points to the top of the stack, which is a region of memory used for dynamic storage during program execution. The stack pointer is adjusted automatically when values are pushed onto or popped off the stack. It’s typically used to keep track of the current position in the stack.</p> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">rbp</code> (Base Pointer)</strong>: This register is used as a reference point for the stack frame of a function. A stack frame contains the function’s local variables and saved registers. The base pointer is usually set to the value of the stack pointer at the beginning of a function and remains constant throughout the function’s execution. This allows for easy access to function parameters and local variables via fixed offsets from <code class="language-plaintext highlighter-rouge">rbp</code>.</p> </li> </ul> <p>Here’s a brief example to illustrate their usage in a function call:</p> <pre><code class="language-assembly">push rbp            ; Save the caller's base pointer
mov rbp, rsp        ; Set the base pointer to the current stack pointer
sub rsp, 16         ; Allocate space for local variables
; Function body
mov rsp, rbp        ; Restore the stack pointer
pop rbp             ; Restore the caller's base pointer
ret                 ; Return to the caller
</code></pre> <p>In this example:</p> <ul> <li>The caller’s base pointer is saved on the stack.</li> <li>The base pointer is set to the current stack pointer, establishing a new stack frame.</li> <li>Space is allocated on the stack for local variables.</li> <li>At the end of the function, the stack pointer and base pointer are restored, and control is returned to the caller.</li> </ul> <p>These registers are essential for implementing nested function calls and maintaining a clean and organized stack structure. If you’re interested in learning more about x86 assembly and stack management, I can provide further details or examples. Just let me know! 😊</p> <p><code class="language-plaintext highlighter-rouge">mov (%%rax), %%rax</code> moves value in <code class="language-plaintext highlighter-rouge">%rax</code> address memory to %rax</p> <p><code class="language-plaintext highlighter-rouge">lea 8(%%rbp), %%rax</code> calculate the effective address <code class="language-plaintext highlighter-rouge">8+%rbp</code> and then load effective address (not memory content) into <code class="language-plaintext highlighter-rouge">%%rax</code></p> <h3 id="parse">Parse</h3> <p>Add assign expression and equality expression.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// expr = assign</span>
<span class="k">static</span> <span class="n">Node</span><span class="o">*</span> <span class="nf">expr</span><span class="p">(</span><span class="n">Token</span> <span class="o">**</span><span class="n">rest</span><span class="p">,</span> <span class="n">Token</span><span class="o">*</span> <span class="n">tok</span><span class="p">){</span>
  <span class="k">return</span> <span class="n">assign</span><span class="p">(</span><span class="n">rest</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// assign = equality ("=" assign)?</span>
<span class="k">static</span> <span class="n">Node</span><span class="o">*</span> <span class="nf">assign</span><span class="p">(</span><span class="n">Token</span><span class="o">**</span> <span class="n">rest</span><span class="p">,</span> <span class="n">Token</span><span class="o">*</span> <span class="n">tok</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">equality</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="p">);</span>
  <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"="</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">new_binary</span><span class="p">(</span><span class="n">ND_ASSIGN</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">assign</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">));</span>
  <span class="p">}</span>

  <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">tok</span><span class="p">;</span>
  <span class="k">return</span> <span class="n">node</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Add ident type in primary expression.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// primary = "(" expr ")" | ident | num</span>
<span class="k">static</span> <span class="n">Node</span><span class="o">*</span> <span class="nf">primary</span><span class="p">(</span><span class="n">Token</span> <span class="o">**</span><span class="n">rest</span><span class="p">,</span> <span class="n">Token</span><span class="o">*</span> <span class="n">tok</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"("</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">expr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tok</span><span class="p">,</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
    <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">skip</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">")"</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">node</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">if</span><span class="p">(</span><span class="n">tok</span><span class="o">-&gt;</span><span class="n">kind</span> <span class="o">==</span> <span class="n">TK_IDENT</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">new_var_node</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">tok</span><span class="o">-&gt;</span><span class="n">loc</span><span class="p">));</span>
    <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">node</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">if</span><span class="p">(</span><span class="n">tok</span><span class="o">-&gt;</span><span class="n">kind</span>  <span class="o">==</span> <span class="n">TK_NUM</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">Node</span><span class="o">*</span> <span class="n">node</span> <span class="o">=</span> <span class="n">new_num</span><span class="p">(</span><span class="n">tok</span><span class="o">-&gt;</span><span class="n">val</span><span class="p">);</span>
    <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">tok</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">node</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">error_tok</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="s">"expected an expression"</span><span class="p">);</span>

<span class="p">}</span>
</code></pre></div></div> <h3 id="codegen">Codegen</h3> <p>Add two new node types.</p> <ol> <li><code class="language-plaintext highlighter-rouge">ND_VAR</code> for loading memory address of variable.</li> <li><code class="language-plaintext highlighter-rouge">ND_ASSIGN</code> for get the value of right hand side expression and assign the value to memory address of the left hand side variable.</li> </ol> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">void</span> <span class="nf">gen_expr</span><span class="p">(</span><span class="n">Node</span><span class="o">*</span> <span class="n">node</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">switch</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">kind</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">case</span> <span class="n">ND_NUM</span><span class="p">:</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">" mov $%d, %%rax</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">node</span><span class="o">-&gt;</span><span class="n">val</span><span class="p">);</span>
    <span class="k">return</span><span class="p">;</span>

    <span class="k">case</span> <span class="n">ND_NEG</span><span class="p">:</span>
    <span class="n">gen_expr</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">lhs</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">" neg %%rax</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="k">return</span><span class="p">;</span>

    <span class="k">case</span> <span class="n">ND_VAR</span><span class="p">:</span>
    <span class="n">gen_addr</span><span class="p">(</span><span class="n">node</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  mov (%%rax), %%rax</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="k">return</span><span class="p">;;</span>

    <span class="k">case</span> <span class="n">ND_ASSIGN</span><span class="p">:</span>
    <span class="n">gen_addr</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">lhs</span><span class="p">);</span>
    <span class="n">push</span><span class="p">();</span>
    <span class="n">gen_expr</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">rhs</span><span class="p">);</span>
    <span class="n">pop</span><span class="p">(</span><span class="s">"%rdi"</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  mov %%rax, (%%rdi)</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

</code></pre></div></div> <p>Only use stack memory to store value of variable. Each single letter variable takes 8 bytes memory space.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">void</span> <span class="nf">gen_addr</span><span class="p">(</span><span class="n">Node</span><span class="o">*</span> <span class="n">node</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">kind</span> <span class="o">==</span> <span class="n">ND_VAR</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">name</span> <span class="o">-</span> <span class="sc">'a'</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">8</span><span class="p">;</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  lea %d(%%rbp), %%rax</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="o">-</span><span class="n">offset</span><span class="p">);</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">error</span><span class="p">(</span><span class="s">"not an lvalue"</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="compiler"/><category term="c"/><category term="compiler"/><summary type="html"><![CDATA[Add single letter identity in c compiler Need to allocate memory address for single letter variable.]]></summary></entry></feed>