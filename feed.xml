<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bilyz98.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bilyz98.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-08T14:04:02+00:00</updated><id>https://bilyz98.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">cpp thread local</title><link href="https://bilyz98.github.io/blog/2024/cpp-thread-local/" rel="alternate" type="text/html" title="cpp thread local"/><published>2024-08-05T11:59:00+00:00</published><updated>2024-08-05T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/cpp-thread-local</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/cpp-thread-local/"><![CDATA[<p><a href="https://github.com/muluoleiguo/interview/blob/master/%E9%9D%A2%E8%AF%95/c%2B%2B%E5%B9%B6%E5%8F%91/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E7%BA%BF%E7%A8%8B%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8.md">thread-local</a></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#include &lt;iostream&gt;
#include &lt;thread&gt;

void add(int n) {
    thread_local int count = 0;
    // static thread_local int count = 0; // 两种写法等价！
    count += n;
    // 休眠n秒，防止输出时数据交错（Mac会出现）
    std::this_thread::sleep_for(std::chrono::seconds(n));
    std::cout&lt;&lt;std::this_thread::get_id()&lt;&lt;":"&lt;&lt;count&lt;&lt;std::endl;
}

int main() {
    std::thread td[2];
    for (int i = 0; i &lt; 2; i++) {
        td[i] = std::thread(add, i+1);
    }
    for (int i = 0; i &lt; 2; i++) {
        td[i].join();
    }
    return 0;
}
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#include &lt;iostream&gt;
#include &lt;thread&gt;

class A {
public:
    void dump() {
        std::cout&lt;&lt;id&lt;&lt;":"&lt;&lt;count&lt;&lt;std::endl;
    }
    std::thread::id id;
    static thread_local int count;
};
thread_local int A::count = 0;

void add(int n) {
    A a;
    a.id = std::this_thread::get_id();
    a.count += n;
    std::this_thread::sleep_for(std::chrono::seconds(n));
    a.dump();
    A aa;
    aa.dump(); // aa 和 a 中的count在同一个线程内相同。
}
int main()
{
    std::thread td[2];
    for (int i = 0; i &lt; 2; i++) {
        td[i] = std::thread(add, i+1);
    }
    for (int i = 0; i &lt; 2; i++) {
        td[i].join();
    }
    return 0;
}
</code></pre></div></div>]]></content><author><name></name></author><category term="cpp"/><category term="cpp"/><summary type="html"><![CDATA[thread-local]]></summary></entry><entry><title type="html">micrograd</title><link href="https://bilyz98.github.io/blog/2024/micrograd/" rel="alternate" type="text/html" title="micrograd"/><published>2024-08-05T11:59:00+00:00</published><updated>2024-08-05T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/micrograd</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/micrograd/"><![CDATA[<p>What is micrograd?</p> <p>micrograd is a simple implementation of pytorch-like autograd engine built by karparthy.</p> <h2 id="test-code">Test code</h2> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">micrograd.engine</span> <span class="kn">import</span> <span class="n">Value</span>

<span class="n">a</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">data</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">c</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">b</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>The output is quite straightforward:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>12.0000
3.0000
4.0000
1.0000
</code></pre></div></div> <p>However, the grad will accumulate in the value if it’s not set to zero.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">micrograd.engine</span> <span class="kn">import</span> <span class="n">Value</span>

<span class="n">a</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">data</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">c</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">b</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>


<span class="c1"># a = Value(4.0)
# b = Value(3.0)
</span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">d</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">b</span><span class="p">.</span><span class="n">grad</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>


</code></pre></div></div> <p>OUtput:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>12.0000
3.0000
4.0000
1.0000
4.0000
5.0000
</code></pre></div></div> <h3 id="internal-implementation">Internal implementation</h3>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="pytorch"/><summary type="html"><![CDATA[What is micrograd?]]></summary></entry><entry><title type="html">python dataframe drop row</title><link href="https://bilyz98.github.io/blog/2024/df-drop-row/" rel="alternate" type="text/html" title="python dataframe drop row"/><published>2024-08-04T11:59:00+00:00</published><updated>2024-08-04T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/df-drop-row</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/df-drop-row/"><![CDATA[<p>To drop rows from a Pandas DataFrame where a key only appears once, you can use the <code class="language-plaintext highlighter-rouge">groupby</code> and <code class="language-plaintext highlighter-rouge">transform</code> methods. Here’s a simple example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Sample DataFrame
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">ID</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">Value</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">60</span><span class="p">]}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Drop rows where 'ID' appears only once
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">ID</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">ID</span><span class="sh">'</span><span class="p">].</span><span class="nf">transform</span><span class="p">(</span><span class="sh">'</span><span class="s">count</span><span class="sh">'</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div> <p>This code will keep only the rows where the ‘ID’ appears more than once¹².</p> <p>Would you like more details or help with a different aspect of your DataFrame?</p>]]></content><author><name></name></author><category term="python"/><category term="python"/><summary type="html"><![CDATA[To drop rows from a Pandas DataFrame where a key only appears once, you can use the groupby and transform methods. Here’s a simple example:]]></summary></entry><entry><title type="html">Git merge file from another branch</title><link href="https://bilyz98.github.io/blog/2024/git-merge-file-from-another-branch/" rel="alternate" type="text/html" title="Git merge file from another branch"/><published>2024-07-29T11:59:00+00:00</published><updated>2024-07-29T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/git-merge-file-from-another-branch</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/git-merge-file-from-another-branch/"><![CDATA[<ol> <li> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout --patch
</code></pre></div> </div> </li> </ol> <p>Use case: when you want to check out specific part in a file.</p> <p>2. Direct checkout Use case: replace entire file from another branch</p> <ol> <li> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git cherry-pick &lt;commit-hash&gt;
</code></pre></div> </div> </li> </ol> <p>Use case: apply enire commit from another branch to your current branch.</p>]]></content><author><name></name></author><category term="git"/><category term="git"/><summary type="html"><![CDATA[git checkout --patch]]></summary></entry><entry><title type="html">Efficiency tips</title><link href="https://bilyz98.github.io/blog/2024/efficiency-tips/" rel="alternate" type="text/html" title="Efficiency tips"/><published>2024-07-19T11:59:00+00:00</published><updated>2024-07-19T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/efficiency-tips</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/efficiency-tips/"><![CDATA[<ul> <li>Do important and urgent things first.</li> <li>Try to avoid urgent and important thing, delegate them if possible.</li> <li>Motivation succeds action , not precedes it. Start doing asap.</li> <li>Try 5mins rule. Just try new things with 5 mins. This helps to ease the mind burden</li> <li>Make decision in advance. <ul> <li>Think about all postive and negative outcomes and come up with solutions.</li> </ul> </li> <li>Difference between research and enginnering <ul> <li>Research is about fail fast and and find the right problem and solution</li> <li>Engineering is about doing execution.</li> </ul> </li> </ul> <p><a href="https://www.youtube.com/live/b33vqX74EcA?si=fC-vlyOsjak_EXtH">https://www.youtube.com/live/b33vqX74EcA?si=fC-vlyOsjak_EXtH</a></p>]]></content><author><name></name></author><category term="tips"/><category term="efficiency"/><category term="tip"/><summary type="html"><![CDATA[Do important and urgent things first. Try to avoid urgent and important thing, delegate them if possible. Motivation succeds action , not precedes it. Start doing asap. Try 5mins rule. Just try new things with 5 mins. This helps to ease the mind burden Make decision in advance. Think about all postive and negative outcomes and come up with solutions. Difference between research and enginnering Research is about fail fast and and find the right problem and solution Engineering is about doing execution.]]></summary></entry><entry><title type="html">Speed up matrix multiplication</title><link href="https://bilyz98.github.io/blog/2024/matrix-multiplication/" rel="alternate" type="text/html" title="Speed up matrix multiplication"/><published>2024-07-12T11:59:00+00:00</published><updated>2024-07-12T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/matrix-multiplication</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/matrix-multiplication/"><![CDATA[<p>https://gist.github.com/chris124567/c45d46fdf4d922389641cc9f591ae577</p> <h3 id="naive-matrix-multiplication">Naive matrix multiplication</h3> <p>Code</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">matmul_byhand</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">weight</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">input_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">input_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">input_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_col_idx</span> <span class="o">=</span><span class="mi">0</span> <span class="p">;</span> <span class="n">output_col_idx</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">output_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">m_idx</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">m_idx</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">m_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">input_idx</span> <span class="o">=</span> <span class="n">input_row_idx</span> <span class="o">*</span> <span class="n">M</span> <span class="o">+</span> <span class="n">m_idx</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">weight_idx</span> <span class="o">=</span> <span class="n">m_idx</span> <span class="o">*</span>  <span class="n">K</span>  <span class="o">+</span> <span class="n">output_col_idx</span><span class="p">;</span>
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">input</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">weight</span><span class="p">[</span><span class="n">weight_idx</span><span class="p">];</span>
      <span class="p">}</span>

      <span class="kt">int</span> <span class="n">out_idx</span> <span class="o">=</span> <span class="n">input_row_idx</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">output_col_idx</span><span class="p">;</span>
      <span class="n">out</span><span class="p">[</span><span class="n">out_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
	<span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">M</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">K</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span> <span class="o">*</span> <span class="n">M</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">M</span> <span class="o">*</span> <span class="n">K</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span> <span class="o">*</span> <span class="n">K</span><span class="p">];</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">*</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">"%f "</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">j</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="n">matmul_byhand</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">"%f "</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">j</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span>
</code></pre></div></div> <p>Explanation: The most outer loop iterates over the rows of the input matrix.</p> <p>The second outer loop iterates over the columns of the output matrix. The inner loop iterates over the columns of the input matrix and the rows of the weight matrix. The inner loop calculates the dot product of the input row and the weight column and stores the result in the output matrix.</p> <p>Please note that how index of input, weight and output matrix are calculated. <img src="https://github.com/user-attachments/assets/dc21b2cc-20f6-4720-8fbb-5adc7855d4f0" alt="matmul3 drawio"/></p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>20.000000 23.000000 26.000000 29.000000
56.000000 68.000000 80.000000 92.000000
92.000000 113.000000 134.000000 155.000000
</code></pre></div></div> <p>Issue: Failed to install cuda 12.1.0</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano-gpt) [nsccgz_qylin_1@ln101%tianhe2-K matmul]$ conda install nvidia/label/cuda-12.1.0::cuda-toolkit
Channels:
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
 - defaults
 - nvidia/label/cuda-12.1.0
 - conda-forge
 - nvidia
 - pytorch
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: failed

InvalidSpec: The package "nvidia/linux-64::cuda-compiler==12.5.1=0" is not available for the specified platform
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-y</span> <span class="nt">-c</span> nvidia <span class="nv">cuda</span><span class="o">=</span>12.1.0 cuda-tools<span class="o">=</span>12.1.0 cuda-toolkit<span class="o">=</span>12.1.0 cuda-version<span class="o">=</span>12.1 cuda-command-line-tools<span class="o">=</span>12.1.0 cuda-compiler<span class="o">=</span>12.1.0 cuda-runtime<span class="o">=</span>12.1.0
</code></pre></div></div>]]></content><author><name></name></author><category term="ml"/><category term="al"/><category term="ml"/><summary type="html"><![CDATA[https://gist.github.com/chris124567/c45d46fdf4d922389641cc9f591ae577]]></summary></entry><entry><title type="html">How to write research paper</title><link href="https://bilyz98.github.io/blog/2024/how-to-write-research-paper/" rel="alternate" type="text/html" title="How to write research paper"/><published>2024-07-07T11:59:00+00:00</published><updated>2024-07-07T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/how-to-write-research-paper</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/how-to-write-research-paper/"><![CDATA[<p>I first found this slide from a wechat offical account.</p> <p>Then I found the original link through google. <a href="https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/">https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/</a></p> <p>What I learn that refreshes my mind is that this talk mentions that</p> <p>one should do this</p> <p>Coming up with research idea -&gt; Writing paper -&gt; Doing research work</p> <p>instead of this</p> <p>Coming up with research idea -&gt; Doing research work -&gt; Writing paper</p> <p>I felt mind blowing when I first read this.</p> <p>This make so much sense.</p> <p>The talk mentions that an idea is meaningless if you just keep it to yourself.</p> <p>So what you should do is to write them down and spread your idea to other people as early as possbile.</p>]]></content><author><name></name></author><category term="research"/><category term="research"/><summary type="html"><![CDATA[How to write research paper]]></summary></entry><entry><title type="html">Fast nano-gpt training</title><link href="https://bilyz98.github.io/blog/2024/gpt-fast/" rel="alternate" type="text/html" title="Fast nano-gpt training"/><published>2024-07-03T11:59:00+00:00</published><updated>2024-07-03T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/gpt-fast</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/gpt-fast/"><![CDATA[<p>In this blog I will document my code and experiment results following karparthy’s latest gpt-2 training tutorial video.</p> <iframe width="560" height="315" src="https://www.youtube.com/embed/l8pRSuU81PU?si=2sEmtmn56XBMPTbU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>[https://youtu.be/l8pRSuU81PU?si=2sEmtmn56XBMPTbU][https://youtu.be/l8pRSuU81PU?si=2sEmtmn56XBMPTbU]</p> <p>Code repo url: <a href="https://github.com/BilyZ98/nano-gpt">https://github.com/BilyZ98/nano-gpt</a></p> <p>What is TFLOPs ? Tera floating point operations per second.</p> <h3 id="weight-sharing">Weight sharing</h3> <p>Share weight of <code class="language-plaintext highlighter-rouge">lm_head</code> and <code class="language-plaintext highlighter-rouge">weight_token_embedding</code> This weight share helps to save memory. This is huge amount of memory. It’s <code class="language-plaintext highlighter-rouge">n_embed * vocab_size</code></p> <h3 id="lower-precision">Lower precision</h3> <p>Default tensor precision:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model device cuda:0
m device cuda:0
torch.float32
</code></pre></div></div> <p>Without TF32: Memory usage:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 1.0s: nvidia-smi                                                                                                               Sat Jul  6 16:51:57 2024

Sat Jul  6 16:51:57 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   71C    P0   309W / 300W |   5359MiB / 81920MiB |    100%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   29C    P0    46W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1116, val loss 1.1166
step 4500, loss: 1.1580688953399658, dt: 113.35ms, tok/sec: 144546.60
Time taken: 739.9126691818237 seconds
Total parameters: 10921049
Trainable parameters: 10921049
        ex is consists of centuries and want-wetlife science method ankners, from the ways wheredco by Benrys, where you're rate ar harm browling preservation in musicians. In Athletes have continuies to munically, and effects create cro-intricate jaz towantical navigating vail respectives to diseariety.

**Befor Hera's Players**

Prannit**
Infaming the game's early which commercial quirtual health, and mobile exammatring sound in football in pain this visualization involvement, is home to shapel key th

</code></pre></div></div> <p>TF32: Memory usage:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 0.1s: nvidia-smi                                                                                                                          Sat Jul  6 17:11:38 2024

Sat Jul  6 17:11:38 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   64C    P0   309W / 300W |   5359MiB / 81920MiB |     99%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   31C    P0    47W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 4998, loss: 1.1807901859283447, dt: 30.66ms, tok/sec: 534324.52
step 4999, loss: 1.1486560106277466, dt: 30.19ms, tok/sec: 542632.14
Time taken: 315.7020351886749 seconds
Total parameters: 10921049
Trainable parameters: 10921049


* Nuriety pattern to Play blogge: The misssiol strategies and transfolk to wo halley focused our work's own forecative ar unique planning. This articiples in the Laine Golden

The Host Potential's top init's experienterment and its particular type and regional warveillance. In this article, I'll shedd a fascinat words contributies and other athlete's early what draw from genre, col up jands, lim body eparthory, there ancient work of pairitization criticallysis.

**Addition Enthusiast Players**
~
</code></pre></div></div> <p>Memory usage does not change much. But the training speed is much faster.</p> <p>With BF16 Memory usage</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 0.1s: nvidia-smi                                                                                                                          Sat Jul  6 17:19:50 2024

Sat Jul  6 17:19:50 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   61C    P0   280W / 300W |   5493MiB / 81920MiB |     98%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   32C    P0    47W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1083, val loss 1.1138
step 4500, loss: 1.149060606956482, dt: 46.36ms, tok/sec: 353409.81
Time taken: 323.1115171909332 seconds
Total parameters: 10921049
Trainable parameters: 10921049


* Thre-known variety is Mobile time forishing and a Piece**

Lerious la trend ammach focusion solus, the ways can date on how theselves presents, and cleaning ung time. While the Long-Anti Central to mitigate efforts, referred time

1. The Ritowastic Qrench Literian ass make-had a numberous unfounded women's explore a court of texture or the engage's earching try of from generating secrets. This benefit for a deterrmining movie excebrations, which its form its applyestriants. By regulating the

</code></pre></div></div> <p>It’s not faster. Why is that?</p> <h3 id="-trochcompile">+ troch.compile()</h3> <p>Got this error duing runtime. So I downgrade the python version to 3.11</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 236, in &lt;module&gt;
    model = torch.compile(model)
            ^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.12/site-packages/torch/__init__.py", line 1868, in compile
    raise RuntimeError("Dynamo is not supported on Python 3.12+")
RuntimeError: Dynamo is not supported on Python 3.12+
</code></pre></div></div> <p>Got another error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/tmp/tmpd5t0oroc/main.c: In function ‘list_to_cuuint64_array’:
/tmp/tmpd5t0oroc/main.c:354:3: error: ‘for’ loop initial declarations are only allowed in C99 mode
   for (Py_ssize_t i = 0; i &lt; len; i++) {
   ^
/tmp/tmpd5t0oroc/main.c:354:3: note: use option -std=c99 or -std=gnu99 to compile your code
/tmp/tmpd5t0oroc/main.c: In function ‘list_to_cuuint32_array’:
/tmp/tmpd5t0oroc/main.c:365:3: error: ‘for’ loop initial declarations are only allowed in C99 mode
   for (Py_ssize_t i = 0; i &lt; len; i++) {
   ^
/tmp/tmp0td94o5b/main.c: In function ‘list_to_cuuint64_array’:
/tmp/tmp0td94o5b/main.c:354:3: error: ‘for’ loop initial declarations are only allowed in C99 mode
   for (Py_ssize_t i = 0; i &lt; len; i++) {
   ^
</code></pre></div></div> <p>I install latest gcc version . Old version is 4.8.5 Fix error above.</p> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1037, val loss 1.1087
step 4500, loss: 1.151484727859497, dt: 23.18ms, tok/sec: 706728.75
Time taken: 285.365624666214 seconds
Total parameters: 10921049
Trainable parameters: 10921049
         emphasizing the parming of a spirit on the development conouches tended from the factor, such as jailust power traffice, and ixo, these can serve the alternatively takes part in processed.

**4. Inform 4applications Movements**

1. **Mush Secrets:** Students in Parleers (airl Warrioring change: Bhern Keshedmn) world, as initial do other basebar from leagues, to employ create messaters and their immpact on our camido opportunities. Its gain political windows, proteins and resoling affian languag
~
</code></pre></div></div> <h3 id="-flash-attention">+ Flash attention</h3> <p>Karpathy gives a brief introduction to flash attention in his video. Check out this video to know more . <a href="https://youtu.be/l8pRSuU81PU?t=7521">https://youtu.be/l8pRSuU81PU?t=7521</a></p> <p>Code:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">One head of self-attention</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">tril</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>
        <span class="c1"># wei = q @ k.transpose(-2, -1) * C **-0.5
</span>        <span class="c1"># wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)
</span>        <span class="c1"># wei = F.softmax(wei, dim=-1) # (B, T, T)
</span>        <span class="c1"># wei = self.dropout(wei)
</span>        <span class="c1"># out = wei @ v #(B,T,T) @ ( B, T, C) -&gt; (B, T, C)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

</code></pre></div></div> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.3369, val loss 1.3376
step 4500, loss: 1.3740415573120117, dt: 22.38ms, tok/sec: 731945.94
Time taken: 288.25680232048035 seconds
Total parameters: 10921049
Trainable parameters: 10921049
        facilitation slowing technique it.

In conclusion, is jilitary Ketins and scale, coreting the programphs of cocentries, many only soidhts:

1. **Ingluentalle performances**: Epchilitations and dinactions has functively cardolles experience witting, significantlyqtually.
3. **Pobitice, SStédio micrositiona Ristice**: The Presed full fame leintarians foreices, day exemisions and community to provide a more endaging of his play byautices. The New clear Ara Case gor as mains, Aheragement textures th
</code></pre></div></div> <p>Throughput increases 4\% compared to the previous version which is not a lot. I guess this is because the model is not big enough to benefit from the flash attention.</p> <h3 id="use-distributed-data-parallelddp">Use distributed data parallel(DDP)</h3> <p>Issue: I can only run with 4 GPUs. If I run with 8 GPUs, I got this error when I try to use all 8 gpus:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ddp_world_size 8
[rank4]: Traceback (most recent call last):
[rank4]:   File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 48, in &lt;module&gt;
[rank4]:     torch.cuda.set_device(device)
[rank4]:   File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/cuda/__init__.py", line 399, in set_device
[rank4]:     torch._C._cuda_setDevice(device)
[rank4]: RuntimeError: CUDA error: invalid device ordinal
[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

E0707 18:10:42.318000 47618031484288 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 4 (pid: 52464) of binary: /GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/bin/python                                                                                         Traceback (most recent call last):                                                                                                                    File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/bin/torchrun", line 33, in &lt;module&gt;                                                              sys.exit(load_entry_point('torch==2.3.1', 'console_scripts', 'torchrun')())                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                        File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:

Failures:
[1]:
  time      : 2024-07-07_18:10:42
  host      : gpu72
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 52465)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-07-07_18:10:42
  host      : gpu72
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 52466)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html                                                          [3]:
  time      : 2024-07-07_18:10:42
  host      : gpu72
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 52467)
  error_file: &lt;N/A&gt;                                                                                                                                   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------                                                                                        Root Cause (first observed failure):                                                                                                                [0]:
  time      : 2024-07-07_18:10:42                                                                                                                     host      : gpu72
  rank      : 4 (local_rank: 4)                                                                                                                       exitcode  : 1 (pid: 52464)                                                                                                                          error_file: &lt;N/A&gt;                                                                                                                                   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html                                                          ============================================================

</code></pre></div></div> <p>Actuall my task is assigned only 4 gpus even thought there are 8 gpus on the compute node. I run following code to get number of available cuda device I can use and it outputs 4.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda device count</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">())</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(nano-gpt) [nsccgz_qylin_1@ln102%tianhe2-K gpt-dev]$ yhrun -n 1 -N 1 -p GPU_A800 python test_gpt_count.py
cuda device count 4
</code></pre></div></div> <p>Asked bing and it gives a post that I can set visible cuda devices to all 8 gpus. And then I check this env variable and found that it only outputs <code class="language-plaintext highlighter-rouge">0,1,2,3</code>.</p> <p>So this is the reason why I can not use all 8 gpus.</p> <p>Problem fixed.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span>python test_gpt_count.py
cuda device count 4
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span>
0,1,2,3
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span>^C
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>nsccgz_qylin_1@gpu73%tianhe2-K gpt-dev]<span class="nv">$ </span><span class="nb">export </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="k">${</span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="k">}</span>,4,5,6,7
</code></pre></div></div> <p>Rewrite dataloader.</p> <p>Since now multiple processes are introduced to use multiple gpus, I need to rewrite the dataloader to make it work with multiple processes.</p> <p>Code: <code class="language-plaintext highlighter-rouge">self.process_rank</code> means current process rank. <code class="language-plaintext highlighter-rouge">self.num_process</code> means total number of processes.</p> <p><code class="language-plaintext highlighter-rouge">self.current_idx</code> is the current index of the data that the dataloader is reading. Note that each process will have it own data to train which is different from other processes. So <code class="language-plaintext highlighter-rouge">self.current_idx</code> moves forward by <code class="language-plaintext highlighter-rouge">B * T * self.num_process</code> each time.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DataLoader</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span>  <span class="n">process_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_process</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span>
        <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="n">self</span><span class="p">.</span><span class="n">process_rank</span> <span class="o">=</span> <span class="n">process_rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_process</span> <span class="o">=</span> <span class="n">num_process</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">process_rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">next_batch</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">current_idx</span><span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">buf</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">buf</span><span class="p">[</span><span class="mi">1</span><span class="p">:]).</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">+=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">num_process</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">num_process</span> <span class="o">+</span> <span class="mi">1</span><span class="o">&gt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">):</span>
            <span class="n">self</span><span class="p">.</span><span class="n">current_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">process_rank</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>


</code></pre></div></div> <p>It takes twice longer time to finish the job after swtiching to new data loader which is weird. I don’t know why. Is this because of cache miss?</p> <p>I think it’s because of the cache miss. Token throughput after switching to new dataloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim  slurm-2769516.out
m device cuda:0
step &lt;built-in function iter&gt;: train loss4.6247, val loss 4.6222
step 0, loss: 6.304970741271973, dt: 253517.79ms, tok/sec: 64.63
step &lt;built-in function iter&gt;: train loss2.7204, val loss 2.7159
step 500, loss: 2.7232675552368164, dt: 22.78ms, tok/sec: 719357.23
step &lt;built-in function iter&gt;: train loss2.6621, val loss 2.6616
</code></pre></div></div> <p>Token throughput before switching to new dataloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim  slurm-2769041.out
step &lt;built-in function iter&gt;: train loss4.6102, val loss 4.6073
step 0, loss: 6.303163528442383, dt: 105310.21ms, tok/sec: 155.58
step &lt;built-in function iter&gt;: train loss2.6170, val loss 2.6169
step 500, loss: 2.6428515911102295, dt: 22.64ms, tok/sec: 723576.18
</code></pre></div></div> <p>It takes even longer after I move <code class="language-plaintext highlighter-rouge">to(device)</code> out of dataloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
tep &lt;built-in function iter&gt;: train loss2.2721, val loss 2.2711
step 4000, loss: 2.2670536041259766, dt: 22.41ms, tok/sec: 730972.72
step &lt;built-in function iter&gt;: train loss1.9989, val loss 1.9980
step 4500, loss: 2.078113079071045, dt: 22.69ms, tok/sec: 722123.90
Time taken: 651.469042301178 seconds
Total parameters: 10921049
Trainable parameters: 10921049                                                                                                                               the melorme slocites apctature to bayysic impessare to to obaytening ridence's, comuties the prayer'res of conngibuidiktle tooly soudiestinabo, creation, lov expertance entifuchitule, stluch pronuctingmat arous on vismok dailioout ateries tipl,, aphige sthatection of ecpppppivaclliency, powanct transps, and the owen, freper. Coulw fame inidtifinge for falstadry exertwing to socinigys the pecos, reame peound to its daman lawabyang ammbots'res coltual ra casergod asem, yucial gor mann textions mu
</code></pre></div></div> <h4 id="4-gpus-for-ddp">4 GPUs for ddp</h4> <p>Memory usage</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) [nsccgz_qylin_1@gpu72%tianhe2-K ~]$ nvidia-smi
Wed Jul 10 17:18:59 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   45C    P0   145W / 300W |   3975MiB / 81920MiB |     44%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   48C    P0   133W / 300W |   3979MiB / 81920MiB |     45%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  On   | 00000000:53:00.0 Off |                    0 |
| N/A   47C    P0   160W / 300W |   3979MiB / 81920MiB |     46%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  On   | 00000000:57:00.0 Off |                    0 |
| N/A   49C    P0   134W / 300W |   3959MiB / 81920MiB |     44%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Every 1.0s: nvidia-smi                                                                                                                                                                    Wed Jul 10 17:37:07 2024

Wed Jul 10 17:37:07 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   32C    P0    67W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   35C    P0    69W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  On   | 00000000:53:00.0 Off |                    0 |
| N/A   34C    P0    66W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  On   | 00000000:57:00.0 Off |                    0 |
| N/A   36C    P0    69W / 300W |   1117MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Memory usage gradually increases. Why is this?</p> <p>Issue: Getting error that nccl report heartbeat error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[rank0]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 0] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=1
[rank0]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 0] ProcessGroupNCCL preparing to dump debug info.
[rank0]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 0] [PG 0 Rank 0] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 1
E0710 20:29:48.177000 47209663704448 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: -6) local_rank: 0 (pid: 1882) of binary: /GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/bin/python
Traceback (most recent call last):
</code></pre></div></div> <p>I think this is because I did not add this line of code</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):
</code></pre></div></div> <p>This code does not fix the problem.</p> <p>I am now trying to first call <code class="language-plaintext highlighter-rouge">destroy_process_group</code> and then calling tokens generation to fix issue above.</p> <p>I should decrease count of training iteration so that I can get error as early as possible.</p> <p>It takes 16 mins to finish model training which is not normal. I wonder why does it take so long after switching to new dataloader.</p> <p>Can this be a research problem?</p> <p>Why does it take so long to generate tokens after training is finished?.</p> <p>Is this because of decode part or is this because of the model inference part?</p> <p>Issue: Get this error saying that expected all tensors to be on the same device.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 292, in generate_tokens
    logits, loss = model(idx_cond) # (B,T,vocab_size)
                   ^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/zt/gpt-dev/persona_gpt.py", line 255, in forward
    tok_emb = self.token_embedding_table(idx) #(B,T,C)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/envs/nano-gpt/lib/python3.11/site-packages/torch/nn/functional.py", line 2264, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
</code></pre></div></div> <p>Don’t know which tensor is not on cuda. So I print device of <code class="language-plaintext highlighter-rouge">idx</code> to see if it’s on cuda.</p> <p>Issue above is sovled after I update the code like this</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span> <span class="p">)</span>
</code></pre></div></div> <p>cuda:0 do use lots of memory while other gpus have freed up memory.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  On   | 00000000:4F:00.0 Off |                    0 |
| N/A   36C    P0    73W / 300W |   4867MiB / 81920MiB |    100%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  On   | 00000000:50:00.0 Off |                    0 |
| N/A   32C    P0    47W / 300W |     27MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  On   | 00000000:53:00.0 Off |                    0 |
| N/A   32C    P0    44W / 300W |      3MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  On   | 00000000:57:00.0 Off |                    0 |
| N/A   34C    P0    47W / 300W |      7MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
</code></pre></div></div> <p>Still can not generate tokens.</p> <p>I think this is because that I did not call model forward for all processes.</p> <p>The code runs successfully after I call model forward for all processes for generating text.</p> <p>It does not work even I set <code class="language-plaintext highlighter-rouge">model.eval()</code> only for master process. Why is that?</p> <p>Here’s the code that can finish successfully without process hanging</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_iters</span><span class="p">):</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="c1"># xb, yb = get_batch('train')
</span>    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">data_loader</span><span class="p">.</span><span class="nf">next_batch</span><span class="p">()</span>
    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">xb</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">yb</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">xb</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># with torch.autocast(device_type=device_type, dtype=torch.bfloat16):
</span>    <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

    <span class="c1"># if ddp:
</span>    <span class="c1">#     model.require_backward_grad_sync = True
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">ddp</span><span class="p">:</span>
        <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">AVG</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span> <span class="c1"># wait for the GPU to finish work
</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">token_processed</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">ddp_world_size</span>
    <span class="c1">#if step % eval_interval == 0 :
</span>    <span class="c1">#    losses = estimate_loss()
</span>    <span class="c1">#    print(f"step {iter}: train loss{losses['train']:.4f}, val loss {losses['val']:.4f}")
</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="p">(</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span> <span class="c1"># milli sec
</span>    <span class="n">token_per_sec</span> <span class="o">=</span> <span class="n">token_processed</span><span class="o">/</span> <span class="p">(</span><span class="n">t1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span>
    <span class="nf">call_generate_tokens</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>                                                                                                  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, dt: </span><span class="si">{</span><span class="n">dt</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">ms, tok/sec: </span><span class="si">{</span><span class="n">token_per_sec</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">master_process</span><span class="p">:</span>
</code></pre></div></div> <p>It looks like from this discuss post that I have to call forward and backwad for all processes. <a href="https://discuss.pytorch.org/t/multiple-forward-functions-in-dp-and-ddp/135029/5">https://discuss.pytorch.org/t/multiple-forward-functions-in-dp-and-ddp/135029/5</a></p> <p>No DDP with 1 gpu + new data loader + tf32, no torch.compile</p> <p>It’s pretty fast though.</p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 4998, loss: 1.5543745756149292, dt: 44.11ms, tok/sec: 371476.71
idx device: cuda:0
rank: 0,         broving and mited rocal hand alchiqe, Ond Wastestern have alter numpre, and society.

**Conclusion**

When 103, hith htreate analyzing into talent of the nets encerlated indituted to degensive sports and projote About to  trace producial stratege. Os a care a coved legelad interacting harswe her in the intricacies, with history and winnlowedge in 140s has bhe Bavill –as cit) Éanta.

**Upproach Inper To Adaptation**

The commmal tOxerium boodls to dimerstry of pinemanical the industry to interes
step 4999, loss: 1.5449110269546509, dt: 44.11ms, tok/sec: 371424.51
Time taken: 301.4059376716614 seconds
Total parameters: 10921049
Trainable parameters: 10921049
</code></pre></div></div> <p>DDP with 4 gpus , no torch.compile, tf32</p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 4998, loss: 1.2392717599868774, dt: 85.30ms, tok/sec: 768330.46
idx device:idx device:idx device:idx device:    cuda:3cuda:0cuda:1cuda:2



rank: 1,         Choroes, and formats. In this a
rank: 3,         Choroes, and formats. In this a
rank: 2,         Choroes, and formats. In this a
rank: 0,         Choroes, and formats. In this a
step 4999, loss: 1.2737213373184204, dt: 86.10ms, tok/sec: 761164.76
Time taken: 442.67850971221924 seconds
Total parameters: 10921049
Trainable parameters: 10921049
</code></pre></div></div> <p>I don’t know how FSDP( Fully sharded data parallel ) works yet.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="llm"/><summary type="html"><![CDATA[llm]]></summary></entry><entry><title type="html">System for machine learning papers</title><link href="https://bilyz98.github.io/blog/2024/sysml-papers/" rel="alternate" type="text/html" title="System for machine learning papers"/><published>2024-07-02T11:59:00+00:00</published><updated>2024-07-02T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/sysml-papers</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/sysml-papers/"><![CDATA[<p>For each paper I will give a brief summary and my thoughts on it. I will link each paper to other related papers if possible. Besides, I might add code and related resources if I have time.</p> <p>For most papers I will first read titles and abstract to decide whether I should read the rest part of the paper. This helps me to quickly filter out papers that are interesting to me.</p> <p>So for majority of papers I will make the summary short. For others that are interesting to me, I will write a longer summary.</p> <h4 id="characterization-of-large-language-model-development-in-the-datacenter">Characterization of Large Language Model Development in the Datacenter</h4> <p><a href="https://www.usenix.org/system/files/nsdi24-hu.pdf">https://www.usenix.org/system/files/nsdi24-hu.pdf</a></p> <p>Summary: This papers studies llm traning job workload in datacenter. It releases job traces in datacenter for training llm. It mentions that gpu take 65% of power usage.</p> <p>I didn’t find any other interesting contribution from this paper other than the job traces. So I won’t spend more time reading this paper.</p> <p>Does this paper mention llm serving ?</p> <h4 id="parcae-proactive-liveput-optimized-dnn-training-on-preemptible-instances">Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible Instances</h4> <p>Preemptive scheduling and checkpointing? <a href="https://www.usenix.org/conference/nsdi24/presentation/duan">https://www.usenix.org/conference/nsdi24/presentation/duan</a></p>]]></content><author><name></name></author><category term="ml"/><category term="mlsys"/><summary type="html"><![CDATA[sysml papers]]></summary></entry><entry><title type="html">nano-gpt and Transformer</title><link href="https://bilyz98.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="nano-gpt and Transformer"/><published>2024-06-29T11:59:00+00:00</published><updated>2024-06-29T11:59:00+00:00</updated><id>https://bilyz98.github.io/blog/2024/transformer</id><content type="html" xml:base="https://bilyz98.github.io/blog/2024/transformer/"><![CDATA[<p>Code repo url: <a href="https://github.com/BilyZ98/nano-gpt">https://github.com/BilyZ98/nano-gpt</a></p> <h3 id="vanilla-bigram-model-without-self-attention">Vanilla bigram model without self attention</h3> <p>As mentioned in the youtube video <a href="https://youtu.be/kCc8FmEb1nY?t=2509">https://youtu.be/kCc8FmEb1nY?t=2509</a>, this code builds a bigram model without self attention. We can use this as baseline to compare with self attention code .</p> <p>Issue: can not use torch cuda module even though I have gpu and install cuda pytorch Solution: Tried to install pytorch cuda again Get this error when I tried to install pytroch-cuda:12.1</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ClobberError: This transaction has incompatible packages due to a shared path.
  packages: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64::jpeg-9e-h5eee18b_1, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'share/man/man1/rdjpgcom.1'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64::jpeg-9e-h5eee18b_1, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'share/man/man1/wrjpgcom.1'


</code></pre></div></div> <p>Solution: Switch to new environment and reinstall pytorch with cuda <a href="https://pytorch.org/get-started/locally/#windows-anaconda">https://pytorch.org/get-started/locally/#windows-anaconda</a></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda clean --all
conda clean -p
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
</code></pre></div></div> <p>Now I am able to see cuda available in pytorch</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># 0 corresponds to the first GPU
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano-gpt
True
NVIDIA A800 80GB PCIe
</code></pre></div></div> <p>Comparison of cpu and gpu for bigram model CPU:</p> <p>Time taken: 14.610548734664917 seconds</p> <p>GPU:</p> <p>Time taken: 18.080146074295044 seconds</p> <p>It takes longer time for gpu to finish. I think it’s because training iteration is not large enough to see the benefit of gpu.</p> <h2 id="transformer-architecture">Transformer architecture</h2> <p>Transformer is a type of neural network architecture that is designed to handle sequential data more effectively than traditional RNNs and LSTMs. It was introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, and has since become a popular choice for natural language processing tasks.</p> <p>As mentioned in this video transformer is like neural network version of map-reduce which also comes from google. The reduce process is the self attention process in transformer. The map process is the feed forward neural network and mutl-head in transformer.</p> <p>Why does transformer have Feedforward and Linear at the same time ? These two looks like the same. Andrej karparthy gives answer to this question at the time point in vide.</p> <p>Feedforward is used to think on the tensor/information the self-attention has produced. <br/> And this feedforward/computation is done in parallel which is pretty fast.</p> <p>The final linear layer is used to output token probabilities.</p> <p><img src="https://github.com/BilyZ98/BilyZ98.github.io/assets/26542149/4ce5458c-3c90-4607-803c-01631327ad0f" width="500" height="500"/></p> <h3 id="bigram-model-with-cpu">bigram model with cpu</h3> <p>Code link: <a href="https://github.com/BilyZ98/nano-gpt/blob/5cae2e1635dc560dc75dc92897ee5add43fa3aed/bigram.py">https://github.com/BilyZ98/nano-gpt/blob/5cae2e1635dc560dc75dc92897ee5add43fa3aed/bigram.py</a></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.5979, val loss 2.5999
Time taken: 38.3419029712677 seconds

SOd HADghe uio choaxlondorerl cy,m thamaw pes$!
LUANIV:
R:
RYo,
W:'th thiveCond wnff tod ghind.
Thathis:Ved esVI!
RUSape ms, yonyail lomustthend? thed of sofatiatherves f het m ssprerh fon,cke d pr&amp;lR.
-IUq-bind p'y w; deland walois WBy ethu l'd t y montircPEMPlanas y dslly?sthan coor ccoust d limald ped il f frs th.
ce our ntLE:

YCEqusI,
K:
wnea!vengjF, 'd
GGe cltLTod.
RSwoppiQYe haland dSt tXESacedDUCORGRENETof hos, sooumouloo meRTooe,
The cke;
ONGu he tpalapy an:
NScheracancoj

HAnend
ANUARK
</code></pre></div></div> <h3 id="bigram-model-with-gpu-with-positional-embedding-and-language-model-head">bigram model with gpu with positional embedding and language model head</h3> <p>No self attention for this version</p> <p>Issue: Get this error while running on gpu</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Indexing.cu:1236: indexSelectSmallIndex: block: [0,0,0], thread: [1,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.
</code></pre></div></div> <p>Searched google and it tells that the problem comes from incorrect indexing while using nn.Embedding.</p> <p>Solution: Check nn.Embedding code and fix it. Fix issue above by adding following code to <code class="language-plaintext highlighter-rouge">generate</code> function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>
  <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
      <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">block_size</span><span class="p">:]</span> <span class="c1"># This line of code fix the issue
</span>      <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># becomes (B, C)
</span>      <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, C)
</span>      <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(B,1)
</span>      <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(B, T+1)
</span>    <span class="k">return</span> <span class="n">idx</span>
</code></pre></div></div> <p>The reason is that <code class="language-plaintext highlighter-rouge">self.token_embedding_table</code> and <code class="language-plaintext highlighter-rouge">self.position_embedding_table</code> shares different input dimensions.</p> <p>If we don’t crop the <code class="language-plaintext highlighter-rouge">idx</code> to <code class="language-plaintext highlighter-rouge">idx_cond</code>, the <code class="language-plaintext highlighter-rouge">pos_emb</code> will take <code class="language-plaintext highlighter-rouge">T</code> that is larger than <code class="language-plaintext highlighter-rouge">block_size</code> which will cause the error.</p> <p>Please check this time in the video <a href="https://youtu.be/kCc8FmEb1nY?t=4854">https://youtu.be/kCc8FmEb1nY?t=4854</a></p> <p>code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>

  <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
      <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">block_size</span><span class="p">:]</span>
      <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
      <span class="c1">#print('shape of logits', logits.shape)
</span>      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># becomes (B, C)
</span>      <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, C)
</span>      <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(B,1)
</span>      <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(B, T+1)
</span>    <span class="k">return</span> <span class="n">idx</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model device cuda:0
m device cuda:0
step &lt;built-in function iter&gt;: train loss2.4926, val loss 2.5021
Time taken: 28.21234440803528 seconds



CEThik bridcowindakis by ble

Hiset bobe d e.
S:
O:
ISM:


Thanss:
Wanthar u qur, vet?
F dilasoate awice my.

Hnstarom oroup
Yowhthetof isth ble mil ndilll,

W:

Yeesengcin lat Heriliov ts, and Win nghire yombousel lind pe llllishe ce hiry:
Supr aisspllw y.
Hllin's noroopetelaves
Momy ll, d mothakeeo W-ndo whthCeiibyo touth dourive weeshieed t so mower; te

AN ad nterupt f s ar iris! m:
</code></pre></div></div> <h3 id="self-attention-with-single-head-on-gpu-with-positional-embedding-and-language-model-head">Self attention with single head on gpu with positional embedding and language model head</h3> <p>According to Andrej karparthy’s video, Self attention has three parts: key, query and value. These three parts are all tensors comming out from Linear layer with input tensor.</p> <p>query means what we are looking for each position in T.</p> <p>key means what we have for each input tensor in (T,C) format.<br/> T means context length in time and C means the number of channels or features.</p> <p>query dot product key to get weight matrix that specify importance of each time position in T.</p> <p>value means the information we get from Linear layer for each input tensor in (T,C) format.</p> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">One head of self-attention</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">tril</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">wei</span> <span class="o">=</span> <span class="n">wei</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">tril</span><span class="p">[:</span><span class="n">T</span><span class="p">,</span> <span class="p">:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span> <span class="c1"># (B, T, T)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">wei</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, T, T)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">wei</span> <span class="o">@</span> <span class="n">v</span> <span class="c1">#(B,T,T) @ ( B, T, C) -&gt; (B, T, C)
</span>        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">Head</span><span class="p">(</span><span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1">#(B,T, C)
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.4015, val loss 2.4166
Time taken: 56.774349212646484 seconds

Whent ik bry cowilen is by bth

Hiset bobe ale.
S:
O:
IS:
Falilauss ar btharu wearthe.
War dilasoate awice my.

HDER:
ANGo oug
Yowhavetof is he ot mil; dill, aes iree sen cie lat Herid ovets, and Win ngar ilerabous lelind peal.
-hull onchiry ptugr aiss hew ye wllinde norod atelaves
Momy ll, dl othake ont---o whth eiiby we ati dourive wee, ired thoouso er; th
To kad nteruptef so;
ARID Wam:
ENGCI inleront ffaf Pre?
</code></pre></div></div> <p>It does not improve a lot.</p> <h3 id="multi-head-self-attention-on-gpu-with-positional-embedding-and-language-model-head">Multi-head self attention on gpu with positional embedding and language model head</h3> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Multiple heads of self-attention in parallel</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">Head</span><span class="p">(</span><span class="n">head_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="c1"># self.sa = Head(n_embd)
</span>    <span class="n">self</span><span class="p">.</span><span class="n">sa_heads</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">//</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="c1"># x = self.sa(x)  #(B,T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa_heads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.2748, val loss 2.2858
Time taken: 90.66784453392029 seconds

Whent if bridcowd, whis byer that set bobe toe anthr-and mealleands:
Warth foulque, vet?
Wedtlay anes wice my.

HDY'n om oroug
Yowns, tof is heir thil; dill, aes isee sen cin lat Hetilrov the and Win now onderabousel.

SFAUS:
Shenser cechiry prugh aissthe, ye wing, u not
To thig I whomeny wod mothake ont---An hat evibys wietit, stile weeshirecs poor gier; to
To k danteref If sor; igre! mef thre inledo the af Pre?

WISo myay I sup!
Atied is:
Sadsal the E'd st hoin couk aar tey Iry to I frouf voul
</code></pre></div></div> <p>It looks better and the loss continues to decrease. But it takes longer to finish. Why is that ?</p> <h3 id="multi-head-self-attention-with-feed-forward-neural-network-on-gpu-with-positional-embedding-and-language-model-head">Multi-head self attention with feed forward neural network on gpu with positional embedding and language model head</h3> <p>Why don’t we add positional embedding again between blocks ?<br/> I think this can help transformer to keep track of the position of the tokens.</p> <p>I think we don’t need to add positional embedding again and again between blocks once we use residual connection.</p> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">a simple linear layer followed by a non-linearity</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="c1"># self.sa = Head(n_embd)
</span>    <span class="n">self</span><span class="p">.</span><span class="n">sa_heads</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">//</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="c1"># x = self.sa(x)  #(B,T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa_heads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <p>output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.2288, val loss 2.2414
Time taken: 102.24195170402527 seconds

And the Ror
Thow and is and thrad thom of oule.
Sthr-' my dall ands:
Warth fou qurord.
War dilth ane aw crup and not, ut onour
Yowns, tof it he cove lend lincath is ees, hain lat Het dulvets, and to poman is wables lill dite ullliser cecrivy prupthaiss hew youn's and knamopetell lownomthy wod moth keacal---A wher eiicks to thour rive cees, meds pood of he thu the hanterth po so;; igis! my to thy ale ontat af Pried my of.
WHINY ICHARD:
Poid:
Ardsal the Eget to uin cour ay andy Rry to chan the!
An
</code></pre></div></div> <p>Loss continue s to decrease but not decreases a lot</p> <h3 id="blocks-of-multi-head-self-attention-on-gpu">Blocks of multi-head self attention on gpu</h3> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Transfomer block: communication followed by computation</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span> <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="c1"># self.sa = Head(n_embd)
</span>    <span class="c1"># self.sa_heads = MultiHeadAttention(4, n_embd//4)
</span>    <span class="c1"># self.ffw = FeedForward(n_embd)
</span>    <span class="n">self</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1">#(B,T,C)
</span>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding_table</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="c1"># (T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span> <span class="c1"># (B, T, C)
</span>    <span class="c1"># x = self.sa(x)  #(B,T, C)
</span>    <span class="c1"># x = self.sa_heads(x) #(B, T, C)
</span>    <span class="c1"># x = self.ffw(x) #(B, T, C)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, vocab_size)
</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.3255, val loss 2.3400
Time taken: 141.53945779800415 seconds

And thif bry cowh, har on, ber
waiset bobe to tavegr-d my dalceauss:
Want he us he hentbardethas ane awche my.

HDEE:
Ay neou waowns
Moof is he me mil; dill, aes ireees, hain latiser drovets, and the nor ond wabousel lind thau.
Hhult cncriby: thartaiss hew you lome.
I yof petelgolg's my yow demeth kleonW nou when eiibas wouth dotrive weeshime sto-oche eroure
Thak danterurt fou ar irist muf thin inle oft to fearr?

KISomerry youu
Hartied is:
Aadsalce.

EIDLHY:
Iin couk aaraney Iry the han yo vely
</code></pre></div></div> <p>Does not improve</p> <h3 id="blocks-of-multi-head-self-attention-with-residule-connection-on-gpu">Blocks of multi-head self attention with residule connection on gpu</h3> <p>No projecttions for residual connection: Code:</p> <p>I only show the code for <code class="language-plaintext highlighter-rouge">Block</code> class because this is the only change in the code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Transfomer block: communication followed by computation</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span> <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.1115, val loss 2.1749
Time taken: 155.8028919696808 seconds

And they bridcown,
The layest madisen bube to tamaght' my daliea
My:
Waith foulqorth
but ceetlay ane awice my.

HEER:
An onour
Yount
Moofuing come mill dill, at miree seng, wilatist in ove the Bent longht is wais welll no me litles;
So chirs: ther aiss haw youn's mause roodeter'd swer:
Ill o' meacke
Ao Windo wht Ceiiby we ath do rive wees ire sto-of of he the the danterty po so;
Ang hink:
'Elt yould ontates
Mare?

KING ENCHENNL:
Hartied is wards beaces and thisin cour ay and
Hire the have fove y
</code></pre></div></div> <p>With projecttions for residual connection:</p> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Multiple heads of self-attention in parallel</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">Head</span><span class="p">(</span><span class="n">head_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">a simple linear layer followed by a non-linearity</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Transfomer block: communication followed by computation</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span> <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.0052, val loss 2.1047
Time taken: 128.87973642349243 seconds


KER:
Dy be will and is by be madised bube to take Our my dagalanss:
Wact me us crome. Wardethas anes wick, you, not to zoknow
Yourselvef is heart milled,
What grive, send, will is therevers, and the now on you me, lord dime littishe courmby pruperais'll woy. Hurmake norfore blaves home.
Who my thake of in on her eis as the most rive cenchimed the come, for unter hands thime son; if hink:
Edway male of wefife
Where, Som.
What suk!
Kered is wards.
Wice Efees bidin couses.
Wher, reath chan the wel
</code></pre></div></div> <p>It’s a little bit better compared to no-projection.</p> <p>What is the difference between these two ? I think projection is used to project output tensor from self attention to the same space with input tensor so that we can add them together.</p> <p>But from the result I can see the not projecting is also fine.</p> <h3 id="residual-blocks-of-self-attention-with-layernorm">Residual blocks of self attention with layernorm</h3> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Transfomer block: communication followed by computation</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffw</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">n_embd</span> <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">n_embd</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">n_embd</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffw</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">n_embd</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="c1"># self.feed_forward = nn.Linear()
</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.9904, val loss 2.0918
Time taken: 165.2521140575409 seconds

And they bridle.

SOROR To beer a seek obe don.
Sagrad my dagalans!
You that us hear buble dilt
Hate away, my fears'd of of my
Yoursert foitie bettlit now
Whimes if ensen cim;
Stistaid ove the the me now on that thell in a wall thus would by pruppiness hiw you:
That I mandpeter'd gond:
Is would that
To Winson her eis all'd they srive will ime strow more-fore
To knom thrupt for trear. Wame monge inlee,
Thef firse?

KISTINUS:
If be!

GRESNY:

Sadave the Edwall?

GRAKE Masceave
Hir-bromence you! My
</code></pre></div></div> <p>loss drops a little bit more compared to no layernorm.</p> <h3 id="residual-blocks-of-self-attention-with-layernorm--dropout-full-transformer">Residual blocks of self attention with layernorm + dropout (Full transformer)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">One head of self-attention</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">tril</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">C</span> <span class="o">**-</span><span class="mf">0.5</span>
        <span class="n">wei</span> <span class="o">=</span> <span class="n">wei</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">tril</span><span class="p">[:</span><span class="n">T</span><span class="p">,</span> <span class="p">:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span> <span class="c1"># (B, T, T)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">wei</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, T, T)
</span>        <span class="n">wei</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">wei</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#(B, T, C)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">wei</span> <span class="o">@</span> <span class="n">v</span> <span class="c1">#(B,T,T) @ ( B, T, C) -&gt; (B, T, C)
</span>        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Multiple heads of self-attention in parallel</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">Head</span><span class="p">(</span><span class="n">head_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">a simple linear layer followed by a non-linearity</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss2.1027, val loss 2.1580
Time taken: 155.4465034008026 seconds


LONIBENT:
Be a most unre maim, gruch, thath pcourdith my it frilf oblis then, nrie?

Mor hould bace                                                                                                                                                          Ce in apitng, anoth you his to cowll
By mbre wand grarist let fead as be meest, Jo afore by shalve my my sade make ta gior mony ow norane;                                                                   Hould-wrind awnAndead notooth. WARKEIY:                                                                                                                                                                                                                                                                                                         Conear gy?                                                                                                                                                              Srom ands, his gahpe with gowis slined fue no lot all wopmeseond in he tha dee knoth quail hen, slyold aus mawers, slosssig, yat but, hery,                             Ond you hom is oalt in, shealve of dRulet my bafker's deforth the sh
</code></pre></div></div> <p>It’s not getting better. I think this dropout will help when we scale up number of parameters.</p> <h3 id="full-transfomer-with-more-parameters">Full transfomer with more parameters</h3> <p>Previous parameters</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># what is the maximum context length for predictions
</span><span class="n">max_iters</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="c1"># device = 'cpu'
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">torch cuda available</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">n_embd</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">head_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
</code></pre></div></div> <p>Cur param:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">256</span> <span class="c1"># what is the maximum context length for predictions
</span><span class="n">max_iters</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="c1"># device = 'cpu'
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">torch cuda available</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">n_embd</span> <span class="o">=</span> <span class="mi">384</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">n_layer</span> <span class="o">=</span> <span class="mi">6</span>
<span class="c1"># head_size = 16
</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.8683, val loss 2.0060
Time taken: 221.6930215358734 seconds

Bale herse?

LAURENNIUS:
When regoubjon theme as be boeet chal speak treatuls a faulse heave ticeso too out boons in of, privy; char dese in paindaur meet is your both talies so are furnt ereworry,
Besse do you grait see fiee in vile should but roonouth
Than I k ount crow on
Sint, I eid, doust not will comon't te refore wife, which young so hing me the grow by-treate, witee of sword That the we great his worse a mick's sestit well sue
I inn frie dam west you that,S more think
That yest deart com
</code></pre></div></div> <p>loss does not drop to 1.4 which the output in karparthy’s video and the running time is too short.</p> <p>I see, there is a duplicate definition of <code class="language-plaintext highlighter-rouge">batch_size=4</code> and <code class="language-plaintext highlighter-rouge">block_size=8</code> in code. Let’s try to run again.</p> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss0.8475, val loss 1.5921
Time taken: 737.4738621711731 seconds

Had you been such tongues, had love me but my lawful hold;
For I, shaked hand I for all discoversion,
To crook his heirs, to his crack that he.
Caius Marcius Corizelland! Murk'd, I had thou
Start with your charters, Warwick, remoning the
powerful leave.

MONTAGUE:
Once, the teernest thy power!

Second Murderer:
I will do not say, i' wedded with thy name?

Second Murderer:
Viello, thou hast affected the king's. Now thou hast
well a knoss to toe a stuffity thou in followine;
what thou hast no news
</code></pre></div></div> <p>The loss drops to 1.5. There is overfitting. The output from model looks better now.</p> <p>Great.</p> <h3 id="full-transformer-without-positional-embedding">Full transformer without positional embedding</h3> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss1.1735, val loss 1.5787
Time taken: 722.4325501918793 seconds

Had you to 'd?

RTANIO:
With is a truless peach was fall by that says.
You will not sleeposed to hand to do on
Friar that thou should have had not daily in
Should show well Richard to him as the Antiates,
And helps in'd blazy smother with a right.

JULIA:
Then Flather sitter, for grief spirit! ah I must,
Then goes hared, now his king. To true. My hence
Be take upon this court-shalt I know.

CAPULET:
Amen, that I will we stille have nothing
Would dibedit her friend be rife;
And then did stuck dis
</code></pre></div></div> <p>Train loss is higher when not using positional encoding but val loss is similar.</p> <h3 id="load-dataset-from-huggingface-locally">Load dataset from huggingface locally</h3> <p>The <code class="language-plaintext highlighter-rouge">datasets</code> library from Hugging Face allows you to load local dataset files. Here’s how you can do it:</p> <p>If your local file is a CSV or JSON file, you can use the <code class="language-plaintext highlighter-rouge">load_dataset</code> function with the ‘csv’ or ‘json’ parameter, and specify the path to your local file³. Here’s an example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># For a CSV file
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="sh">'</span><span class="s">path/to/your/file.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># For a JSON file
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">json</span><span class="sh">'</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="sh">'</span><span class="s">path/to/your/file.json</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Please replace <code class="language-plaintext highlighter-rouge">'path/to/your/file.csv'</code> and <code class="language-plaintext highlighter-rouge">'path/to/your/file.json'</code> with the actual paths to your files³.</p> <p>If you have a dataset saved locally that was previously processed and saved using the <code class="language-plaintext highlighter-rouge">datasets</code> library’s <code class="language-plaintext highlighter-rouge">save_to_disk</code> method, you can load it using the <code class="language-plaintext highlighter-rouge">load_from_disk</code> function⁴:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_from_disk</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_from_disk</span><span class="p">(</span><span class="sh">'</span><span class="s">path/to/your/dataset</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Please replace <code class="language-plaintext highlighter-rouge">'path/to/your/dataset'</code> with the actual path to your dataset⁴.</p> <p>Remember to handle any errors that might occur when loading the dataset to make your code more robust¹.</p> <h3 id="try-use-another-dataset">Try use another dataset</h3> <p>I use this persona dataset from hugging face.</p> <p><a href="https://huggingface.co/datasets/proj-persona/PersonaHub">https://huggingface.co/datasets/proj-persona/PersonaHub</a></p> <p>Data preprocessing script</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="nb">dir</span> <span class="o">=</span> <span class="sh">'</span><span class="s">/mnt/nvme1n1/zt/persona_dataset/PersonaHub/</span><span class="sh">'</span>

<span class="n">output_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">./</span><span class="sh">'</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="nb">dir</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">file</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.jsonl</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">file_without_suffix</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nb">dir</span><span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">json</span><span class="sh">"</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="n">path</span> <span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="sh">'</span><span class="s">input persona</span><span class="sh">'</span>
        <span class="k">if</span> <span class="sh">'</span><span class="s">input persona</span><span class="sh">'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">continue</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">input persona</span><span class="sh">'</span><span class="p">])</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">synthesized text</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">output_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">file_without_suffix</span> <span class="o">+</span> <span class="sh">'</span><span class="s">.txt</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">])):</span>
                <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">input persona</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
                <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">synthesized text</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p>Everything is the same as before including tokenizer.</p> <p>Code:</p> <p>Small transformer model: Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>m device cuda:0
step &lt;built-in function iter&gt;: train loss6.3584, val loss 6.3542
step &lt;built-in function iter&gt;: train loss2.5460, val loss 2.5392
step &lt;built-in function iter&gt;: train loss2.3751, val loss 2.3770
step &lt;built-in function iter&gt;: train loss2.3018, val loss 2.3027
step &lt;built-in function iter&gt;: train loss2.2458, val loss 2.2467
step &lt;built-in function iter&gt;: train loss2.2214, val loss 2.2201
step &lt;built-in function iter&gt;: train loss2.1890, val loss 2.1726
step &lt;built-in function iter&gt;: train loss2.1662, val loss 2.1680
step &lt;built-in function iter&gt;: train loss2.1399, val loss 2.1323
step &lt;built-in function iter&gt;: train loss2.1246, val loss 2.1263
Time taken: 187.76913928985596 seconds
        That Reprotionsray explewtrale, and seactivity add ders eserts sor skedsaling.

Te ***: Ats, undives, and ceintilies, provarting onroegose, in insope develourate the venta portital ofcreal as coutwival naginaps tochud vellegican chand in da Stral Vorle.
3+ **Subre Spione Figsiples and of oon asterst a ow and The wern**: Hure devaleir a keverst and arine wathe pesing gemtunizing happesor ondins toudes storvititiciects.
3. The talue can-Ed thig SEflichat and lalleD, to a arte's and and expald on c
</code></pre></div></div> <p>Large model: Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss0.9089, val loss 0.9231
Time taken: 759.7053854465485 seconds
Total parameters: 11102681
Trainable parameters: 11102681
        2. Bruck-Mining and the level of the Nethiopy class, with a focus on the below vasle assems.
3. **Jasar's "Dayslettle)**: This subsequent fertilization of humor and time was named from the tournament of this women. The creation of forpireʾle artists across sitell signifying water patterns, romantic beefwere data that gained tasting in family.

**Game and Name Implacement: A Improvemented Shaped Story**

A following student football in this growth, typically family, and stigma's approach landscap
</code></pre></div></div> <p>Looks better</p> <p>Even larger model: Params:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">256</span> <span class="c1"># what is the maximum context length for predictions
</span><span class="n">max_iters</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="c1"># device = 'cpu'
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">torch cuda available</span><span class="sh">'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">n_embd</span> <span class="o">=</span> <span class="mi">384</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">n_layer</span> <span class="o">=</span> <span class="mi">6</span>
<span class="c1"># head_size = 16
</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>

</code></pre></div></div> <p>Spent 6 times longer training time. We do see loss decrease though.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step &lt;built-in function iter&gt;: train loss0.7625, val loss 0.7859
Time taken: 4855.381325721741 seconds
Total parameters: 43635161
Trainable parameters: 43635161
        ll share stories that lessons they continue to have on Florist and the planet.

**Recent Recent Recognition: A Deep Dive into Your Authority**

When you begin to take a complex, it's essential. Our young music and information of florist taga), recognize the significance of "Rent Recognition in Baltin." This taggaent's contributions to fats of art, pushing it uses to engage with their examples with a similar panel that draw upon trade with fragmented flowing and dawn.

* Pay Flowing: Weed With a
</code></pre></div></div> <p>There is gpu memory usage flunctuation during training. Why is that?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch -n 0.1 nvidia-smi
</code></pre></div></div>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="ai"/><category term="llm"/><summary type="html"><![CDATA[llm]]></summary></entry></feed>