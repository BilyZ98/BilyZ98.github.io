<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> nanogpt kv cache first attempt | Zhutao Zhuang </title> <meta name="author" content="Zhutao Zhuang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bilyz98.github.io/blog/2025/llm-kv-cache-first-attempt/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhutao</span> Zhuang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">nanogpt kv cache first attempt</h1> <p class="post-meta"> Created in January 18, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a>   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="1-run-basic-nano-gpt">1. Run basic nano-gpt</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/karpathy/nanoGPT.git

</code></pre></div></div> <p>Install necessary packages</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install -r requirements.txt
</code></pre></div></div> <p>I have these packages in the requirements.txt</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>blobfile==2.0.1
certifi==2022.12.7
charset-normalizer==3.0.1
filelock==3.9.0
idna==3.4
lxml==4.9.2
numpy==1.24.2
pycryptodomex==3.17
pytz==2022.7.1
regex==2022.10.31
requests==2.28.2
tokenizers==0.13.2
torch==2.0.0
typing_extensions==4.4.0
urllib3==1.26.14
torch==2.0.0
numpy==1.24.2
transformers==4.28.1
datasets==2.11.0
tiktoken==0.3.3
wandb==0.14.2
tqdm==4.65.0
</code></pre></div></div> <p>Follow quick start guidance in nanogpt repo do make sure that we can run training and inference successfully.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python data/shakespeare_char/prepare.py
python train.py --compile=False config/train_shakespeare_char.py
python sample.py --out_dir=out-shakespeare-char
</code></pre></div></div> <p>My python version is 3.11 which is too high for model compile so I added <code class="language-plaintext highlighter-rouge">--compile=False</code> in train command.</p> <p>With my A800 gpu, I get a loss 0.0449 after 5000 iteration training.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>iter 4970: loss 0.0461, time 18.12ms, mfu 20.21%
iter 4980: loss 0.0441, time 18.14ms, mfu 20.24%
iter 4990: loss 0.0464, time 18.13ms, mfu 20.27%
step 5000: train loss 0.0383, val loss 4.7262
iter 5000: loss 0.0449, time 3352.84ms, mfu 18.26%
</code></pre></div></div> <h2 id="2-load-gpt-2-models--checkpoints-and-test-performance">2. Load GPT-2 models checkpoints and test performance</h2> <p>https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce</p> <p>proxy error while trying to download gpt2 model from huggingface: <a href="https://github.com/huggingface/transformers/issues/17611" rel="external nofollow noopener" target="_blank">https://github.com/huggingface/transformers/issues/17611</a></p> <p>First downgrad requests version to 2.27.1</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span><span class="nv">requests</span><span class="o">==</span>2.27.1
</code></pre></div></div> <p>And then adding these two lines of code in <code class="language-plaintext highlighter-rouge">train.py</code> and <code class="language-plaintext highlighter-rouge">sample.py</code> fix the proxy connection issue for me</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">CURL_CA_BUNDLE</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">''</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">HF_ENDPOINT</span><span class="sh">'</span><span class="p">]</span><span class="o">=</span> <span class="sh">'</span><span class="s">https://hf-mirror.com</span><span class="sh">'</span>
</code></pre></div></div> <p>Run <code class="language-plaintext highlighter-rouge">sample.py</code> to get a test of gpt2 model with params downloaded from huggingface.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> python sample.py --init_from='gpt2'
</code></pre></div></div> <p>I tried to start with “please tell me a joke.” The output is not anything like joke but still very readable.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>please tell me a joke

[…]

My name is Zarek, but I am extremely sad for you.

You can't even come to my house anymore

I'm sorry, I know

I have a dream

I don't know how long this thing will last

My name Is Zarek

I'm an adult who believes that

The problem with your friend is that he doesnt know

He doesn't know how to act
</code></pre></div></div> <p>running time for 10 times inference:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------
Elapsed time: 25.4s
</code></pre></div></div> <h2 id="3-implement-kv-cache-for-faster-inference">3. Implement KV cache for faster inference</h2> <p><a href="https://github.com/BilyZ98/nano-gpt-kv-cache/commit/606e4e4e881db6c769e0bdca51bdac96f00a55e1" rel="external nofollow noopener" target="_blank">Commit hisotry for kv cache implementation</a></p> <p>Please check code above for implementation details.</p> <p>Issue:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>shape of past k proj is  torch.Size([1, 12, 946, 64])
shape of k is  torch.Size([1, 12, 44, 64]) shape of v is  torch.Size([1, 12, 44, 64])
q len is  45
shape of past k proj is  torch.Size([1, 12, 990, 64])
shape of k is  torch.Size([1, 12, 45, 64]) shape of v is  torch.Size([1, 12, 45, 64])
Traceback (most recent call last):
  File "/GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/sample.py", line 93, in &lt;module&gt;
    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py", line 359, in generate
    logits, _, past_kv_proj = self(idx_cond, past_kv_proj=past_kv_proj,start_pos=start_pos)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py", line 204, in forward
    x, layer_kv_proj = block(x, past_kv_proj=past_kv_proj[i])
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py", line 122, in forward
    attn_res, present_kv_proj = self.attn(self.ln_1(x), past_kv_proj=past_kv_proj)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py", line 78, in forward
    assert KV &lt; self.block_size, f"KV: {KV} &gt;= block_size: {self.block_size}"
           ^^^^^^^^^^^^^^^^^^^^
AssertionError: KV: 1035 &gt;= block_size: 1024
yhrun: error: gpu73: task 0: Exited with exit code 1
(nano-gpt-kv-cache) [nsccgz_qylin_1@ln101 nano-gpt-kv-cache]$
</code></pre></div></div> <p>Fix</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
            <span class="c1"># This is the righ condition
</span>            <span class="k">if</span> <span class="n">idx</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="o">==</span> <span class="n">T</span><span class="p">:</span>
                <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">start_pos</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
                <span class="n">start_pos</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</code></pre></div></div> <p>The limitation of this code is that it can only handles condition where <code class="language-plaintext highlighter-rouge">max_new_tokens &lt; self.config.block_size</code></p> <p>I don’t know why yet.</p> <h2 id="4-test-kv-cache-performance">4. Test KV cache performance</h2> <p><a href="https://github.com/karpathy/nanoGPT/pull/76" rel="external nofollow noopener" target="_blank">The commit</a> mentions that it only brings performance boost with cpu but not on A100 gpu. Why is that ? Is this because that linear projections can be quickly done with fast gpu matrix multiplication?</p> <p><a href="https://github.com/huggingface/transformers/pull/14118/files" rel="external nofollow noopener" target="_blank">This commit</a> <a href="https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227" rel="external nofollow noopener" target="_blank">and this discussion</a> talks about how to handle long text generation. I have not yet understanded it completely how it deals with long text geneartion.</p> <p>There is a technique called rotary positional embeddings as mentioned in this <a href="https://github.com/karpathy/nanoGPT/pull/76" rel="external nofollow noopener" target="_blank">commit</a>. But I don’t know how does it works yet. And all I want to do right now is to simply test how kv cache helps with inference speed.</p> <p>My naive solution right now is to simply cut past_kv_proj to latest self.config.block_size tokens</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            if past_kv_proj is not None:
                past_k_proj, past_v_proj = past_kv_proj
                print('shape of past k proj is ', past_k_proj.shape )
                print('shape of k is ', k.shape, 'shape of v is ', v.shape)
                if KV &gt;= self.block_size:
                    past_k_proj = past_k_proj[:, :, -self.block_size:, :]
                    past_v_proj = past_v_proj[:, :, -self.block_size:, :]
                k = torch.cat((past_k_proj, k), dim=2)
                v = torch.cat((past_v_proj, v), dim=2)

</code></pre></div></div> <p>gpu v100</p> <p>with kv cache, no flash attention</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> yhrun -p gpu_v100  python   sample.py --init_from='gpt2'  --use_kv_cache=True --dtype=float32  --num_samples=10 --max_
new_tokens=1000
</code></pre></div></div> <p>time:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------
Elapsed time: 102.6s
</code></pre></div></div> <p>memory: almost the same for peak memory usage ?</p> <p>without kv cache, no flash attention</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python   sample.py --init_from='gpt2'  --use_kv_cache=False --dtype=float32  --num_samples=10 --max_new_tokens=1000
</code></pre></div></div> <p>time:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Elapsed time: 151.8s
</code></pre></div></div> <p>memory:</p> <p>Saves 30% time. Not bad.</p> <p>Function profile: I run 10 times, each with 1000 generation sequence length.</p> <p>with kv cache:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      15990616 function calls (14380614 primitive calls) in 110.086 seconds

   Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
   120000   21.130    0.000   56.527    0.000 /GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py:58(forward)
   490000   14.545    0.000   14.545    0.000 {built-in method torch._C._nn.linear}
1620000/10000   13.119    0.000  103.105    0.010 /GPUFS/nsccgz_qylin_1/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1494(_call_impl)
  3420000    9.789    0.000    9.789    0.000 /GPUFS/nsccgz_qylin_1/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1601(__getattr__)
   120000    5.108    0.000   97.206    0.001 /GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py:135(forward)
   250000    4.809    0.000    4.809    0.000 {built-in method torch.layer_norm}
   120000    3.840    0.000    3.840    0.000 {method 'masked_fill' of 'torch._C._TensorBase' objects}
       10    3.737    0.374  109.808   10.981 /GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py:350(generate)
   250000    3.694    0.000    3.694    0.000 {built-in method torch.cat}
   120000    2.772    0.000   20.640    0.000 /GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py:113(forward)
   370000    2.268    0.000    3.704    0.000 /GPUFS/nsccgz_qylin_1/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:1235(dropout)

    10000    2.075    0.000  102.922    0.010 /GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py:206(forward)

</code></pre></div></div> <p>without kv cache</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       15380126 function calls (13770124 primitive calls) in 154.658 seconds

   Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)

    10000   45.431    0.005  148.326    0.015 /GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py:206(forward)

   120000   22.252    0.000   58.175    0.000 /GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py:58(forward)
   490000   17.891    0.000   17.891    0.000 {built-in method torch._C._nn.linear}
1620000/10000   13.188    0.000  148.485    0.015 /GPUFS/nsccgz_qylin_1/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1494(_call_impl)
  3420000    9.488    0.000    9.488    0.000 /GPUFS/nsccgz_qylin_1/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1601(__getattr__)
   120000    5.289    0.000   99.160    0.001 /GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py:135(forward)
   250000    5.037    0.000    5.037    0.000 {built-in method torch.layer_norm}
   120000    4.087    0.000    4.087    0.000 {method 'masked_fill' of 'torch._C._TensorBase' objects}
       10    2.952    0.295  154.334   15.433 /GPUFS/nsccgz_qylin_1/zt/nano-gpt-kv-cache/model.py:350(generate)
</code></pre></div></div> <p>The main time difference comes from the function call to self attention block. Per call time for without kv cache gpt is 0.015 and it’s 0.010 for with kv cache. I guess this explains why benefit of kv cache for short sequence geneartion on A100 is negligible because it takes very short amount of time to generate key and value embedding with more advanced gpu.</p> <p>500 tokens, cpu</p> <p>with kv cache,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The law gives the government access to consumer information only if the government's purpose is to provide health care to the general public. If those
---------------
Elapsed time: 218.9s


The law gives the government access to consumer information only if the government's purpose is to provide health care to the general public. If those
---------------
Elapsed time: 251.4s
</code></pre></div></div> <p>without kv cache</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The law gives the government access to consumer information only if the government's purpose is to provide health care to the general public. If those
---------------
Elapsed time: 1191.4s
</code></pre></div></div> <p>5 times inference time saving. Not bad.</p> <p>The peak memory usage between with kv cache and without kv cache is nearly the same. This is because that sequence length is the same with or without kv cache. However, kv cache do bring some advantages. Here’s the answer from gpt.</p> <blockquote> <p>Actually, there is a difference in memory usage when using KV cache for LLM inference. While it’s true that the maximum memory usage might be similar, the way memory is utilized and managed can vary significantly.</p> <ol> <li> <strong>Memory Allocation</strong>: With KV cache, memory is allocated for storing key-value pairs from previous computations. This can lead to more efficient memory usage as the model doesn’t need to recompute values, reducing the overall memory footprint during inference.</li> <li> <strong>Memory Management</strong>: KV cache helps in better memory management by reusing previously computed values. This can lead to more stable memory usage patterns, avoiding spikes in memory consumption that might occur without caching.</li> <li> <strong>Performance Optimization</strong>: By reducing redundant computations, KV cache can lead to faster inference times, which indirectly affects memory usage. Faster computations mean less time spent holding intermediate values in memory, leading to more efficient memory utilization.</li> </ol> </blockquote> <h2 id="references">References</h2> <p><a href="https://www.youtube.com/watch?v=80bIUggRJf4&amp;t=247s" rel="external nofollow noopener" target="_blank">youtube video llm kv cache explanation</a></p> <p><a href="https://github.com/karpathy/nanoGPT/pull/246/commits/5cc9bab7e2402caf69a00e9c38fc45517e958748" rel="external nofollow noopener" target="_blank">requirements.txt to run nano-gpt</a></p> <p><a href="https://github.com/karpathy/nanoGPT/pull/76" rel="external nofollow noopener" target="_blank">nano-gpt kv cache pr example</a></p> <p><a href="https://github.com/huggingface/transformers/blob/6bc0fbcfa7acb6ac4937e7456a76c2f7975fefec/src/transformers/modeling_outputs.py#L714" rel="external nofollow noopener" target="_blank">huggingface transformers kv cache source code on github</a></p> <p>https://zhuanlan.zhihu.com/p/646577898</p> <p>https://zhuanlan.zhihu.com/p/624740065</p> <p><a href="https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast" rel="external nofollow noopener" target="_blank">huggingface transformers API documentation</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/learning-based-memory-allocation-for-c-server-workloads-summary/">Learning-based memory allocation for C++ server workloads summary</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/my-question/">my question:</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/binary-search-algorithm-variant/">Binary search algorithm variant</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/docker-rocksdb-build/">Docker Rocksdb build</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/difference-between-dockerfile-and-docker-compose/">Difference between Dockerfile and Docker Compose</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhutao Zhuang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: February 14, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-lock-free-queue-implementation-cpp",title:"Lock free queue implementation cpp",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/lock-free-queue/"}},{id:"post-nanogpt-kv-cache-first-attempt",title:"nanogpt kv cache first attempt",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/llm-kv-cache-first-attempt/"}},{id:"post-set-up-vim-in-vscode",title:"Set up vim in vscode",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/set-up-vim-in-vscode/"}},{id:"post-chibicc-simple-c-compiler-for-statement",title:"chibicc - Simple c compiler for statement",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-for-statement/"}},{id:"post-chibicc-simple-c-compiler-if-statement",title:"chibicc - Simple c compiler if statement",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-if-statement/"}},{id:"post-chibicc-simple-c-compiler-block-node",title:"chibicc - Simple c compiler block {} node",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-block/"}},{id:"post-chibicc-simple-c-compiler-return-keyword",title:"chibicc - Simple c compiler return keyword",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-return-keyword/"}},{id:"post-learned-idnex-survey",title:"Learned idnex survey",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/learned-index-survery/"}},{id:"post-chibicc-c-compiler-multi-char-variable-name",title:"chibicc C compiler - multi char variable name",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-multi-char-var-name/"}},{id:"post-c-and-linux-kernel-memory-allocation",title:"C++ and linux kernel memory allocation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/cpp-interview-questions/"}},{id:"post-chibicc-c-compiler-parser-review-and-expression-evaluator",title:"chibicc C compiler - parser review and expression evaluator",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-parser-review-and-calculator/"}},{id:"post-imperative-programming-vs-declarative-programming",title:"Imperative programming vs. Declarative programming",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/imperative-programming/"}},{id:"post-autodiff-implementation-kernel-and-memory-management",title:"Autodiff implementation - kernel and memory management",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign2/"}},{id:"post-hash-in-cpp",title:"Hash in cpp",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hash-in-cpp/"}},{id:"post-computer-basics",title:"Computer basics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/some-basic-computer-knowledge/"}},{id:"post-rdma",title:"Rdma",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/rdma/"}},{id:"post-topo-sort",title:"Topo sort",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/topo-sort/"}},{id:"post-autodiff-implementation",title:"Autodiff implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign/"}},{id:"post-palindrome-substring-partition",title:"palindrome substring partition",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/palindrome-substring-partition/"}},{id:"post-stf-cs149-flash-attention",title:"Stf CS149 flash attention",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149gpt/"}},{id:"post-speed-up-matrix-multiplication-2",title:"Speed up matrix multiplication 2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-mul-comparison/"}},{id:"post-elf-loading",title:"Elf Loading",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/elf-loading/"}},{id:"post-stf-cs149-parallel-programming-assign3",title:"Stf CS149 Parallel Programming - Assign3",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign3/"}},{id:"post-stf-cs149-parallel-programming-lecture11-cache-coherence",title:"Stf CS149 Parallel Programming - Lecture11 - Cache coherence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture11-cache-coherence/"}},{id:"post-stf-cs149-parallel-programming-lecture-7-cuda-programming-model",title:"Stf CS149 Parallel Programming - Lecture 7 - Cuda programming model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture7-cuda/"}},{id:"post-stf-cs149-parallel-programming-assign2",title:"Stf CS149 Parallel Programming - Assign2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign2/"}},{id:"post-stf-cs149-parallel-programming-lecture-5-amp-6-performance-optimization",title:"Stf CS149 Parallel Programming - Lecture 5&amp;6 - Performance optimization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-lecture-takeaway/"}},{id:"post-stf-cs149-parallel-programming-assign1",title:"Stf CS149 Parallel Programming - Assign1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-assign1/"}},{id:"post-ssh-display-image-on-local-server",title:"ssh display image on local server",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/ssh-display-img/"}},{id:"post-c-compiler-single-letter-local-variable",title:"C compiler - single letter local variable",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-single-letter-ident/"}},{id:"post-c-compiler-parse-example-walkthrough",title:"C compiler - parse example walkthrough",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/statement-and-comparison/"}},{id:"post-linux-get-cpu-time-and-wall-clock-time",title:"Linux get cpu time and wall clock time",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/get-function-cpu-time/"}},{id:"post-simple-c-compiler-unary",title:"Simple c compiler unary",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-unary/"}},{id:"post-simple-c-compiler-gen-expr",title:"Simple c compiler gen expr",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-gen-expr/"}},{id:"post-python-pyplot-trick",title:"Python pyplot trick",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-plot-trick/"}},{id:"post-simple-lru-cache-cpp-implementation",title:"Simple lru cache cpp implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lru-cache/"}},{id:"post-python-capture-function-print-output",title:"Python capture function print output",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/capture-output-python/"}},{id:"post-simple-c-compiler",title:"Simple c compiler",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-compiler/"}},{id:"post-calloc-and-malloc",title:"calloc and malloc",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/calloc/"}},{id:"post-lightgbm-dataset",title:"LightGBM dataset",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-dataset/"}},{id:"post-difference-between-deep-copy-and-shallow-copy-in-python",title:"Difference between deep copy and shallow copy in python",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-deep-copy/"}},{id:"post-python-package-path",title:"Python Package Path",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-package-path/"}},{id:"post-micrograd",title:"micrograd",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/micrograd/"}},{id:"post-cpp-thread-local",title:"cpp thread local",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-thread-local/"}},{id:"post-cpp-async",title:"cpp async",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-async/"}},{id:"post-python-dataframe-drop-row",title:"python dataframe drop row",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/df-drop-row/"}},{id:"post-git-merge-file-from-another-branch",title:"Git merge file from another branch",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/git-merge-file-from-another-branch/"}},{id:"post-efficiency-tips",title:"Efficiency tips",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/efficiency-tips/"}},{id:"post-speed-up-matrix-multiplication",title:"Speed up matrix multiplication",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-multiplication/"}},{id:"post-how-to-write-research-paper",title:"How to write research paper",description:"How to write research paper",section:"Posts",handler:()=>{window.location.href="/blog/2024/how-to-write-research-paper/"}},{id:"post-fast-nano-gpt-training",title:"Fast nano-gpt training",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/gpt-fast/"}},{id:"post-system-for-machine-learning-papers",title:"System for machine learning papers",description:"sysml papers",section:"Posts",handler:()=>{window.location.href="/blog/2024/sysml-papers/"}},{id:"post-nano-gpt-and-transformer",title:"nano-gpt and Transformer",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer/"}},{id:"post-pytorch-tensor-to",title:"pytorch tensor.to",description:"pytorch",section:"Posts",handler:()=>{window.location.href="/blog/2024/pytorch/"}},{id:"post-install-neovim-with-old-glibc",title:"Install neovim with old glibc",description:"vim",section:"Posts",handler:()=>{window.location.href="/blog/2024/install-neovim/"}},{id:"post-llm-c",title:"llm.c",description:"llm minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/llm-c/"}},{id:"post-basic-digital-electronic",title:"Basic digital electronic",description:"transistor",section:"Posts",handler:()=>{window.location.href="/blog/2024/digital-electronic/"}},{id:"post-k8s-advance",title:"K8s Advance",description:"k8s minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/k8s-advance/"}},{id:"post-difference-between-dockerfile-and-docker-compose",title:"Difference between Dockerfile and Docker Compose",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/difference-between-dockerfile-and-docker-compose-d6ebdc687785?source=rss-da1663a42461------2","_blank")}},{id:"post-docker-rocksdb-build",title:"Docker Rocksdb build",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/docker-rocksdb-build-18a0bf0e0bb0?source=rss-da1663a42461------2","_blank")}},{id:"post-k3s-beginner",title:"K3s beginner",description:"k3s",section:"Posts",handler:()=>{window.location.href="/blog/2024/k3s/"}},{id:"post-docker-rocksdb",title:"Docker RocksDB",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-rocksdb/"}},{id:"post-docker-beginner",title:"Docker beginner",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-file-compose-diff/"}},{id:"post-git",title:"Git",description:"git",section:"Posts",handler:()=>{window.location.href="/blog/2024/git/"}},{id:"post-lightgbm-usage-and-implementation",title:"LightGBM usage and implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-usage/"}},{id:"post-backpropogation-c-implementation",title:"Backpropogation C++ Implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/back-propagation/"}},{id:"post-conda-usage",title:"Conda usage",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/conda/"}},{id:"post-install-k8s-cluster-with-3-ubuntu-nodes",title:"Install K8s cluster with 3 ubuntu nodes",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/cloud/"}},{id:"post-convert-svg-figures-to-pdf-latex-before-submitting-to-arxiv",title:"Convert SVG figures to pdf_latex before submitting to arxiv",description:"Convert SVG figures to pdf_latex before submitting to arxiv",section:"Posts",handler:()=>{window.location.href="/blog/2024/arxiv-cleaner/"}},{id:"post-binary-search-algorithm-variant",title:"Binary search algorithm variant",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/binary-search-algorithm-variant-9b5310473471?source=rss-da1663a42461------2","_blank")}},{id:"post-my-question",title:"my question:",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/my-question-a69930f167f0?source=rss-da1663a42461------2","_blank")}},{id:"post-learning-based-memory-allocation-for-c-server-workloads-summary",title:"Learning-based memory allocation for C++ server workloads summary",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/learning-based-memory-allocation-for-c-server-workloads-summary-479e9cd6d6f6?source=rss-da1663a42461------2","_blank")}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-paper-dumpkv-accepted-by-vldb-39-25",title:"A paper(DumpKV) accepted by VLDB&#39;25",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%68%75%61%6E%67%7A%68%75%74%61%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/BilyZ98","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>