<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Autodiff implementation - kernel and memory management | Zhutao Zhuang </title> <meta name="author" content="Zhutao Zhuang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bilyz98.github.io/blog/2024/uw-sysml-assign2/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhutao</span> Zhuang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Autodiff implementation - kernel and memory management</h1> <p class="post-meta"> Created in December 24, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/framework"> <i class="fa-solid fa-hashtag fa-sm"></i> framework</a>   ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a>   <a href="/blog/category/framework"> <i class="fa-solid fa-tag fa-sm"></i> framework</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This is fun to implement</p> <p>Never aware that we can do management in python language like this.</p> <p>I always think that memory management in only done in cpp.</p> <p>Interesting, so interesting.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def memory_plan(self, feed_shapes):
        """Allocates tvm.nd.array for every node except feed_dict nodes.

        Implementation note:
        Easy Option: Alloc a tvm.nd.array per node that persists across run()

        Use self.node_to_arr_map to store node-&gt;tvm.nd.array mapping
        to allow mapping to persist across multiple executor.run().

        Parameters
        ----------
        feed_shapes: node-&gt;shapes mapping for feed_dict nodes.
        """
        """TODO: Your code here"""


</code></pre></div></div> <h2 id="what-is-placeholder-in-tvm-">What is placeholder in tvm ?</h2> <p>In TVM, a <code class="language-plaintext highlighter-rouge">placeholder</code> is used to define an input tensor for a computation. It essentially acts as a symbolic representation of the input data, specifying the shape and optionally the data type. This is useful for defining the structure of the computation graph before actual data is provided.</p> <p>Here’s a simple example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="c1"># Define the shape of the input tensor
</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="sh">"</span><span class="s">float32</span><span class="sh">"</span>

<span class="c1"># Create a placeholder tensor
</span><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Print the placeholder tensor
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Placeholder tensor A:</span><span class="sh">"</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</code></pre></div></div> <p>In this example:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">te.placeholder</code> creates a tensor <code class="language-plaintext highlighter-rouge">A</code> with the specified shape <code class="language-plaintext highlighter-rouge">(3, 4)</code> and data type <code class="language-plaintext highlighter-rouge">"float32"</code>.</li> <li>This tensor can then be used in further computations, such as defining operations that use <code class="language-plaintext highlighter-rouge">A</code> as input.</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">placeholder</code> is a fundamental concept in TVM’s Tensor Expression (TE) language, allowing you to build complex computation graphs by defining how data flows through various operations¹².</p> <p>¹: <a href="https://tvm.apache.org/docs/reference/api/python/te.html" rel="external nofollow noopener" target="_blank">TVM Documentation</a></p> <p>²: <a href="https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html" rel="external nofollow noopener" target="_blank">TVM Tutorial on Tensor Expressions</a></p> <p>(1) tvm.te — tvm 0.19.dev0 documentation - The Apache Software Foundation. https://tvm.apache.org/docs/reference/api/python/te.html.</p> <p>(2) TVM 自底向上（三）：TE 的概念和编译原理 - 知乎. https://zhuanlan.zhihu.com/p/534313816.</p> <p>(3) Working with Operators Using Tensor Expression — tvm 0.19.dev0 …. https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html.</p> <p>(4) TVM学习（六）细读前端 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/346514871.</p> <p>(5) External Tensor Functions — tvm 0.19.dev0 documentation. https://tvm.apache.org/docs/how_to/work_with_schedules/extern_op.html.</p> <h2 id="tvm-create-ndarray-with-specified-shape">tvm create nd.array with specified shape</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="c1"># Specify the shape and data type
</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="sh">"</span><span class="s">float32</span><span class="sh">"</span>

<span class="c1"># Create an empty TVM NDArray with the specified shape
</span><span class="n">tvm_array</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Print the TVM NDArray
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TVM NDArray with specified shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tvm_array</span><span class="p">)</span>

</code></pre></div></div> <h2 id="difference-between-tvm-and-ncnn">Difference between tvm and ncnn</h2> <p>tvm is a open source deep learning compiler for cpus, gpus and specialized hardware.</p> <p>ncnn is a neural network inference framework optimized for mobile and embedded devices.</p> <p>We can assume input shape of mat_mul is 2d in this task.</p> <h2 id="matmul-kernel-for-unknown-input-dimension-shape">matmul kernel for unknown input dimension shape</h2> <p>This code is from gpt.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span>

<span class="k">def</span> <span class="nf">make_flexible_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="c1"># Determine the shapes of the input tensors
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">shapeA</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">shapeA</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">shapeA</span>
    
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">shapeB</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">shapeB</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">shapeB</span>
    
    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch</span> <span class="nf">else </span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch</span> <span class="nf">else </span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Create placeholders for the input tensors
</span>    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    
    <span class="c1"># Define the reduction axis
</span>    <span class="n">k_axis</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># Compute the matrix multiplication based on transpose flags and dimensionality
</span>    <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeA</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeA</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">transposeB</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k_axis</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k_axis</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
            <span class="p">)</span>
    
    <span class="c1"># Create a schedule for the computation
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    
    <span class="c1"># Apply optimizations: split, reorder, vectorize, parallel
</span>    <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">bo</span><span class="p">,</span> <span class="n">bi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">io</span><span class="p">,</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">jo</span><span class="p">,</span> <span class="n">ji</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">bo</span><span class="p">,</span> <span class="n">io</span><span class="p">,</span> <span class="n">jo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">bi</span><span class="p">,</span> <span class="n">ii</span><span class="p">,</span> <span class="n">ji</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">bo</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">io</span><span class="p">,</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">jo</span><span class="p">,</span> <span class="n">ji</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">io</span><span class="p">,</span> <span class="n">jo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">ii</span><span class="p">,</span> <span class="n">ji</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">ki</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">io</span><span class="p">)</span>
    
    <span class="c1"># Lower the schedule to generate the IR code
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">tvm</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    
    <span class="c1"># Build the function
</span>    <span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">func</span>

<span class="c1"># Example usage
</span><span class="n">tgt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">tgt_host</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">func_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">flexible_matrix_mul</span><span class="sh">"</span>
<span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># Batch size of 32
</span><span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>  <span class="c1"># Batch size of 32
</span><span class="n">transposeA</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">transposeB</span> <span class="o">=</span> <span class="bp">False</span>
<span class="nf">make_flexible_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">)</span>

</code></pre></div></div> <h2 id="conv2d-tvm-kernel">conv2d tvm kernel</h2> <p>Code is generated by gpt</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="n">tvm</span> <span class="kn">import</span> <span class="n">te</span><span class="p">,</span> <span class="n">topi</span>

<span class="k">def</span> <span class="nf">make_conv2d</span><span class="p">(</span><span class="n">shapeX</span><span class="p">,</span> <span class="n">shapeF</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">assert</span><span class="p">(</span><span class="n">shapeX</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeF</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">shapeX</span>
    <span class="n">M</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">shapeF</span>

    <span class="c1"># Create placeholders for the input tensor and filter
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">F</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># Define the reduction axes
</span>    <span class="n">rc</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rc</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">rr</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">R</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rr</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">rs</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">rs</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Compute the convolution
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">H</span> <span class="o">-</span> <span class="n">R</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">S</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
        <span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="n">rr</span><span class="p">,</span> <span class="n">w</span> <span class="o">+</span> <span class="n">rs</span><span class="p">]</span> <span class="o">*</span> <span class="n">F</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">]),</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span>
    <span class="p">)</span>

    <span class="c1"># Create a schedule for the computation
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">Y</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># Apply optimizations: split, reorder, vectorize, parallel
</span>    <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
    <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span>
    <span class="n">ho</span><span class="p">,</span> <span class="n">hi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">wo</span><span class="p">,</span> <span class="n">wi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">ho</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">hi</span><span class="p">,</span> <span class="n">wi</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">rr</span><span class="p">,</span> <span class="n">rs</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">Y</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">ho</span><span class="p">)</span>

    <span class="c1"># Lower the schedule to generate the IR code
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">tvm</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="c1"># Build the function
</span>    <span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">func</span>

<span class="c1"># Example usage
</span><span class="n">tgt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">tgt_host</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llvm</span><span class="sh">"</span>
<span class="n">func_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">conv2d</span><span class="sh">"</span>
<span class="n">shapeX</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>  <span class="c1"># Example input shape (N, C, H, W)
</span><span class="n">shapeF</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   <span class="c1"># Example filter shape (M, C, R, S)
</span><span class="nf">make_conv2d</span><span class="p">(</span><span class="n">shapeX</span><span class="p">,</span> <span class="n">shapeF</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">)</span>


</code></pre></div></div> <h2 id="install-tvm-by-building-from-source">Install tvm by building from source</h2> <p>Can not install tvm through pip. Have to download source code and build it myself. I don’t know why.</p> <p>Folow steps in this <a href="https://github.com/apache/tvm/issues/13507" rel="external nofollow noopener" target="_blank">issue</a> to compile locally.</p> <p>Need to disable gtest inroder to pass cmake</p> <p><a href="https://tvm.apache.org/docs/install/from_source.html#install-from-source" rel="external nofollow noopener" target="_blank">Offical install document</a></p> <p>Finally finish installing tvm after building locally.</p> <p>Run this command to verify</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>uwsyml<span class="o">)</span> ➜  tvm git:<span class="o">(</span>main<span class="o">)</span> python <span class="nt">-c</span> <span class="s2">"import tvm; print(tvm.__file__)"</span>
/mnt/nvme1n1/zt/tvm/python/tvm/__init__.py
</code></pre></div></div> <p>nosetests does not use python in conda. It uses that in /usr/bin which is not what I want.</p> <p>It reports error that it can not find numpy which I have already installed in conda environment</p> <h2 id="update-code-to-use-latest-function-in-tvm-instead-of-old-function-in-tvm">Update code to use latest function in tvm instead of old function in tvm</h2> <p>New code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="k">def</span> <span class="nf">test_matrix_elementwise_mul</span><span class="p">():</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">arr_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_y</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_z</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">elemwise_mul</span> <span class="o">=</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_mul</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_add</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">elemwise_mul</span><span class="p">(</span><span class="n">arr_x</span><span class="p">,</span> <span class="n">arr_y</span><span class="p">,</span> <span class="n">arr_z</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">arr_z</span><span class="p">.</span><span class="nf">asnumpy</span><span class="p">()</span>
    <span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>


    
<span class="k">def</span> <span class="nf">make_elemwise_mul</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <p>Old code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">test_matrix_elementwise_mul_by_const</span><span class="p">():</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">const_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">arr_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">arr_y</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">elemwise_mul_by_const</span> <span class="o">=</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_mul_by_const</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">const_val</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_mul_by_const</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">elemwise_mul_by_const</span><span class="p">(</span><span class="n">arr_x</span><span class="p">,</span> <span class="n">arr_y</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">arr_y</span><span class="p">.</span><span class="nf">asnumpy</span><span class="p">()</span>
    <span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">const_val</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_elemwise_mul_by_const</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">const_k</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">const_k</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>
</code></pre></div></div> <h3 id="how-node-value-is-stored-in-autodiff-">How node value is stored in autodiff ?</h3> <p>It’s stored in dict <code class="language-plaintext highlighter-rouge">node_to_val_map</code></p> <p>The graph is just computation graph. Each node is an operation node.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">convert_to_numpy_ret_vals</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Parameters
        ----------
        feed_dict: a dictionary of node-&gt;np.ndarray supplied by user.
        convert_to_numpy_ret_vals: whether to convert ret vals to np.array.

        Returns
        -------
        A list of values for nodes in eval_node_list. tvm.nd.array or np.ndarray.
        </span><span class="sh">"""</span>
        <span class="k">def</span> <span class="nf">are_feed_shapes_equal</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="n">sb</span><span class="p">):</span>
            <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sb</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
                <span class="k">return</span> <span class="bp">False</span>
            <span class="n">unmatched_item</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">sa</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span> <span class="o">^</span> <span class="nf">set</span><span class="p">(</span><span class="n">sb</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
            <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">unmatched_item</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="n">node_to_val_map</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">feed_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">ndarray</span><span class="p">.</span><span class="n">NDArray</span><span class="p">),</span>\
                <span class="sh">"</span><span class="s">feed_dict value type not supported</span><span class="sh">"</span>    
            <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

</code></pre></div></div> <h3 id="how-node-computation-is-done-in-graph-">How node computation is done in graph ?</h3> <p>In run function , <code class="language-plaintext highlighter-rouge">compute</code> is called for each operation node.</p> <p>Each opeartion node in computation graph has its own compute function.</p> <p>For example , <code class="language-plaintext highlighter-rouge">AddOp</code> has its own compute function and this compute function calls passed-in compiled_func to do the function call from compiled code.</p> <p>Note that this <code class="language-plaintext highlighter-rouge">compiled_func</code> is built before forward of computation graph.</p> <p>And the return function of <code class="language-plaintext highlighter-rouge">make_elemwise_add</code> is a tvm build function that takes <code class="language-plaintext highlighter-rouge">[A,B,C]</code> three tensor as input instead of parameters in <code class="language-plaintext highlighter-rouge">compute</code> function.</p> <p><code class="language-plaintext highlighter-rouge">tgt</code> and <code class="language-plaintext highlighter-rouge">shape</code> is defined during function compilation in <code class="language-plaintext highlighter-rouge">make_elemwise_add</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">convert_to_numpy_ret_vals</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Parameters
        ----------
        feed_dict: a dictionary of node-&gt;np.ndarray supplied by user.
        convert_to_numpy_ret_vals: whether to convert ret vals to np.array.

        Returns
        -------
        A list of values for nodes in eval_node_list. tvm.nd.array or np.ndarray.
        </span><span class="sh">"""</span>
        <span class="k">def</span> <span class="nf">are_feed_shapes_equal</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="n">sb</span><span class="p">):</span>
            <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sa</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sb</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
                <span class="k">return</span> <span class="bp">False</span>
            <span class="n">unmatched_item</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">sa</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span> <span class="o">^</span> <span class="nf">set</span><span class="p">(</span><span class="n">sb</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
            <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">unmatched_item</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="n">node_to_val_map</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">feed_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">ndarray</span><span class="p">.</span><span class="n">NDArray</span><span class="p">),</span>\
                <span class="sh">"</span><span class="s">feed_dict value type not supported</span><span class="sh">"</span>    
            <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># collect shapes for all placeholders
</span>        <span class="n">feed_shapes</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_to_val_map</span><span class="p">:</span>
            <span class="n">feed_shapes</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">].</span><span class="n">shape</span>

        <span class="c1"># infer shape if feed_shapes changed since last run
</span>        <span class="c1"># e.g. call run() on test data after trainng
</span>        <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="nf">are_feed_shapes_equal</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">feed_shapes</span><span class="p">)):</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">infer_shape</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">feed_shapes</span> <span class="o">=</span> <span class="n">feed_shapes</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">memory_plan</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">compile_funcs</span><span class="p">(</span><span class="n">feed_shapes</span><span class="p">)</span>

        <span class="c1"># Traverse graph in topo order and compute values for all nodes.
</span>        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">topo_order</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_to_val_map</span><span class="p">:</span>
                <span class="c1"># Skip placeholder nodes. Values already provided by feed_dict.
</span>                <span class="k">continue</span>
            <span class="n">input_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_to_val_map</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">node_val</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">node_to_arr_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
            <span class="c1"># node_val is modified in-place
</span>            <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="n">node</span><span class="p">,</span> <span class="n">input_vals</span><span class="p">,</span> <span class="n">node_val</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">node_to_compiled_func</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
            <span class="n">node_to_val_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_val</span>
        <span class="c1"># Collect node values.
</span>        <span class="k">if</span> <span class="n">convert_to_numpy_ret_vals</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">node_to_val_map</span><span class="p">[</span><span class="n">n</span><span class="p">].</span><span class="nf">asnumpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">eval_node_list</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">node_to_val_map</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">eval_node_list</span><span class="p">]</span>


</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AddOp</span><span class="p">(</span><span class="n">Op</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node_A</span><span class="p">,</span> <span class="n">node_B</span><span class="p">):</span>
        <span class="n">new_node</span> <span class="o">=</span> <span class="n">Op</span><span class="p">.</span><span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
        <span class="n">new_node</span><span class="p">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_A</span><span class="p">,</span> <span class="n">node_B</span><span class="p">]</span>
        <span class="n">new_node</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">(%s+%s)</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">node_A</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">node_B</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_node</span>

    <span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">input_vals</span><span class="p">,</span> <span class="n">output_val</span><span class="p">,</span> <span class="n">compiled_func</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">input_vals</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">assert</span> <span class="n">input_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span> <span class="o">==</span> <span class="n">input_vals</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">shape</span>
        <span class="nf">compiled_func</span><span class="p">(</span><span class="n">input_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_vals</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_val</span><span class="p">)</span>  

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">output_grad</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">input_shapes</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Need to handle input_vals[0].shape != input_vals[1].shape</span><span class="sh">"""</span>
        <span class="k">return</span> <span class="nf">broadcast_rule</span><span class="p">(</span><span class="n">input_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">compiled_func</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">input_shapes</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tvm_op</span><span class="p">.</span><span class="nf">make_elemwise_add</span><span class="p">(</span>
            <span class="n">input_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="sh">"</span><span class="s">elem_add</span><span class="sh">"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_elemwise_add</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">B</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="nc">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="nc">B</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <h3 id="matrix_mul-impl-debug">matrix_mul impl debug</h3> <p>Get this erro while debgging matrix_mul</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>======================================================================
ERROR: test_tvm_op.test_matrix_multiply
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/zt/miniconda3/envs/uwsyml/lib/python3.8/site-packages/nose/case.py", line 197, in runTest
    self.test(*self.arg)
  File "/mnt/nvme1n1/zt/assignment2-2018/tests/test_tvm_op.py", line 94, in test_matrix_multiply
    matrix_mul(arr_x, arr_y, arr_z)
  File "/mnt/nvme1n1/zt/tvm/python/tvm/runtime/module.py", line 201, in __call__
    return self.entry_func(*args)
  File "/mnt/nvme1n1/zt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 245, in __call__
    raise_last_ffi_error()
  File "/mnt/nvme1n1/zt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm._ffi.base.TVMError: Traceback (most recent call last): 0: operator()                                                                                                                                                                                           
at /mnt/nvme1n1/zt/tvm/src/runtime/library_module.cc:82                                                                                                                                   
TVMError: Assert fail: T.Cast("int32", matrix_mul_B_shape[0]) == 500, Argument matrix_mul.B.shape[0] has an unsatisfied constraint: 500 == T.Cast("int32", matrix_mul_B_shape[0])   
</code></pre></div></div> <p>Wrong code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="o">//</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">wrong</span><span class="p">,</span> <span class="n">we</span> <span class="n">should</span> <span class="n">put</span> <span class="n">this</span> <span class="n">code</span> <span class="n">before</span> <span class="k">if</span> <span class="n">transposeA</span>  
    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">come here</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">a shape </span><span class="sh">'</span><span class="p">,</span> <span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="sh">'</span><span class="s">b shape</span><span class="sh">'</span><span class="p">,</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>



</code></pre></div></div> <p>Correct code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="c1"># print('a shape ', A.shape, 'b shape', B.shape)
</span>        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>

</code></pre></div></div> <p>I asked gpt to find the bug in this code it does not find the bug.</p> <h3 id="softmax-cross-entropy-impl">softmax cross entropy impl</h3> <p>Wrong code</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def make_matrix_softmax_cross_entropy(shape, tgt, tgt_host, func_name,
                                      dtype="float32"):
    """TODO: Your code here"""
    """Hint: output shape should be (1,)"""

    A = te.placeholder(shape, dtype=dtype, name='A')
    A_ = te.placeholder(shape, dtype=dtype, name='A_')

    B = te.compute(
           shape,
            lambda i, j: A_[i,j ] * te.log(A[i, j]),
            name='B'
            )

    row, col = shape
    axis_j = te.reduce_axis((0, col))
    axis_k = te.reduce_axis((0,row))
    C = te.compute(
            (1,),
            lambda :  -te.sum(B[ axis_j, axis_k], axis=[axis_j, axis_k]),
            name='C'
            )

    D = te.compute(
            (1,),
            lambda:  C / (row*col),
            name = 'D'
            )

    s = te.create_schedule(D.op)
    f = tvm.build(s, [A, A_, D], tgt, target_host=tgt_host, name=func_name)
    return f
</code></pre></div></div> <p>Got this error with fowllowing code</p> <p>error:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "/home/zt/miniconda3/envs/uwsyml/lib/python3.8/site-packages/nose/case.py", line 197, in runTest
    self.test(*self.arg)
  File "/mnt/nvme1n1/zt/assignment2-2018/tests/test_tvm_op.py", line 238, in test_softmax_cross_entropy
    matrix_softmax_cross_entropy = tvm_op.make_matrix_softmax_cross_entropy(shape, tgt, tgt_host, "softmax_cross_entropy")
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/tvm_op.py", line 217, in make_matrix_softmax_cross_entropy
    B = te.compute(

    ICHECK(0 == level_) &lt;&lt; "Reductions are only allowed at the top level of compute. "
tvm.error.InternalError: Traceback (most recent call last):
  7: operator()
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:168
  6: tvm::te::ComputeOp::ComputeOp(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tvm::runtime::Map&lt;tvm::runtime::String, tvm::runtime::ObjectRef, void, void&gt;, tvm::runtime::Array&lt;tvm::tir::IterVar, void&gt;, tvm::runtime::Array&lt;tvm::PrimExpr, void&gt;)
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:161
  5: VerifyComputeOp
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:581
  4: Run
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:551
  3: tvm::tir::ExprVisitor::VisitExpr_(tvm::tir::AddNode const*)
        at /mnt/nvme1n1/zt/tvm/src/tir/ir/expr_functor.cc:60
  2: VisitExpr
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:560
  1: VisitExpr_
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:564
  0: VisitExpr_
        at /mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc:566
  File "/mnt/nvme1n1/zt/tvm/src/te/operation/compute_op.cc", line 566
InternalError: Check failed: (0 == level_) is false: Reductions are only allowed at the top level of compute. Please create another tensor for further composition.
</code></pre></div></div> <p>Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_softmax_cross_entropy</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: output shape should be (1,)</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">A_</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A_</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">))</span>
    <span class="n">axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">row</span><span class="p">))</span>

    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
           <span class="p">(</span><span class="n">row</span><span class="p">,),</span>
           <span class="k">lambda</span> <span class="n">i</span> <span class="p">:</span> <span class="o">-</span><span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span> <span class="p">]</span> <span class="o">*</span> <span class="n">te</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_j</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span>
            <span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">axis_k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_k</span><span class="p">)</span><span class="o">/</span> <span class="n">row</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
        <span class="p">)</span>

</code></pre></div></div> <p>I think the error is saying that we should not use reduction and te.log() together?</p> <p>https://discuss.tvm.apache.org/t/non-top-level-reductions-in-compute-statements/5693</p> <p><a href="https://github.com/wyc-ruiker/CSE-599W-2018/blob/master/assignment2/python/dlsys/tvm_op.py" rel="external nofollow noopener" target="_blank">softmax cross entropy reference impl</a></p> <p>Try another impl code</p> <p>Fix code above after calculating te.log first and then do te.sum</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_softmax_cross_entropy</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: output shape should be (1,)</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">A_</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A_</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">j</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">row</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">log</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="n">sum_cross_entropy</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">row</span><span class="p">,),</span>
            <span class="c1"># lambda i: te.sum(B[i, axis_j], axis=axis_j ),
</span>            <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="o">-</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_j</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">sum_cross_entropy</span><span class="sh">'</span>
            <span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
        <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">sum_cross_entropy</span><span class="p">[</span><span class="n">axis_k</span><span class="p">]</span><span class="o">/</span><span class="n">row</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_k</span> <span class="p">)</span> <span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
        <span class="p">)</span>
</code></pre></div></div> <p>Correct code :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_softmax_cross_entropy</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: output shape should be (1,)</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">A_</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A_</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">softmax_axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax_j</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">softmax_axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax_k</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">max_x</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">),</span> 
           <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">softmax_axis_j</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="n">softmax_axis_j</span><span class="p">),</span> 
           <span class="n">name</span><span class="o">=</span> <span class="sh">'</span><span class="s">max_x</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">e_x</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="p">]</span> <span class="o">-</span> <span class="n">max_x</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">e_x</span><span class="sh">"</span>
            <span class="p">)</span>
    <span class="n">ex_sum</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">),</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">e_x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">softmax_axis_k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">softmax_axis_k</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">ex_sm</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">softmax</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">e_x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">ex_sum</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax_x</span><span class="sh">'</span><span class="p">)</span>
 

    <span class="n">axis_j</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">j</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axis_k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">row</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">log</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">softmax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span>
            <span class="p">)</span>
    <span class="n">sum_cross_entropy</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
            <span class="p">(</span><span class="n">row</span><span class="p">,),</span>
            <span class="c1"># lambda i: te.sum(B[i, axis_j], axis=axis_j ),
</span>            <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="o">-</span><span class="n">A_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">axis_j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_j</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">sum_cross_entropy</span><span class="sh">'</span>
            <span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
        <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">sum_cross_entropy</span><span class="p">[</span><span class="n">axis_k</span><span class="p">]</span><span class="o">/</span><span class="n">row</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis_k</span> <span class="p">)</span> <span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
        <span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">A_</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <h3 id="fix-infer_shape-error">Fix infer_shape error</h3> <p><code class="language-plaintext highlighter-rouge">infer_shape()</code> is called for nodes in feed_dict. I don’t know why this happens.</p> <p>I am fixing it.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>type of node &lt;class 'dlsys.autodiff.Node'&gt; node name X
Traceback (most recent call last):
  File "tests/mnist_dlsys.py", line 373, in &lt;module&gt;
    m(executor_ctx, num_epochs, print_loss_val_each_epoch)
  File "tests/mnist_dlsys.py", line 132, in mnist_logreg
    loss_val, grad_W1_val, grad_b1_val, _ = executor.run(
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/autodiff.py", line 706, in run
    self.infer_shape(feed_shapes)
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/autodiff.py", line 616, in infer_shape
    infer_shape = node.op.infer_shape(node, input_shapes)
  File "/mnt/nvme1n1/zt/assignment2-2018/python/dlsys/autodiff.py", line 320, in infer_shape
    assert False, "placeholder %s shape provided by feed_shape" % node.name
AssertionError: placeholder X shape provided by feed_shape
</code></pre></div></div> <p>I fixed this issue by swapping <code class="language-plaintext highlighter-rouge">shape[0]</code> and <code class="language-plaintext highlighter-rouge">shape[1]</code> directly instead of calling <code class="language-plaintext highlighter-rouge">np.transpose</code></p> <h3 id="speed-up-matrix-multilication">Speed up matrix multilication</h3> <p>Original matrix mul code and execution time Code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="c1"># print('a shape ', A.shape, 'b shape', B.shape)
</span>        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(uwsyml) ➜  assignment2-2018 git:(master) ✗ python tests/mnist_dlsys.py -l -m mlp
=== Build 3-layer MLP model...
Loading data...
Start training loop...
/mnt/nvme1n1/zt/tvm/python/tvm/driver/build_module.py:280: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.
  warnings.warn(
epoch 0
shape a 1 784 shapeB 0 784
shape a 1 256 shapeB 0 256
shape a 1 100 shapeB 0 100
shape a 1 10 shapeB 0 10
shape a 1 100 shapeB 0 100
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
loss = 0.565684; Time taken this epoch = 39.259721 s
epoch 1
loss = 0.302340; Time taken this epoch = 37.834584 s
epoch 2
loss = 0.227699; Time taken this epoch = 37.836843 s
epoch 3
loss = 0.199743; Time taken this epoch = 37.733063 s
epoch 4
loss = 0.174254; Time taken this epoch = 37.731381 s
epoch 5
loss = 0.189644; Time taken this epoch = 37.791435 s
epoch 6
loss = 0.125607; Time taken this epoch = 37.795841 s
epoch 7
loss = 0.104398; Time taken this epoch = 37.821751 s
epoch 8
loss = 0.088052; Time taken this epoch = 37.845443 s
epoch 9
loss = 0.073229; Time taken this epoch = 37.798183 s
Validation set accuracy = 0.971600
Average Time per Training Epoch = 37.944825 s
</code></pre></div></div> <p>Optmized code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">make_matrix_mul</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">transposeA</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">,</span> <span class="n">transposeB</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_host</span><span class="p">,</span>
                    <span class="n">func_name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: use tvm.reduce_axis, tvm.sum</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: treat 4 cases of transposeA, transposeB separately</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: for tvm schedule, use split, reorder, vectorize, parallel</span><span class="sh">"""</span>
    <span class="sh">"""</span><span class="s">Hint: debug tvm schedule using tvm.lower</span><span class="sh">"""</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shapeB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transposeA</span><span class="p">:</span>
        <span class="n">shapeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">shapeB</span> <span class="o">=</span> <span class="p">(</span><span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">shape a 1</span><span class="sh">"</span><span class="p">,</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">shapeB 0</span><span class="sh">"</span><span class="p">,</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">shapeA</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transposeA</span> <span class="ow">and</span> <span class="n">transposeB</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nc">B</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">transposeA</span>  <span class="ow">and</span> <span class="p">(</span><span class="n">transposeB</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="nf">elif </span><span class="p">(</span><span class="n">transposeA</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transposeB</span> <span class="p">:</span>
        <span class="c1"># print('a shape ', A.shape, 'b shape', B.shape)
</span>        <span class="nf">assert</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span>
                <span class="p">(</span><span class="n">shapeA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapeB</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span>
                <span class="p">)</span>


    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="p">.</span><span class="nf">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="p">.</span><span class="n">op</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">axis</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="n">op</span><span class="p">.</span><span class="n">reduce_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">xo</span><span class="p">,</span> <span class="n">xi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">yo</span><span class="p">,</span> <span class="n">yi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">ko</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">reorder</span><span class="p">(</span><span class="n">xo</span><span class="p">,</span> <span class="n">yo</span><span class="p">,</span> <span class="n">ko</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">vectorize</span><span class="p">(</span><span class="n">yi</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">].</span><span class="nf">parallel</span><span class="p">(</span><span class="n">xo</span><span class="p">)</span>


    <span class="c1"># here to speed up matrix multiplication
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="nf">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(uwsyml) ➜  assignment2-2018 git:(master) ✗ python tests/mnist_dlsys.py -l -m mlp
=== Build 3-layer MLP model...
Loading data...
Start training loop...
/mnt/nvme1n1/zt/tvm/python/tvm/driver/build_module.py:280: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.
  warnings.warn(
epoch 0
shape a 1 784 shapeB 0 784
shape a 1 256 shapeB 0 256
shape a 1 100 shapeB 0 100
shape a 1 10 shapeB 0 10
shape a 1 100 shapeB 0 100
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
shape a 1 1000 shapeB 0 1000
loss = 0.565684; Time taken this epoch = 2.756940 s
epoch 1
loss = 0.302340; Time taken this epoch = 0.805593 s
epoch 2
loss = 0.227699; Time taken this epoch = 0.840440 s
epoch 3
loss = 0.199743; Time taken this epoch = 0.827800 s
epoch 4
loss = 0.174254; Time taken this epoch = 0.829301 s
epoch 5
loss = 0.189644; Time taken this epoch = 0.835602 s
epoch 6
loss = 0.125607; Time taken this epoch = 0.836001 s
epoch 7
loss = 0.104398; Time taken this epoch = 0.831005 s
epoch 8
loss = 0.088052; Time taken this epoch = 0.829629 s
epoch 9
loss = 0.073229; Time taken this epoch = 0.830049 s
Validation set accuracy = 0.971600
Average Time per Training Epoch = 1.022236 s
</code></pre></div></div> <p>Explaination of <code class="language-plaintext highlighter-rouge">k</code> Certainly! In the line <code class="language-plaintext highlighter-rouge">k = s[C].op.reduce_axis[0]</code>, we are accessing the first reduction axis of the operation that computes <code class="language-plaintext highlighter-rouge">C</code>.</p> <p>Here’s a breakdown:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">s[C]</code>: This refers to the schedule for the computation of <code class="language-plaintext highlighter-rouge">C</code>.</li> <li> <code class="language-plaintext highlighter-rouge">s[C].op</code>: This accesses the operation that computes <code class="language-plaintext highlighter-rouge">C</code>.</li> <li> <code class="language-plaintext highlighter-rouge">s[C].op.reduce_axis</code>: This is a list of reduction axes used in the computation of <code class="language-plaintext highlighter-rouge">C</code>. Reduction axes are the dimensions over which we perform a reduction operation, such as summing elements.</li> <li> <code class="language-plaintext highlighter-rouge">[0]</code>: This accesses the first reduction axis in the list.</li> </ul> <p>In the context of matrix multiplication, the reduction axis <code class="language-plaintext highlighter-rouge">k</code> represents the dimension over which we sum the products of elements from matrices <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>. For example, if <code class="language-plaintext highlighter-rouge">A</code> is of shape <code class="language-plaintext highlighter-rouge">(m, n)</code> and <code class="language-plaintext highlighter-rouge">B</code> is of shape <code class="language-plaintext highlighter-rouge">(n, p)</code>, the reduction axis <code class="language-plaintext highlighter-rouge">k</code> would range from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">n-1</code>, summing the products of corresponding elements from <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> to compute each element of the resulting matrix <code class="language-plaintext highlighter-rouge">C</code>.</p> <p>Let me know if you need any further clarification!</p> <h4 id="problem-setup">Problem Setup</h4> <p>Suppose <code class="language-plaintext highlighter-rouge">C</code> computes the result of multiplying two matrices ( A ) (size ( 64 \times 64 )) and ( B ) (size ( 64 \times 64 )):</p> <p>[ C[i, j] = \sum_{k=0}^{63} A[i, k] \cdot B[k, j] ]</p> <p>Here:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">x</code> corresponds to <code class="language-plaintext highlighter-rouge">i</code> (rows of ( C )),</li> <li> <code class="language-plaintext highlighter-rouge">y</code> corresponds to <code class="language-plaintext highlighter-rouge">j</code> (columns of ( C )),</li> <li> <code class="language-plaintext highlighter-rouge">k</code> is the reduction axis (over ( k )).</li> </ul> <h4 id="steps-in-context">Steps in Context</h4> <ol> <li> <strong>Splitting Axes</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">x</code> (rows) and <code class="language-plaintext highlighter-rouge">y</code> (columns) are split into blocks of size <code class="language-plaintext highlighter-rouge">32</code>. This creates a <strong>tile-based computation</strong>: <ul> <li> <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> iterate over ( 2 \times 2 ) blocks (as ( 64 / 32 = 2 )).</li> <li> <code class="language-plaintext highlighter-rouge">xi</code> and <code class="language-plaintext highlighter-rouge">yi</code> handle elements within each ( 32 \times 32 ) block.</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">k</code> is split into chunks of size <code class="language-plaintext highlighter-rouge">4</code>: <ul> <li> <code class="language-plaintext highlighter-rouge">ko</code> iterates over 16 chunks (as ( 64 / 4 = 16 )).</li> <li> <code class="language-plaintext highlighter-rouge">ki</code> handles individual reduction operations within each chunk.</li> </ul> </li> </ul> </li> <li> <strong>Reordering</strong><br> The computation is reordered to maximize: <ul> <li> <strong>Data locality</strong>: Processing elements in nearby memory locations together.</li> <li> <strong>Parallelism</strong>: Outer loops (<code class="language-plaintext highlighter-rouge">xo</code>, <code class="language-plaintext highlighter-rouge">yo</code>) can often run in parallel.</li> </ul> </li> </ol> <h4 id="execution-order">Execution Order</h4> <p>The reordered iteration could look like this (pseudocode):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">xo</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>         <span class="c1"># Iterate over 32-row blocks
</span>    <span class="k">for</span> <span class="n">yo</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>     <span class="c1"># Iterate over 32-column blocks
</span>        <span class="k">for</span> <span class="n">ko</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span> <span class="c1"># Iterate over reduction chunks (k-axis)
</span>            <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">32</span><span class="p">):</span>  <span class="c1"># Process rows within a block
</span>                <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">32</span><span class="p">):</span>  <span class="c1"># Process columns within a block
</span>                    <span class="k">for</span> <span class="n">ki</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>  <span class="c1"># Perform reduction within the chunk
</span>                        <span class="n">C</span><span class="p">[</span><span class="n">xo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">yi</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">xo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">xi</span><span class="p">,</span> <span class="n">ko</span><span class="o">*</span><span class="mi">4</span> <span class="o">+</span> <span class="n">ki</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">ko</span><span class="o">*</span><span class="mi">4</span> <span class="o">+</span> <span class="n">ki</span><span class="p">,</span> <span class="n">yo</span><span class="o">*</span><span class="mi">32</span> <span class="o">+</span> <span class="n">yi</span><span class="p">]</span>
</code></pre></div></div> <hr> <h3 id="optimization-insight">Optimization Insight</h3> <p>This approach of splitting and reordering:</p> <ul> <li> <strong>Improves memory access patterns</strong>: Data is processed in small blocks, reducing cache misses.</li> <li> <strong>Enables parallel execution</strong>: Larger outer loops (<code class="language-plaintext highlighter-rouge">xo</code>, <code class="language-plaintext highlighter-rouge">yo</code>, <code class="language-plaintext highlighter-rouge">ko</code>) can be distributed across threads or cores.</li> <li> <strong>Reduces computation overhead</strong>: By carefully controlling inner loops, computation can be streamlined for specific hardware.</li> </ul> <p>This code snippet is an example of how to define a computation schedule in <strong>TVM</strong>, a machine learning compiler framework used for optimizing tensor computations. Let’s break it down step by step:</p> <hr> <h3 id="1-s--tecreate_schedulecop">1. <strong><code class="language-plaintext highlighter-rouge">s = te.create_schedule(C.op)</code></strong> </h3> <ul> <li>This creates a schedule for the operation (<code class="language-plaintext highlighter-rouge">C.op</code>) that needs to be optimized.</li> <li>A schedule defines how the computation will be organized in terms of loops, parallelism, vectorization, etc.</li> </ul> <hr> <h3 id="2-x-y--scopaxis">2. <strong><code class="language-plaintext highlighter-rouge">x, y = s[C].op.axis</code></strong> </h3> <ul> <li> <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> are the primary loop axes of the computation. These are typically the dimensions of the tensor being computed.</li> <li>For instance, if <code class="language-plaintext highlighter-rouge">C</code> is a 2D tensor, <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> might represent its row and column dimensions.</li> </ul> <hr> <h3 id="3-k--scopreduce_axis0">3. <strong><code class="language-plaintext highlighter-rouge">k = s[C].op.reduce_axis[0]</code></strong> </h3> <ul> <li> <code class="language-plaintext highlighter-rouge">k</code> is a reduction axis, commonly used in operations like matrix multiplication or summation over an axis.</li> <li>If <code class="language-plaintext highlighter-rouge">C</code> is the result of a matrix multiplication ( A \times B ), then <code class="language-plaintext highlighter-rouge">k</code> would be the summation axis.</li> </ul> <hr> <h3 id="4-xo-xi--scsplitx-factor32">4. <strong><code class="language-plaintext highlighter-rouge">xo, xi = s[C].split(x, factor=32)</code></strong> </h3> <ul> <li>Splits the <code class="language-plaintext highlighter-rouge">x</code> axis into two parts: <ul> <li> <code class="language-plaintext highlighter-rouge">xo</code>: Outer loop, handling chunks of 32 iterations.</li> <li> <code class="language-plaintext highlighter-rouge">xi</code>: Inner loop, handling the remaining iterations within each chunk.</li> </ul> </li> <li>This is a technique for tiling the computation, which improves cache efficiency.</li> </ul> <hr> <h3 id="5-yo-yi--scsplity-factor32">5. <strong><code class="language-plaintext highlighter-rouge">yo, yi = s[C].split(y, factor=32)</code></strong> </h3> <ul> <li>Similar to <code class="language-plaintext highlighter-rouge">x</code>, this splits the <code class="language-plaintext highlighter-rouge">y</code> axis into outer (<code class="language-plaintext highlighter-rouge">yo</code>) and inner (<code class="language-plaintext highlighter-rouge">yi</code>) loops, with a tile size of 32.</li> </ul> <hr> <h3 id="6-ko-ki--scsplitk-factor4">6. <strong><code class="language-plaintext highlighter-rouge">ko, ki = s[C].split(k, factor=4)</code></strong> </h3> <ul> <li>Splits the reduction axis <code class="language-plaintext highlighter-rouge">k</code> into: <ul> <li> <code class="language-plaintext highlighter-rouge">ko</code>: Outer loop for reduction.</li> <li> <code class="language-plaintext highlighter-rouge">ki</code>: Inner loop for reduction, with a tile size of 4.</li> </ul> </li> </ul> <hr> <h3 id="7-screorderxo-yo-ko-xi-yi-ki">7. <strong><code class="language-plaintext highlighter-rouge">s[C].reorder(xo, yo, ko, xi, yi, ki)</code></strong> </h3> <ul> <li>Rearranges the loop order to optimize computation.</li> <li>The new order ensures that the outer loops (<code class="language-plaintext highlighter-rouge">xo</code>, <code class="language-plaintext highlighter-rouge">yo</code>, <code class="language-plaintext highlighter-rouge">ko</code>) iterate first, followed by the inner loops (<code class="language-plaintext highlighter-rouge">xi</code>, <code class="language-plaintext highlighter-rouge">yi</code>, <code class="language-plaintext highlighter-rouge">ki</code>).</li> <li>This can improve locality of reference and allow efficient use of the hardware.</li> </ul> <hr> <h3 id="8-scvectorizeyi">8. <strong><code class="language-plaintext highlighter-rouge">s[C].vectorize(yi)</code></strong> </h3> <ul> <li>Applies vectorization to the <code class="language-plaintext highlighter-rouge">yi</code> loop.</li> <li>This instructs the compiler to use SIMD (Single Instruction, Multiple Data) instructions to compute multiple iterations of the <code class="language-plaintext highlighter-rouge">yi</code> loop simultaneously, boosting performance.</li> </ul> <hr> <h3 id="9-scparallelxo">9. <strong><code class="language-plaintext highlighter-rouge">s[C].parallel(xo)</code></strong> </h3> <ul> <li>Parallelizes the <code class="language-plaintext highlighter-rouge">xo</code> loop, enabling multiple threads to compute different chunks of the <code class="language-plaintext highlighter-rouge">x</code> axis concurrently.</li> <li>This is particularly useful on multi-core processors.</li> </ul> <hr> <h3 id="example"><strong>Example</strong></h3> <p>Suppose <code class="language-plaintext highlighter-rouge">C</code> is the result of a matrix multiplication ( A[m, k] \times B[k, n] ), and we are optimizing the computation.</p> <ul> <li> <strong>Matrix Dimensions</strong>: ( A ) is ( 64 \times 128 ), ( B ) is ( 128 \times 64 ).</li> <li>The schedule: <ul> <li>Splits the computation into tiles of ( 32 \times 32 ) for <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> dimensions (row and column).</li> <li>Reduces over <code class="language-plaintext highlighter-rouge">k</code> in chunks of 4.</li> <li>Reorders loops to optimize memory access patterns.</li> <li>Vectorizes operations within a 32-element column tile (<code class="language-plaintext highlighter-rouge">yi</code>).</li> <li>Parallelizes over outermost rows (<code class="language-plaintext highlighter-rouge">xo</code>), enabling multi-threading.</li> </ul> </li> </ul> <p>This structured approach ensures better utilization of the CPU/GPU by leveraging parallelism, vectorization, and efficient memory access.</p> <h2 id="we-can-parallelize-xo-and-yo-at-the-same-time">We can parallelize xo and yo at the same time</h2> <p>Parallelizing <code class="language-plaintext highlighter-rouge">xo</code> distributes the workload of the outer <code class="language-plaintext highlighter-rouge">x</code> axis (rows of tiles) across multiple threads, allowing each thread to compute one or more row tiles simultaneously. This is often chosen because:</p> <ol> <li> <p><strong>Independent Workload</strong>: Each tile along <code class="language-plaintext highlighter-rouge">xo</code> is independent, meaning computations for different tiles don’t depend on each other. This independence is a prerequisite for parallelization.</p> </li> <li> <p><strong>Balanced Workload</strong>: If the number of row tiles (determined by the size of <code class="language-plaintext highlighter-rouge">xo</code>) matches the number of available threads or cores, it results in a balanced workload.</p> </li> <li> <p><strong>Memory Access</strong>: For row-major memory layouts, accessing data row-wise tends to be more cache-friendly. Parallelizing <code class="language-plaintext highlighter-rouge">xo</code> aligns well with this layout.</p> </li> </ol> <hr> <h3 id="can-we-parallelize-yo-at-the-same-time">Can We Parallelize <code class="language-plaintext highlighter-rouge">yo</code> at the Same Time?</h3> <p>Yes, it’s possible to parallelize <code class="language-plaintext highlighter-rouge">yo</code> as well, but there are important considerations:</p> <ol> <li> <strong>Nested Parallelism</strong>: <ul> <li>If <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> are both parallelized, this results in nested parallelism (threads inside threads).</li> <li>This is often inefficient because hardware threads can’t dynamically spawn or manage sub-threads efficiently, and most hardware supports a flat parallel structure.</li> </ul> </li> <li> <strong>Overhead</strong>: <ul> <li>Parallelizing both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> can introduce thread management overhead.</li> <li>The system must divide and coordinate work across all threads, which can diminish performance if not carefully managed.</li> </ul> </li> <li> <strong>Thread Count</strong>: <ul> <li>Hardware has a limited number of threads or cores. If you parallelize both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code>, you risk oversubscribing threads, causing context-switching overhead.</li> </ul> </li> </ol> <hr> <h3 id="when-parallelizing-both-could-work"><strong>When Parallelizing Both Could Work</strong></h3> <p>Parallelizing both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> is feasible under specific conditions:</p> <ol> <li> <strong>Low Tile Counts in <code class="language-plaintext highlighter-rouge">xo</code> or <code class="language-plaintext highlighter-rouge">yo</code></strong>: <ul> <li>If <code class="language-plaintext highlighter-rouge">xo</code> alone doesn’t provide enough parallelism (e.g., when the matrix is small), you might also parallelize <code class="language-plaintext highlighter-rouge">yo</code>.</li> </ul> </li> <li> <strong>Hierarchical Parallelism</strong>: <ul> <li>You can map <code class="language-plaintext highlighter-rouge">xo</code> to thread blocks and <code class="language-plaintext highlighter-rouge">yo</code> to threads within each block on GPUs or similar architectures.</li> </ul> </li> <li> <strong>Custom Scheduling</strong>: <ul> <li>For some specialized cases, you can explicitly balance thread allocation across <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> to avoid over-parallelization.</li> </ul> </li> </ol> <hr> <h3 id="example-scenario">Example Scenario</h3> <p>If you’re working with a 2D matrix computation and your target machine has 16 cores:</p> <ul> <li> <strong>Parallelize only <code class="language-plaintext highlighter-rouge">xo</code></strong>: Efficient if the number of row tiles ( \frac{\text{rows}}{\text{tile_size}} ) is ≥ 16.</li> <li> <strong>Parallelize both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code></strong>: Useful when row and column tiles combined can provide enough parallelism (e.g., ( 4 \times 4 = 16 ) tiles).</li> </ul> <p>To summarize, while it’s theoretically possible to parallelize both <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code>, in practice, parallelizing only one is simpler and avoids excessive thread management overhead on most CPUs. For GPUs, however, hierarchical or hybrid parallelization across <code class="language-plaintext highlighter-rouge">xo</code> and <code class="language-plaintext highlighter-rouge">yo</code> is more common.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/learning-based-memory-allocation-for-c-server-workloads-summary/">Learning-based memory allocation for C++ server workloads summary</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/my-question/">my question:</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/binary-search-algorithm-variant/">Binary search algorithm variant</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/docker-rocksdb-build/">Docker Rocksdb build</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/difference-between-dockerfile-and-docker-compose/">Difference between Dockerfile and Docker Compose</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhutao Zhuang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: February 14, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-lock-free-queue-implementation-cpp",title:"Lock free queue implementation cpp",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/lock-free-queue/"}},{id:"post-nanogpt-kv-cache-first-attempt",title:"nanogpt kv cache first attempt",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/llm-kv-cache-first-attempt/"}},{id:"post-set-up-vim-in-vscode",title:"Set up vim in vscode",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/set-up-vim-in-vscode/"}},{id:"post-chibicc-simple-c-compiler-for-statement",title:"chibicc - Simple c compiler for statement",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-for-statement/"}},{id:"post-chibicc-simple-c-compiler-if-statement",title:"chibicc - Simple c compiler if statement",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-if-statement/"}},{id:"post-chibicc-simple-c-compiler-block-node",title:"chibicc - Simple c compiler block {} node",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-block/"}},{id:"post-chibicc-simple-c-compiler-return-keyword",title:"chibicc - Simple c compiler return keyword",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-return-keyword/"}},{id:"post-learned-idnex-survey",title:"Learned idnex survey",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/learned-index-survery/"}},{id:"post-chibicc-c-compiler-multi-char-variable-name",title:"chibicc C compiler - multi char variable name",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-multi-char-var-name/"}},{id:"post-c-and-linux-kernel-memory-allocation",title:"C++ and linux kernel memory allocation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/cpp-interview-questions/"}},{id:"post-chibicc-c-compiler-parser-review-and-expression-evaluator",title:"chibicc C compiler - parser review and expression evaluator",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-parser-review-and-calculator/"}},{id:"post-imperative-programming-vs-declarative-programming",title:"Imperative programming vs. Declarative programming",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/imperative-programming/"}},{id:"post-autodiff-implementation-kernel-and-memory-management",title:"Autodiff implementation - kernel and memory management",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign2/"}},{id:"post-hash-in-cpp",title:"Hash in cpp",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hash-in-cpp/"}},{id:"post-computer-basics",title:"Computer basics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/some-basic-computer-knowledge/"}},{id:"post-rdma",title:"Rdma",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/rdma/"}},{id:"post-topo-sort",title:"Topo sort",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/topo-sort/"}},{id:"post-autodiff-implementation",title:"Autodiff implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign/"}},{id:"post-palindrome-substring-partition",title:"palindrome substring partition",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/palindrome-substring-partition/"}},{id:"post-stf-cs149-flash-attention",title:"Stf CS149 flash attention",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149gpt/"}},{id:"post-speed-up-matrix-multiplication-2",title:"Speed up matrix multiplication 2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-mul-comparison/"}},{id:"post-elf-loading",title:"Elf Loading",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/elf-loading/"}},{id:"post-stf-cs149-parallel-programming-assign3",title:"Stf CS149 Parallel Programming - Assign3",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign3/"}},{id:"post-stf-cs149-parallel-programming-lecture11-cache-coherence",title:"Stf CS149 Parallel Programming - Lecture11 - Cache coherence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture11-cache-coherence/"}},{id:"post-stf-cs149-parallel-programming-lecture-7-cuda-programming-model",title:"Stf CS149 Parallel Programming - Lecture 7 - Cuda programming model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture7-cuda/"}},{id:"post-stf-cs149-parallel-programming-assign2",title:"Stf CS149 Parallel Programming - Assign2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign2/"}},{id:"post-stf-cs149-parallel-programming-lecture-5-amp-6-performance-optimization",title:"Stf CS149 Parallel Programming - Lecture 5&amp;6 - Performance optimization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-lecture-takeaway/"}},{id:"post-stf-cs149-parallel-programming-assign1",title:"Stf CS149 Parallel Programming - Assign1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-assign1/"}},{id:"post-ssh-display-image-on-local-server",title:"ssh display image on local server",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/ssh-display-img/"}},{id:"post-c-compiler-single-letter-local-variable",title:"C compiler - single letter local variable",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-single-letter-ident/"}},{id:"post-c-compiler-parse-example-walkthrough",title:"C compiler - parse example walkthrough",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/statement-and-comparison/"}},{id:"post-linux-get-cpu-time-and-wall-clock-time",title:"Linux get cpu time and wall clock time",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/get-function-cpu-time/"}},{id:"post-simple-c-compiler-unary",title:"Simple c compiler unary",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-unary/"}},{id:"post-simple-c-compiler-gen-expr",title:"Simple c compiler gen expr",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-gen-expr/"}},{id:"post-python-pyplot-trick",title:"Python pyplot trick",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-plot-trick/"}},{id:"post-simple-lru-cache-cpp-implementation",title:"Simple lru cache cpp implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lru-cache/"}},{id:"post-python-capture-function-print-output",title:"Python capture function print output",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/capture-output-python/"}},{id:"post-simple-c-compiler",title:"Simple c compiler",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-compiler/"}},{id:"post-calloc-and-malloc",title:"calloc and malloc",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/calloc/"}},{id:"post-lightgbm-dataset",title:"LightGBM dataset",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-dataset/"}},{id:"post-difference-between-deep-copy-and-shallow-copy-in-python",title:"Difference between deep copy and shallow copy in python",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-deep-copy/"}},{id:"post-python-package-path",title:"Python Package Path",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-package-path/"}},{id:"post-micrograd",title:"micrograd",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/micrograd/"}},{id:"post-cpp-thread-local",title:"cpp thread local",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-thread-local/"}},{id:"post-cpp-async",title:"cpp async",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-async/"}},{id:"post-python-dataframe-drop-row",title:"python dataframe drop row",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/df-drop-row/"}},{id:"post-git-merge-file-from-another-branch",title:"Git merge file from another branch",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/git-merge-file-from-another-branch/"}},{id:"post-efficiency-tips",title:"Efficiency tips",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/efficiency-tips/"}},{id:"post-speed-up-matrix-multiplication",title:"Speed up matrix multiplication",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-multiplication/"}},{id:"post-how-to-write-research-paper",title:"How to write research paper",description:"How to write research paper",section:"Posts",handler:()=>{window.location.href="/blog/2024/how-to-write-research-paper/"}},{id:"post-fast-nano-gpt-training",title:"Fast nano-gpt training",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/gpt-fast/"}},{id:"post-system-for-machine-learning-papers",title:"System for machine learning papers",description:"sysml papers",section:"Posts",handler:()=>{window.location.href="/blog/2024/sysml-papers/"}},{id:"post-nano-gpt-and-transformer",title:"nano-gpt and Transformer",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer/"}},{id:"post-pytorch-tensor-to",title:"pytorch tensor.to",description:"pytorch",section:"Posts",handler:()=>{window.location.href="/blog/2024/pytorch/"}},{id:"post-install-neovim-with-old-glibc",title:"Install neovim with old glibc",description:"vim",section:"Posts",handler:()=>{window.location.href="/blog/2024/install-neovim/"}},{id:"post-llm-c",title:"llm.c",description:"llm minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/llm-c/"}},{id:"post-basic-digital-electronic",title:"Basic digital electronic",description:"transistor",section:"Posts",handler:()=>{window.location.href="/blog/2024/digital-electronic/"}},{id:"post-k8s-advance",title:"K8s Advance",description:"k8s minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/k8s-advance/"}},{id:"post-difference-between-dockerfile-and-docker-compose",title:"Difference between Dockerfile and Docker Compose",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/difference-between-dockerfile-and-docker-compose-d6ebdc687785?source=rss-da1663a42461------2","_blank")}},{id:"post-docker-rocksdb-build",title:"Docker Rocksdb build",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/docker-rocksdb-build-18a0bf0e0bb0?source=rss-da1663a42461------2","_blank")}},{id:"post-k3s-beginner",title:"K3s beginner",description:"k3s",section:"Posts",handler:()=>{window.location.href="/blog/2024/k3s/"}},{id:"post-docker-rocksdb",title:"Docker RocksDB",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-rocksdb/"}},{id:"post-docker-beginner",title:"Docker beginner",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-file-compose-diff/"}},{id:"post-git",title:"Git",description:"git",section:"Posts",handler:()=>{window.location.href="/blog/2024/git/"}},{id:"post-lightgbm-usage-and-implementation",title:"LightGBM usage and implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-usage/"}},{id:"post-backpropogation-c-implementation",title:"Backpropogation C++ Implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/back-propagation/"}},{id:"post-conda-usage",title:"Conda usage",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/conda/"}},{id:"post-install-k8s-cluster-with-3-ubuntu-nodes",title:"Install K8s cluster with 3 ubuntu nodes",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/cloud/"}},{id:"post-convert-svg-figures-to-pdf-latex-before-submitting-to-arxiv",title:"Convert SVG figures to pdf_latex before submitting to arxiv",description:"Convert SVG figures to pdf_latex before submitting to arxiv",section:"Posts",handler:()=>{window.location.href="/blog/2024/arxiv-cleaner/"}},{id:"post-binary-search-algorithm-variant",title:"Binary search algorithm variant",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/binary-search-algorithm-variant-9b5310473471?source=rss-da1663a42461------2","_blank")}},{id:"post-my-question",title:"my question:",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/my-question-a69930f167f0?source=rss-da1663a42461------2","_blank")}},{id:"post-learning-based-memory-allocation-for-c-server-workloads-summary",title:"Learning-based memory allocation for C++ server workloads summary",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/learning-based-memory-allocation-for-c-server-workloads-summary-479e9cd6d6f6?source=rss-da1663a42461------2","_blank")}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-paper-dumpkv-accepted-by-vldb-39-25",title:"A paper(DumpKV) accepted by VLDB&#39;25",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%68%75%61%6E%67%7A%68%75%74%61%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/BilyZ98","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>