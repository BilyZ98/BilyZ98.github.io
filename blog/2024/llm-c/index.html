<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> llm.c | Zhutao Zhuang </title> <meta name="author" content="Zhutao Zhuang"> <meta name="description" content="llm minikune"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bilyz98.github.io/blog/2024/llm-c/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhutao</span> Zhuang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">llm.c</h1> <p class="post-meta"> Created in June 22, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/cuda"> <i class="fa-solid fa-hashtag fa-sm"></i> cuda</a>   ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>roadmap</p> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Running llm.c</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Running llm.c with cuda</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Multiple gpus</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Inference with fp16</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Inference with vllm</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>try other inference acceleartion tech</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Submit pr to check existence of openmpi by specifying openmpi path. ( can use conda as example)</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Submit pr to check curl result when proxy server returns 503 error.</li> </ul> <h2 id="running-llmc">Running llm.c</h2> <p><a href="https://github.com/karpathy/llm.c" rel="external nofollow noopener" target="_blank">https://github.com/karpathy/llm.c</a></p> <h3 id="gpu">GPU</h3> <p>Had issue running gpu There is only cuda 11.2 on my machine but torch 2.1.0 is installed which requires cuda 12.0</p> <p>Solution: Manually specify torch==1.3.1</p> <p>Get error</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yhrun <span class="nt">-n</span> 4 <span class="nt">-p</span> gpu_v100 python train_gpt2.py
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
yhrun: error: gpu55: tasks 0-3: Exited with exit code 1
</code></pre></div></div> <p>The issue is that nullcontext is introduced in python &gt;=3.7 So I need to upgrade python version</p> <p>Still can not solve problem above because I can’t not import new module to existing module list.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Currently Loaded Modulefiles:
 1) proxy/1.0   2) CUDA/10.0   3) cudnn/7.6.4-CUDA10.0   4) PyTorch/1.2.0-CUDA10.0-py3.6

 $ yhrun -n 4 -p gpu_v100 python train_gpt2.py
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
Traceback (most recent call last):
  File "train_gpt2.py", line 24, in &lt;module&gt;
    from contextlib import nullcontext
ImportError: cannot import name 'nullcontext'
</code></pre></div></div> <p>My friend told me that I can just use conda to create new namespace and then I can ssh to the compute node and activate the conda environment. And then I can run training process.</p> <p>This means that compute node shares the same file system with login node. But the operating system is different. Because each node has its own hostname.</p> <p>Learn new thing every day.</p> <p>Here’s all available nodes I have.</p> <p>Karpathy has updated gpt2 parameter download script so now I can download parameter via shell script</p> <p>Issue: Can not connect to huggingface todownload pretrained model via proxy</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln102%tianhe2-K llm.c]$ curl -v https://huggingface.co
* About to connect() to proxy 10.20.18.21 port 3128 (#0)
*   Trying 10.20.18.21...
* Connected to 10.20.18.21 (10.20.18.21) port 3128 (#0)
* Establish HTTP proxy tunnel to huggingface.co:443
&gt; CONNECT huggingface.co:443 HTTP/1.1
&gt; Host: huggingface.co:443
&gt; User-Agent: curl/7.29.0
&gt; Proxy-Connection: Keep-Alive
&gt;
&lt; HTTP/1.1 503 Service Unavailable
&lt; Proxy-Agent: gost/2.11.1
&lt; Content-Length: 0
&lt;
* Received HTTP code 503 from proxy after CONNECT
* Connection #0 to host 10.20.18.21 left intact
curl: (56) Received HTTP code 503 from proxy after CONNECT
</code></pre></div></div> <p>Solution: I decide to download on my local laptop and then upload these model parameter files to gpu nodes.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod </span>u+x ./dev/download_starter_pack.sh
./dev/download_starter_pack.sh
make train_gpt2fp32cu
./train_gpt2fp32cu
</code></pre></div></div> <p>cuda env:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Currently Loaded Modulefiles:
 1) proxy/1.0   2) python/3.6.7_anaconda3   3) CUDA/11.2   4) gmp/4.2.4   5) mpfr/2.4.2   6) mpc/0.8.1   7) gcc/9.2.0
</code></pre></div></div> <p>Output :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step   61/74: train loss 3.213066 (312.014672 ms, 13127 tok/s)
step   62/74: train loss 3.450736 (314.262273 ms, 13033 tok/s)
step   63/74: train loss 3.370245 (315.130342 ms, 12997 tok/s)
step   64/74: train loss 3.407992 (316.778140 ms, 12930 tok/s)
step   65/74: train loss 3.580323 (315.324538 ms, 12989 tok/s)
step   66/74: train loss 3.029552 (317.274858 ms, 12909 tok/s)
step   67/74: train loss 3.296448 (317.588671 ms, 12897 tok/s)
step   68/74: train loss 3.675703 (314.929981 ms, 13006 tok/s)
step   69/74: train loss 3.297087 (313.282229 ms, 13074 tok/s)
step   70/74: train loss 3.646337 (315.271277 ms, 12991 tok/s)
step   71/74: train loss 3.566427 (316.123225 ms, 12956 tok/s)
step   72/74: train loss 3.732521 (315.446478 ms, 12984 tok/s)
step   73/74: train loss 3.825229 (318.325142 ms, 12867 tok/s)
step   74/74: train loss 3.380326 (318.066751 ms, 12877 tok/s)
val loss 3.491223
generating:
---
BUCKINGHAM:
But of my penitent ambition
Rome Slicom against Reimy, justice about him!
In case the witness should speak with joy:
Shall now that by these dwelling House,
Suspicions are declaim'd of the Albanian king.
Go
---
total average iteration time: 312.354733 ms
</code></pre></div></div> <h3 id="multiple-gpus">Multiple GPUs</h3> <p>Run with MPI. Don’t know mpi works internally but I will just start using it to train model.</p> <p>I will learn the internals later.</p> <p>Now I just login to gpu node and run the following command</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make train_gpt2cu
mpirun <span class="nt">-np</span> &lt;number of GPUs&gt; ./train_gpt2cu
</code></pre></div></div> <p>Issue: failed to compile with openmpi I used hpc cluster which has openmpi library installed in directory that is different from standard directory.</p> <p>Here’s Makefile in llm.c</p> <div class="language-makefile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">ifeq</span> <span class="nv">($(NO_MULTI_GPU), 1)</span>
  <span class="err">$(info</span> <span class="err">→</span> <span class="err">Multi-GPU</span> <span class="err">(OpenMPI</span> <span class="err">+</span> <span class="err">NCCL)</span> <span class="err">is</span> <span class="err">manually</span> <span class="err">disabled)</span>
<span class="k">else</span>
  <span class="k">ifneq</span> <span class="nv">($(OS), Windows_NT)</span>
    <span class="c"># Detect if running on macOS or Linux
</span>    <span class="k">ifeq</span> <span class="nv">($(SHELL_UNAME), Darwin)</span>
      <span class="err">$(info</span> <span class="err">✗</span> <span class="err">Multi-GPU</span> <span class="err">on</span> <span class="err">CUDA</span> <span class="err">on</span> <span class="err">Darwin</span> <span class="err">is</span> <span class="err">not</span> <span class="err">supported,</span> <span class="err">skipping</span> <span class="err">OpenMPI</span> <span class="err">+</span> <span class="err">NCCL</span> <span class="err">support)</span>
    <span class="err">else</span> <span class="k">ifeq</span> <span class="nv">($(shell [ -d /usr/lib/x86_64-linux-gnu/openmpi/lib/ ] &amp;&amp; [ -d /usr/lib/x86_64-linux-gnu/openmpi/include/ ] &amp;&amp; echo "exists"), exists)</span>
      <span class="err">$(info</span> <span class="err">✓</span> <span class="err">OpenMPI</span> <span class="err">found,</span> <span class="err">OK</span> <span class="err">to</span> <span class="err">train</span> <span class="err">with</span> <span class="err">multiple</span> <span class="err">GPUs)</span>
      <span class="nv">NVCC_INCLUDES</span> <span class="o">+=</span> <span class="nt">-I</span>/usr/lib/x86_64-linux-gnu/openmpi/include
      <span class="nv">NVCC_LDFLAGS</span> <span class="o">+=</span> <span class="nt">-L</span>/usr/lib/x86_64-linux-gnu/openmpi/lib/
      <span class="nv">NVCC_LDLIBS</span> <span class="o">+=</span> <span class="nt">-lmpi</span> <span class="nt">-lnccl</span>
      <span class="nv">NVCC_FLAGS</span> <span class="o">+=</span> <span class="nt">-DMULTI_GPU</span>
    <span class="k">else</span>
      <span class="err">$(info</span> <span class="err">✗</span> <span class="err">OpenMPI</span> <span class="err">is</span> <span class="err">not</span> <span class="err">found,</span> <span class="err">disabling</span> <span class="err">multi-GPU</span> <span class="err">support)</span>
      <span class="err">$(info</span> <span class="err">---&gt;</span> <span class="err">On</span> <span class="err">Linux</span> <span class="err">you</span> <span class="err">can</span> <span class="err">try</span> <span class="err">install</span> <span class="err">OpenMPI</span> <span class="err">with</span> <span class="err">`sudo</span> <span class="err">apt</span> <span class="err">install</span> <span class="err">openmpi-bin</span> <span class="err">openmpi-doc</span> <span class="err">libopenmpi-dev`)</span>
    <span class="k">endif</span>
  <span class="k">endif</span>
<span class="k">endif</span>
</code></pre></div></div> <p>It checks existence of openmpi library in <code class="language-plaintext highlighter-rouge">/usr/lib/x86_64-linux-gnu/openmpi/lib/</code> Openmpi library is at ` ~/local/lib/` in my hpc cluster. Should I raise a pr?</p> <p>Issue： Get compilation error when linking nccl</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/GPUFS/app_GPU/compiler/CUDA/11.2.0/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 -DMULTI_GPU -DENABLE_BF16 train_gpt2.cu -lcublas -lcublasLt -L~/local/lib/  -I~/local/include/   -lmpi -lnccl -o train_gpt2cu
llmc/zero.cuh(28): error: identifier "ncclBfloat16" is undefined

llmc/zero.cuh(209): error: identifier "ncclAvg" is undefined

llmc/zero.cuh(219): error: identifier "ncclAvg" is undefined

3 errors detected in the compilation of "train_gpt2.cu".
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>I have load nccl module but I still get this error and I don’t know how to fix it. Try to compile train_gpt2fp32cu</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load  CUDA/11.2
module load  gcc/9.2.0
<span class="c">#module load  openmpi/1.10.2-pgi-17.1</span>
module load openmpi/3.1.4-icc-18.0.1
module load  nccl/2.9.9-1-cuda-11.0
module list
which nvcc
<span class="nb">pushd </span>llm.c
<span class="c">#make train_gpt2cu</span>
make train_gpt2fp32cu
mpirun <span class="nt">-np</span> 2 ./train_gpt2fp32cu
<span class="nb">popd</span>

</code></pre></div></div> <p>Get out of memory error when running</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 74                                                 |
| val_num_batches       | 8                                                  |
+-----------------------+----------------------------------------------------+
allocated 474 MiB for model parameters
| train_num_batches     | 74                                                 |
| val_num_batches       | 8                                                  |
+-----------------------+----------------------------------------------------+
allocated 474 MiB for model parameters
allocated 5706 MiB for activations
allocated 5706 MiB for activations
val loss 4.513921
val loss 4.513921
allocated 474 MiB for parameter gradients
allocated 252 MiB for activation gradients
allocated 474 MiB for AdamW optimizer state m
allocated 474 MiB for AdamW optimizer state v
allocated 474 MiB for parameter gradients
allocated 252 MiB for activation gradients
[CUDA ERROR] at file train_gpt2_fp32.cu:1443:
out of memory
</code></pre></div></div> <p>I am not famaliar with how cuda can work with multiple gpus when doing training.</p> <p>Should I learn a little bit more about how can I use multiple gpus to do computation when working with cuda?</p> <p>Why do we have to use mpi to run with multiple gpus?</p> <p>Let’s check whether single gpu code actually uses single gpu or not.</p> <p>There’s only one gpu running when training with single gpu. And it works pretty well. So I want to know how to use multiple gpus to train model.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) [nsccgz_qylin_1@gpu29%tianhe2-K zt]$ nvidia-smi
Fri Jun 21 12:17:35 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:8A:00.0 Off |                    0 |
| N/A   62C    P0   276W / 300W |   8354MiB / 16160MiB |     98%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   32C    P0    38W / 300W |      3MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   31C    P0    37W / 300W |      2MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   31C    P0    37W / 300W |      3MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A    153170      C   ./train_gpt2fp32cu               8351MiB |
+-----------------------------------------------------------------------------+
</code></pre></div></div> <p>Here’s the code that initializes multi gpu training config in train_gpt2.cu</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MultiGpuConfig</span> <span class="nf">multi_gpu_config_init</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">***</span><span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
<span class="cp">#ifdef MULTI_GPU
</span>    <span class="c1">// Initialize MPI.</span>
    <span class="n">MultiGpuConfig</span> <span class="n">result</span><span class="p">;</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Init</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">));</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">process_rank</span><span class="p">));</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">num_processes</span><span class="p">));</span>
    <span class="n">result</span><span class="p">.</span><span class="n">local_device_idx</span> <span class="o">=</span> <span class="n">multi_gpu_get_local_device_idx</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">process_rank</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">num_processes</span><span class="p">);</span>
    <span class="n">cudaCheck</span><span class="p">(</span><span class="n">cudaSetDevice</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">local_device_idx</span><span class="p">));</span>
    <span class="n">ncclUniqueId</span> <span class="n">nccl_id</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">process_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">ncclCheck</span><span class="p">(</span><span class="n">ncclGetUniqueId</span><span class="p">(</span><span class="o">&amp;</span><span class="n">nccl_id</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="n">mpiCheck</span><span class="p">(</span><span class="n">MPI_Bcast</span><span class="p">((</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">nccl_id</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">nccl_id</span><span class="p">),</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">));</span>
    <span class="n">ncclCheck</span><span class="p">(</span><span class="n">ncclCommInitRank</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">nccl_comm</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">num_processes</span><span class="p">,</span> <span class="n">nccl_id</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">process_rank</span><span class="p">));</span>
    <span class="n">cudaCheck</span><span class="p">(</span><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">nccl_stream</span><span class="p">));</span>
    <span class="c1">// event without timing for maximum performance</span>
    <span class="n">cudaCheck</span><span class="p">(</span><span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">.</span><span class="n">compute_nccl_sync</span><span class="p">,</span> <span class="n">cudaEventDisableTiming</span><span class="p">));</span>
    <span class="n">nvtxNameCudaStreamA</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">nccl_stream</span><span class="p">,</span> <span class="s">"nccl stream"</span><span class="p">);</span>
    <span class="n">nvtxNameCudaEventA</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">compute_nccl_sync</span><span class="p">,</span> <span class="s">"nccl compute sync"</span><span class="p">);</span>

</code></pre></div></div> <p>Ask bingchat to explain this code This code is for initializing a multi-GPU configuration using MPI (Message Passing Interface), CUDA, and NCCL (NVIDIA Collective Communications Library). Here’s a breakdown of what the code does:</p> <ol> <li> <p><code class="language-plaintext highlighter-rouge">MultiGpuConfig result;</code>: This line initializes a <code class="language-plaintext highlighter-rouge">MultiGpuConfig</code> object named <code class="language-plaintext highlighter-rouge">result</code>.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Init(argc, argv));</code>: This line initializes the MPI environment.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Comm_rank(MPI_COMM_WORLD, &amp;result.process_rank));</code>: This line determines the rank of the calling process in the communicator.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Comm_size(MPI_COMM_WORLD, &amp;result.num_processes));</code>: This line determines the number of processes in the communicator.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">result.local_device_idx = multi_gpu_get_local_device_idx(result.process_rank, result.num_processes);</code>: This line gets the local device index based on the process rank and the number of processes.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cudaCheck(cudaSetDevice(result.local_device_idx));</code>: This line sets the device to be used for GPU executions.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">ncclUniqueId nccl_id;</code>: This line declares a unique NCCL ID.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">if (result.process_rank == 0) { ncclCheck(ncclGetUniqueId(&amp;nccl_id)); }</code>: This line generates a new NCCL unique ID and assigns it to <code class="language-plaintext highlighter-rouge">nccl_id</code> if the process rank is 0.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mpiCheck(MPI_Bcast((void *)&amp;nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD));</code>: This line broadcasts the NCCL unique ID from the root process (process with rank 0) to all other processes in the communicator.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">ncclCheck(ncclCommInitRank(&amp;result.nccl_comm, result.num_processes, nccl_id, result.process_rank));</code>: This line initializes the NCCL communicator according to the rank, the number of processes, and the unique NCCL ID.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cudaCheck(cudaStreamCreate(&amp;result.nccl_stream));</code>: This line creates a new asynchronous stream.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cudaCheck(cudaEventCreate(&amp;result.compute_nccl_sync, cudaEventDisableTiming));</code>: This line creates an event with no timing data. This is used for synchronization purposes.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nvtxNameCudaStreamA(result.nccl_stream, "nccl stream");</code>: This line names the CUDA stream “nccl stream” for profiling and debugging purposes.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nvtxNameCudaEventA(result.compute_nccl_sync, "nccl compute sync");</code>: This line names the CUDA event “nccl compute sync” for profiling and debugging purposes.</p> </li> </ol> <p>This function is designed to be used in a multi-GPU environment where multiple processes are running in parallel, each possibly on a different GPU.</p> <p>The use of MPI allows these processes to communicate with each other, and the use of NCCL allows them to collectively perform operations such as reductions and broadcasts.</p> <p>The CUDA streams and events are used for managing and synchronizing the GPU computations.</p> <p>The NVTX calls are used for profiling and debugging the GPU code.</p> <p>Note that this code assumes that the <code class="language-plaintext highlighter-rouge">MULTI_GPU</code> macro is defined; if it is not, this function will not do anything.</p> <p>Is there any way to run with multi-gpus without callling like this ?</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make train_gpt2cu
mpirun <span class="nt">-np</span> &lt;number of GPUs&gt; ./train_gpt2cu
</code></pre></div></div> <p>So now I will try to run very simple mpi and cuda program to test cuda and mpi env on my hpc cluster.</p> <pre><code class="language-cu">#include &lt;stdio.h&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;mpi.h&gt;

__global__ void helloFromGPU(void) {
    printf("Hello World from GPU %d!\n", blockIdx.x);
}

int main(int argc, char** argv) {
    // Initialize the MPI environment
    MPI_Init(&amp;argc, &amp;argv);

    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);

    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);

    // Set the device to the rank of the current MPI process
    cudaSetDevice(world_rank);

    printf("Hello World from CPU of MPI process %d!\n", world_rank);

    // Launch the kernel on the GPU
    helloFromGPU&lt;&lt;&lt;world_size, 1&gt;&gt;&gt;();

    // Wait for GPU to finish before accessing on host
    cudaDeviceSynchronize();

    // Finalize the MPI environment.
    MPI_Finalize();
}

~                                                                                                                                                                                   ~                                                                                                                                                                                   ~
</code></pre> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) $ nvcc -I/GPUFS/nsccgz_qylin_1/local/include -L/GPUFS/nsccgz_qylin_1/local/lib  -lmpi hello_mpi_cuda.cu -o hello_mpi_cuda
(base) $ ls
hello.cu  hello_cuda  hello_mpi  hello_mpi.c  hello_mpi_cuda  hello_mpi_cuda.cu
(base) $ mpirun -np 2 ./hello_mpi_cuda
--------------------------------------------------------------------------
WARNING: No preset parameters were found for the device that Open MPI
detected:

  Local host:            gpu29
  Device name:           mlx5_9
  Device vendor ID:      0x02c9
  Device vendor part ID: 4116

Default device parameters will be used, which may result in lower
performance.  You can edit any of the files specified by the
btl_openib_device_param_files MCA parameter to set values for your
device.

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_no_device_params_found to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   gpu29
  Local device: mlx5_9
--------------------------------------------------------------------------
Hello World from CPU of MPI process 0!
Hello World from CPU of MPI process 1!
Hello World from GPU 0!
Hello World from GPU 1!
Hello World from GPU 0!
Hello World from GPU 1!
[gpu29:229661] 1 more process has sent help message help-mpi-btl-openib.txt / no device params found
[gpu29:229661] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[gpu29:229661] 1 more process has sent help message help-mpi-btl-openib.txt / error in device init
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">nvcc</code> is just like <code class="language-plaintext highlighter-rouge">gcc</code> which is used to compile cuda code.</p> <p>What is <code class="language-plaintext highlighter-rouge">nccl</code>? Basically <code class="language-plaintext highlighter-rouge">nccl</code> is just like <code class="language-plaintext highlighter-rouge">mpi</code>. It’s used to do collective communication between gpus.</p> <p>This struct is used as config to maintain as information about each gpu.</p> <pre><code class="language-cu">// Parameters specific to training on multiple GPUs.
typedef struct {
  int process_rank;      // Rank of this process among all MPI processes on all hosts. 0 if no multi-GPU.
  int num_processes;     // Total number of processes on all hosts. 1 if no multi-GPU.
  int local_device_idx;  // This process GPU index on current machine. 0 if no multi-GPU.
  ncclComm_t nccl_comm;  // NCCL communication primitive, used for collective mutli-GPU work.
} MultiGpuConfig;

// Determine which GPU this process should use.
// Processes on the same machines use different GPU indicies. Processes on other machines don't.
// Copied from NCCL examples: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/examples.html#example-2-one-device-per-process-or-thread
int multi_gpu_get_local_device_idx(int process_rank, int num_processes) {
  char hostname[1024];
  hostname[1023] = '\0';
  // All processes on the same machine will share the same hostname.
  gethostname(hostname, 1023);
  for (int i=0; i &lt; 1024; i++) {
    if (hostname[i] == '.') {
        hostname[i] = '\0';
        break;
    }
  }
  uint64_t hostname_hash = 5381;
  for (int c = 0; hostname[c] != '\0'; c++){ hostname_hash = ((hostname_hash &lt;&lt; 5) + hostname_hash) ^ hostname[c]; }

  // Distribute all hostname hashes to all processes.
  uint64_t* all_hostsname_hashes = (uint64_t*)malloc(num_processes * sizeof(uint64_t));
  all_hostsname_hashes[process_rank] = hostname_hash;
  mpiCheck(MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, all_hostsname_hashes, sizeof(uint64_t), MPI_BYTE, MPI_COMM_WORLD));

  // Identify which GPU we need to use.
  int local_device_idx = 0;
  for (int current_process = 0; current_process &lt; num_processes; ++current_process) {
     if (current_process == process_rank) {
      // Found my gpu, local_device_idx now has my target GPU index.
      break;
     }
     if (all_hostsname_hashes[current_process] == all_hostsname_hashes[process_rank]) {
      // This process ID runs on the same machine, but it's not me, skip this GPU
      local_device_idx++;
     }
  }

  free(all_hostsname_hashes);
  return local_device_idx;
}
</code></pre> <p>Get this error.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>error: identifier "ncclAvg" is undefined
</code></pre></div></div> <p>Check nccl commit message and found that ncclAvg is added in version 2.10.3 I have only <code class="language-plaintext highlighter-rouge">nccl/2.9.9-1-cuda-11.0</code>.</p> <p>What should I do?</p> <p>Just learned that I can use <code class="language-plaintext highlighter-rouge">conda</code> to install cuda and nccl. Let’s try it.</p> <p>So what is conda and how does it work? I think conda is just a package manager like <code class="language-plaintext highlighter-rouge">apt</code> in ubuntu. <code class="language-plaintext highlighter-rouge">conda</code> helps with environment management and package installation.</p> <p><code class="language-plaintext highlighter-rouge">apt</code> helps with package installation and update.</p> <p><code class="language-plaintext highlighter-rouge">spack</code> is another package manager that is used in hpc cluster.</p> <p><a href="https://stackoverflow.com/a/78227826/14600569" rel="external nofollow noopener" target="_blank">https://stackoverflow.com/questions/77873047/what-are-the-key-differences-between-spack-and-conda-package-managers</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda <span class="nb">install </span>nvidia::cuda-toolkit
</code></pre></div></div> <p>Get another compilation error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/gcc/x86_64-conda-linux-gnu/12.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib//libcublas.so: undefined reference to `memcpy@GLIBC_2.14'
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/gcc/x86_64-conda-linux-gnu/12.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/libpmix.so.2: undefined reference to `clock_gettime@GLIBC_2.17'
collect2: error: ld returned 1 exit status
make: *** [train_gpt2cu] Error 255
(llmc) [nsccgz_qylin_1@gpu29%tianhe2-K llm.c]$ gcc --version
gcc (conda-forge gcc 12.3.0-11) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre></div></div> <p>check glibc version.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu29%tianhe2-K llm.c]$ ldd --version
ldd (GNU libc) 2.17
Copyright (C) 2012 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
Written by Roland McGrath and Ulrich Drepper.
(llmc) [nsccgz_qylin_1@gpu29%tianhe2-K llm.c]$ which ldd
/usr/bin/ldd
</code></pre></div></div> <p>I think I need to use ldd in conda env to check glibc version. How can I do this? I can not install specified glibc version with conda.</p> <p>Remove conda env and create a new one. Install nccl first</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge
    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge
    cuda-version-12.5          |       hd4f0392_3          21 KB  conda-forge
    libgcc-ng-13.2.0           |      h77fa898_11         777 KB  conda-forge
    libgomp-13.2.0             |      h77fa898_11         434 KB  conda-forge
    libstdcxx-ng-13.2.0        |      hc0a3c3a_11         3.7 MB  conda-forge
    nccl-2.22.3.1              |       hbc370b7_0       107.3 MB  conda-forge
    ------------------------------------------------------------
                                           Total:       112.2 MB

The following NEW packages will be INSTALLED:

  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge
  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu
  cuda-version       conda-forge/noarch::cuda-version-12.5-hd4f0392_3
  libgcc-ng          conda-forge/linux-64::libgcc-ng-13.2.0-h77fa898_11
  libgomp            conda-forge/linux-64::libgomp-13.2.0-h77fa898_11
  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-13.2.0-hc0a3c3a_11
  nccl               conda-forge/linux-64::nccl-2.22.3.1-hbc370b7_0
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span>nvidia/label/cuda-12.5.0::cuda-toolkit
</code></pre></div></div> <p>conda adjust channels priority</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda config --describe channel_priority
conda config --set channel_priority flexible
conda config --prepend channels conda-forge
conda config --prepend channels nvidia
conda config --show channels
</code></pre></div></div> <p>Error no nvtx3 and no openmpi</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable
✓ OpenMP found
✗ OpenMPI is not found, disabling multi-GPU support
---&gt; On Linux you can try install OpenMPI with `sudo apt install openmpi-bin openmpi-doc libopenmpi-dev`
✓ nvcc found, including GPU/CUDA support
---------------------------------------------
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmcc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt   -o train_gpt2cu
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>I remove channel conda-forge and try to install cuda from main anaconda channel.</p> <p>channel config is stored in <code class="language-plaintext highlighter-rouge">~/.condarc</code></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda info
conda remove <span class="nt">--name</span> myenv <span class="nt">--all</span>
</code></pre></div></div> <p>Fix openmpi issue after installing cuda from main channel</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@ln101 llm.c]$ PRECISION=FP32 make train_gpt2cu
__nvcc_device_query failed to call cudaLoader::cuInit(0) with error 0x64 (CUDA_ERROR_NO_DEVICE)
---------------------------------------------
→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable
✓ OpenMP found
✓ OpenMPI found, OK to train with multiple GPUs
✓ nvcc found, including GPU/CUDA support
---------------------------------------------
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
nvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>Forget that I should ssh to compute node. Let’s try again</p> <p>Still get no nvtx3 error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ PRECISION=FP32 make train_gpt2cu
---------------------------------------------
→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable
✓ OpenMP found
✓ OpenMPI found, OK to train with multiple GPUs
✓ nvcc found, including GPU/CUDA support
---------------------------------------------
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:34:
llmc/cuda_common.h:12:10: fatal error: nvtx3/nvToolsExt.h: No such file or directory
   12 | #include &lt;nvtx3/nvToolsExt.h&gt;
      |          ^~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [train_gpt2cu] Error 255
</code></pre></div></div> <p>Get installment error again</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ conda install cuda-nvtx -c nvidia
Channels:
 - nvidia
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: failed

InvalidSpec: The package "nvidia/linux-64::cuda==12.5.0=0" is not available for the specified platform

(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ exit
</code></pre></div></div> <p>https://github.com/NVIDIA/NVTX I will manually pull header files to node if I can’t now solve this issue this time by install 12.0.0 version of cuda</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install nvidia/label/cuda-12.0.0::cuda-toolkit
</code></pre></div></div> <p>Fix nvtx header file after copying to system path</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In file included from train_gpt2.cu:65:
llmc/zero.cuh:16:10: fatal error: nccl.h: No such file or directory
   16 | #include &lt;nccl.h&gt;
      |          ^~~~~~~~
compilation terminated.
In file included from train_gpt2.cu:65:
llmc/zero.cuh:16:10: fatal error: nccl.h: No such file or directory
   16 | #include &lt;nccl.h&gt;
      |          ^~~~~~~~
compilation terminated.
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K nccl_2.22.3-1+cuda12.4_x86_64]$ cp -r include/* /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/
(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K nccl_2.22.3-1+cuda12.4_x86_64]$ cp -r lib/* /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc -O3 -t=0 --use_fast_math -std=c++17 --generate-code arch=compute_70,code=[compute_70,sm_70] -DMULTI_GPU -DENABLE_FP32 train_gpt2.cu -lcublas -lcublasLt -L/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/  -I/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/include/  -lmpi -lnccl -o train_gpt2cu
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /GPUFS/nsccgz_qylin_1/local/lib/libopen-pal.so.40: undefined reference to `pci_device_cfg_read'
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/</code> is not included in <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> when doing <code class="language-plaintext highlighter-rouge">ld</code> command. So I need to include that in <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> before running <code class="language-plaintext highlighter-rouge">ld</code> command.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ echo $LD_LIBRARY_PATH
/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/t/spack/opt/spack/linux-centos7-haswell/gcc-4.8.5/libevent-2.1.12-oysfi7miuhw62ginwjrr2uy6yldr2oav/lib:/GPUFS/app_GPU/application/anaconda3/5.3.1/envs/python-3.6/lib:/GPUFS/nsccgz_qylin_1/ryz/MARL-test/util/lib:/GPUFS/nsccgz_qylin_1/ryz/icf_test/build_GPTL/gptl_gcc/lib::/GPUFS/nsccgz_qylin_1/software/spack/opt/spack/linux-centos7-skylake_avx512/
</code></pre></div></div> <p>Should I switch to spack next time?</p> <p>Finally I am able to compile this train_gpt2cu after several days of failing and trying. I put conda lib at the first of <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> and it works.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ export  LD_LIBRARY_PATH=/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:$
LD_LIBRARY_PATH
(llmc) [nsccgz_qylin_1@gpu30%tianhe2-K llm.c]$ echo $LD_LIBRARY_PATH
/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/local/lib:/GPUFS/nsccgz_qylin_1/t/spack/opt/spack/linux-centos7-haswell/gcc-4.8.5/libevent-2.1.12-oysfi7miuhw62ginwjrr2uy6yldr2oav/lib:/GPUFS/app_GPU/application/anaconda3/5.3.1/envs/python-3.6/lib:/GPUFS/nsccgz_qylin_1/ryz/MARL-test/util/lib:/GPUFS/nsccgz_qylin_1/ryz/icf_test/build_GPTL/gptl_gcc/lib::/GPUFS/nsccgz_qylin_1/software/spack/opt/spack/linux-centos7-skylake_avx512/:/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/
</code></pre></div></div> <p>Issue: cuda driver version is not compatible with cuda runtime version. What should I do?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
gpu 30
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:8A:00.0 Off |                    0 |
| N/A   39C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   34C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   33C    P0    37W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   36C    P0    51W / 300W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre></div></div> <p>Guess I have to install cuda 11.2 to match the driver version. Have to download nccl with cuda 11.</p> <p>However, I don’t find cuda:toolkit that is compatible with cuda 11.2 in conda.</p> <p>I think one solution is to compile nccl manually on this compute node with old cuda driver.</p> <p>But now I just switch use A800 which comes with cuda 12.0 driver. Let’s go. I have two A800 available so I can test cross node training and multi gpu card training. This is great.</p> <p>Issue: Seems that program is loading a .bin file with _bf16</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
Precision is configured as FP32 but model at gpt2_124M_bf16.bin is not.
---&gt; HINT: to turn on FP32 you have to compile like: `make train_gpt2cu PRECISION=FP32`
---&gt; HINT: are you sure you're loading a .bin file without any _bf16 in the name?
</code></pre></div></div> <p>Finally I think I successfully run model training on two gpus on A800. Here’s compile and run script.</p> <p>compile.sh</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">echo</span> <span class="nv">$LD_LIBRARY_PATH</span>
yhrun <span class="nt">-n1</span> <span class="nt">-p</span> GPU_A800 make train_gpt2cu
</code></pre></div></div> <p>run.sh</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
conda activate llmc
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">echo</span> <span class="nv">$LD_LIBRARY_PATH</span>
yhrun <span class="nt">-n2</span> <span class="nt">-p</span> GPU_A800 /GPUFS/nsccgz_qylin_1/miniconda3/envs/llmc/bin/mpirun <span class="nt">-np</span> 2 train_gpt2cu
</code></pre></div></div> <p>output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| weight init method    | OpenAI's GPT-2 checkpoint                          |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| weight init method    | OpenAI's GPT-2 checkpoint                          |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 37                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 37                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 2                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
HellaSwag eval not found at dev/data/hellaswag/hellaswag_val.bin, skipping its evaluation
You can run `python dev/data/hellaswag.py` to export and use it with `-h 1`.
num_parameters: 124475904 =&gt; bytes: 248951808
allocated 237 MiB for model parameters
batch_size B=4 * seq_len T=1024 * num_processes=2 and total_batch_size=8192

step   15/37 | train loss 3.640566 | norm 2.2486 | lr 3.00e-04 | 268.99 ms | -100.0% bf16 MFU | 63701 tok/s
step   14/37 | train loss 3.381429 | norm 2.5808 | lr 3.00e-04 | 287.48 ms | -100.0% bf16 MFU | 57996 tok/s
step   16/37 | train loss 3.413064 | norm 2.0632 | lr 3.00e-04 | 174.35 ms | -100.0% bf16 MFU | 62144 tok/s
step   15/37 | train loss 3.640008 | norm 2.3061 | lr 3.00e-04 | 147.72 ms | -100.0% bf16 MFU | 57748 tok/s
step   17/37 | train loss 3.584605 | norm 2.0251 | lr 3.00e-04 | 158.54 ms | -100.0% bf16 MFU | 61209 tok/s
step   16/37 | train loss 3.412876 | norm 2.0334 | lr 3.00e-04 | 135.09 ms | -100.0% bf16 MFU | 58018 tok/s
step   18/37 | train loss 3.486408 | norm 1.6377 | lr 3.00e-04 | 115.45 ms | -100.0% bf16 MFU | 62047 tok/s
step   17/37 | train loss 3.584733 | norm 2.0324 | lr 3.00e-04 | 141.30 ms | -100.0% bf16 MFU | 58014 tok/s
step   19/37 | train loss 3.450470 | norm 2.1760 | lr 3.00e-04 | 120.89 ms | -100.0% bf16 MFU | 62521 tok/s
step   18/37 | train loss 3.487690 | norm 1.6469 | lr 3.00e-04 | 128.45 ms | -100.0% bf16 MFU | 58510 tok/s
step   20/37 | train loss 3.542054 | norm 2.2739 | lr 3.00e-04 | 66.91 ms | -100.0% bf16 MFU | 67331 tok/s
step   19/37 | train loss 3.451924 | norm 2.1979 | lr 3.00e-04 | 118.83 ms | -100.0% bf16 MFU | 59375 tok/s
step   20/37 | train loss 3.544027 | norm 2.2397 | lr 3.00e-04 | 108.95 ms | -100.0% bf16 MFU | 60644 tok/s
</code></pre></div></div> <p>6x throughput compared to v100.</p> <p>However, I am not sure if this trully accelerate training. How can I verify that multiple gpu training accelerate training process ?</p> <h3 id="cpu">CPU</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
python dev/data/tinyshakespeare.py
python train_gpt2.py
make train_gpt2
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>8 ./train_gpt2
</code></pre></div></div> <p>Output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>step 20: train loss 4.527330 (took 2636.617334 ms)
step 21: train loss 4.065797 (took 2701.692621 ms)
step 22: train loss 3.965316 (took 2681.297241 ms)
step 23: train loss 3.449409 (took 2650.111416 ms)
step 24: train loss 4.490954 (took 2637.116332 ms)
step 25: train loss 4.035361 (took 2659.843151 ms)
step 26: train loss 3.445302 (took 2652.557792 ms)
step 27: train loss 3.993789 (took 2649.868369 ms)
step 28: train loss 4.199468 (took 2638.095098 ms)
step 29: train loss 4.538460 (took 2669.385015 ms)
val loss 4.350866
step 30: train loss 4.306292 (took 2658.306411 ms)
step 31: train loss 4.851407 (took 2634.616368 ms)
step 32: train loss 4.577479 (took 2670.470130 ms)
step 33: train loss 4.124943 (took 2660.545565 ms)
step 34: train loss 4.330319 (took 2669.532886 ms)
step 35: train loss 3.399416 (took 2639.378693 ms)
step 36: train loss 3.661207 (took 2632.377219 ms)
step 37: train loss 3.330453 (took 2637.114896 ms)
step 38: train loss 3.567853 (took 2645.744510 ms)
step 39: train loss 3.902004 (took 2635.939546 ms)
val loss 4.319361
generating:
---
EditBOOK IX:
Under the boasted sute of Georges:
So lordly is the prize had sin is high;
Hell is the way to God: frankish friends from blessed daughters
To Bermuda have heard the saying,
Then how to place the artscape.
Strong should a bellow
---
step 40: train loss 3.952987 (took 2665.948189 ms)
</code></pre></div></div> <p>Some questions? How many low end gpus are there in the market? I am thinking about utilizing low end gpus to train model, large or small model.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/learning-based-memory-allocation-for-c-server-workloads-summary/">Learning-based memory allocation for C++ server workloads summary</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/my-question/">my question:</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/binary-search-algorithm-variant/">Binary search algorithm variant</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/docker-rocksdb-build/">Docker Rocksdb build</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/difference-between-dockerfile-and-docker-compose/">Difference between Dockerfile and Docker Compose</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhutao Zhuang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: February 25, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-bpe-tokenizer-implementation",title:"BPE tokenizer implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/tokenizer/"}},{id:"post-lock-free-queue-implementation-cpp",title:"Lock free queue implementation cpp",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/lock-free-queue/"}},{id:"post-nanogpt-kv-cache-first-attempt",title:"nanogpt kv cache first attempt",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/llm-kv-cache-first-attempt/"}},{id:"post-set-up-vim-in-vscode",title:"Set up vim in vscode",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/set-up-vim-in-vscode/"}},{id:"post-chibicc-simple-c-compiler-for-statement",title:"chibicc - Simple c compiler for statement",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-for-statement/"}},{id:"post-chibicc-simple-c-compiler-if-statement",title:"chibicc - Simple c compiler if statement",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-if-statement/"}},{id:"post-chibicc-simple-c-compiler-block-node",title:"chibicc - Simple c compiler block {} node",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-block/"}},{id:"post-chibicc-simple-c-compiler-return-keyword",title:"chibicc - Simple c compiler return keyword",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-return-keyword/"}},{id:"post-learned-idnex-survey",title:"Learned idnex survey",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/learned-index-survery/"}},{id:"post-chibicc-c-compiler-multi-char-variable-name",title:"chibicc C compiler - multi char variable name",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-multi-char-var-name/"}},{id:"post-c-and-linux-kernel-memory-allocation",title:"C++ and linux kernel memory allocation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/cpp-interview-questions/"}},{id:"post-chibicc-c-compiler-parser-review-and-expression-evaluator",title:"chibicc C compiler - parser review and expression evaluator",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-parser-review-and-calculator/"}},{id:"post-imperative-programming-vs-declarative-programming",title:"Imperative programming vs. Declarative programming",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/imperative-programming/"}},{id:"post-autodiff-implementation-kernel-and-memory-management",title:"Autodiff implementation - kernel and memory management",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign2/"}},{id:"post-hash-in-cpp",title:"Hash in cpp",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hash-in-cpp/"}},{id:"post-computer-basics",title:"Computer basics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/some-basic-computer-knowledge/"}},{id:"post-rdma",title:"Rdma",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/rdma/"}},{id:"post-topo-sort",title:"Topo sort",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/topo-sort/"}},{id:"post-autodiff-implementation",title:"Autodiff implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign/"}},{id:"post-palindrome-substring-partition",title:"palindrome substring partition",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/palindrome-substring-partition/"}},{id:"post-stf-cs149-flash-attention",title:"Stf CS149 flash attention",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149gpt/"}},{id:"post-speed-up-matrix-multiplication-2",title:"Speed up matrix multiplication 2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-mul-comparison/"}},{id:"post-elf-loading",title:"Elf Loading",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/elf-loading/"}},{id:"post-stf-cs149-parallel-programming-assign3",title:"Stf CS149 Parallel Programming - Assign3",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign3/"}},{id:"post-stf-cs149-parallel-programming-lecture11-cache-coherence",title:"Stf CS149 Parallel Programming - Lecture11 - Cache coherence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture11-cache-coherence/"}},{id:"post-stf-cs149-parallel-programming-lecture-7-cuda-programming-model",title:"Stf CS149 Parallel Programming - Lecture 7 - Cuda programming model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture7-cuda/"}},{id:"post-stf-cs149-parallel-programming-assign2",title:"Stf CS149 Parallel Programming - Assign2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign2/"}},{id:"post-stf-cs149-parallel-programming-lecture-5-amp-6-performance-optimization",title:"Stf CS149 Parallel Programming - Lecture 5&amp;6 - Performance optimization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-lecture-takeaway/"}},{id:"post-stf-cs149-parallel-programming-assign1",title:"Stf CS149 Parallel Programming - Assign1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-assign1/"}},{id:"post-ssh-display-image-on-local-server",title:"ssh display image on local server",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/ssh-display-img/"}},{id:"post-c-compiler-single-letter-local-variable",title:"C compiler - single letter local variable",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-single-letter-ident/"}},{id:"post-c-compiler-parse-example-walkthrough",title:"C compiler - parse example walkthrough",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/statement-and-comparison/"}},{id:"post-linux-get-cpu-time-and-wall-clock-time",title:"Linux get cpu time and wall clock time",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/get-function-cpu-time/"}},{id:"post-simple-c-compiler-unary",title:"Simple c compiler unary",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-unary/"}},{id:"post-simple-c-compiler-gen-expr",title:"Simple c compiler gen expr",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-gen-expr/"}},{id:"post-python-pyplot-trick",title:"Python pyplot trick",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-plot-trick/"}},{id:"post-simple-lru-cache-cpp-implementation",title:"Simple lru cache cpp implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lru-cache/"}},{id:"post-python-capture-function-print-output",title:"Python capture function print output",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/capture-output-python/"}},{id:"post-simple-c-compiler",title:"Simple c compiler",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-compiler/"}},{id:"post-calloc-and-malloc",title:"calloc and malloc",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/calloc/"}},{id:"post-lightgbm-dataset",title:"LightGBM dataset",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-dataset/"}},{id:"post-difference-between-deep-copy-and-shallow-copy-in-python",title:"Difference between deep copy and shallow copy in python",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-deep-copy/"}},{id:"post-python-package-path",title:"Python Package Path",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-package-path/"}},{id:"post-micrograd",title:"micrograd",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/micrograd/"}},{id:"post-cpp-thread-local",title:"cpp thread local",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-thread-local/"}},{id:"post-cpp-async",title:"cpp async",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-async/"}},{id:"post-python-dataframe-drop-row",title:"python dataframe drop row",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/df-drop-row/"}},{id:"post-git-merge-file-from-another-branch",title:"Git merge file from another branch",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/git-merge-file-from-another-branch/"}},{id:"post-efficiency-tips",title:"Efficiency tips",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/efficiency-tips/"}},{id:"post-speed-up-matrix-multiplication",title:"Speed up matrix multiplication",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-multiplication/"}},{id:"post-how-to-write-research-paper",title:"How to write research paper",description:"How to write research paper",section:"Posts",handler:()=>{window.location.href="/blog/2024/how-to-write-research-paper/"}},{id:"post-fast-nano-gpt-training",title:"Fast nano-gpt training",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/gpt-fast/"}},{id:"post-system-for-machine-learning-papers",title:"System for machine learning papers",description:"sysml papers",section:"Posts",handler:()=>{window.location.href="/blog/2024/sysml-papers/"}},{id:"post-nano-gpt-and-transformer",title:"nano-gpt and Transformer",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer/"}},{id:"post-pytorch-tensor-to",title:"pytorch tensor.to",description:"pytorch",section:"Posts",handler:()=>{window.location.href="/blog/2024/pytorch/"}},{id:"post-install-neovim-with-old-glibc",title:"Install neovim with old glibc",description:"vim",section:"Posts",handler:()=>{window.location.href="/blog/2024/install-neovim/"}},{id:"post-llm-c",title:"llm.c",description:"llm minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/llm-c/"}},{id:"post-basic-digital-electronic",title:"Basic digital electronic",description:"transistor",section:"Posts",handler:()=>{window.location.href="/blog/2024/digital-electronic/"}},{id:"post-k8s-advance",title:"K8s Advance",description:"k8s minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/k8s-advance/"}},{id:"post-difference-between-dockerfile-and-docker-compose",title:"Difference between Dockerfile and Docker Compose",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/difference-between-dockerfile-and-docker-compose-d6ebdc687785?source=rss-da1663a42461------2","_blank")}},{id:"post-docker-rocksdb-build",title:"Docker Rocksdb build",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/docker-rocksdb-build-18a0bf0e0bb0?source=rss-da1663a42461------2","_blank")}},{id:"post-k3s-beginner",title:"K3s beginner",description:"k3s",section:"Posts",handler:()=>{window.location.href="/blog/2024/k3s/"}},{id:"post-docker-rocksdb",title:"Docker RocksDB",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-rocksdb/"}},{id:"post-docker-beginner",title:"Docker beginner",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-file-compose-diff/"}},{id:"post-git",title:"Git",description:"git",section:"Posts",handler:()=>{window.location.href="/blog/2024/git/"}},{id:"post-lightgbm-usage-and-implementation",title:"LightGBM usage and implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-usage/"}},{id:"post-backpropogation-c-implementation",title:"Backpropogation C++ Implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/back-propagation/"}},{id:"post-conda-usage",title:"Conda usage",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/conda/"}},{id:"post-install-k8s-cluster-with-3-ubuntu-nodes",title:"Install K8s cluster with 3 ubuntu nodes",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/cloud/"}},{id:"post-convert-svg-figures-to-pdf-latex-before-submitting-to-arxiv",title:"Convert SVG figures to pdf_latex before submitting to arxiv",description:"Convert SVG figures to pdf_latex before submitting to arxiv",section:"Posts",handler:()=>{window.location.href="/blog/2024/arxiv-cleaner/"}},{id:"post-binary-search-algorithm-variant",title:"Binary search algorithm variant",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/binary-search-algorithm-variant-9b5310473471?source=rss-da1663a42461------2","_blank")}},{id:"post-my-question",title:"my question:",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/my-question-a69930f167f0?source=rss-da1663a42461------2","_blank")}},{id:"post-learning-based-memory-allocation-for-c-server-workloads-summary",title:"Learning-based memory allocation for C++ server workloads summary",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/learning-based-memory-allocation-for-c-server-workloads-summary-479e9cd6d6f6?source=rss-da1663a42461------2","_blank")}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-paper-dumpkv-accepted-by-vldb-39-25",title:"A paper(DumpKV) accepted by VLDB&#39;25",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%68%75%61%6E%67%7A%68%75%74%61%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/BilyZ98","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>