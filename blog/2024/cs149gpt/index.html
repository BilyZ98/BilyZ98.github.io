<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Stf CS149 flash attention | Zhutao Zhuang </title> <meta name="author" content="Zhutao Zhuang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bilyz98.github.io/blog/2024/cs149gpt/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhutao</span> Zhuang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Stf CS149 flash attention</h1> <p class="post-meta"> Created in November 12, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="https://github.com/stanford-cs149/cs149gpt" rel="external nofollow noopener" target="_blank">Stf CS149 flash attention lab assignment repo</a></p> <h2 id="install-library-to-compile-code">Install library to compile code</h2> <p>It shows error about error loading shared object.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(cs149gpt) ➜  cs149gpt git:(main) ✗ python3 gpt149.py 4Daccess
/home/zt/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)                                                                                        return torch._C._cuda_getDeviceCount() &gt; 0
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 14, in &lt;module&gt;
    import module_ref as ms
ImportError: /home/zt/stf-cs149-pp/cs149gpt/module_ref.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE
</code></pre></div></div> <p>I tried to use conda to create new env and install low version of pytorch but conda always installs 2.3.x version of torch for me.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 cpuonly python=3.10 numpy=1.26 ninja tiktoken -c pytorch -c conda-forge
</code></pre></div></div> <p>What it is required is 2.1.x version of torch.</p> <p>Then I manually use pip to uninstall torch and reinstall 2.1.x version of torch</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> pip3 uninstall torch
pip3 install torch==2.1.2

</code></pre></div></div> <p>Got another issue saying that I should use numpy version &lt; 2.0.</p> <p>Then I uninstall numpy and reinstall it with version 1.2.x</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 uninstall numpy
 pip3 install numpy==1.26
</code></pre></div></div> <p>So now I can run the code successfully.</p> <h2 id="part-1-naive-attention">Part 1: naive attention</h2> <p>My code produce the value that is 0.0003 less than what solution produces for each element.</p> <p>I don’t know why.</p> <p>Should I use double ?</p> <p>First implementation:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myNaiveAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QK_tTensor</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>

    <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
    <span class="c1">//QK^t Intermediate Tensor has Shape (N, N)</span>
    
    <span class="c1">//Make O Tensor with Shape (B, H, N, d) </span>
    <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

    <span class="c1">//Format O, Q, K, and V tensors into 4D vectors</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>

    <span class="c1">//Format QK_t Tensor into a 2D vector.</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">QK_t</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QK_tTensor</span><span class="p">);</span>
    
    <span class="cm">/* Here is an example of how to read/write 0's to  Q (B, H, N, d) using the 4D accessors

        //loop over Batch Size
         for (int b = 0; b &lt; B; b++) {

             //loop over Heads
             for (int h = 0; h &lt; H; h++) {

                 //loop over Sequence Length
                 for (int i = 0; i &lt; N; i++) {

                     //loop over Embedding Dimensionality
                     for (int j = 0; j &lt; d; j++) {
                        float val = fourDimRead(Q, b, h, i, j, H, N, d);
                        val = 0.0;
                        fourDimWrite(Q, b, h, i, j, H, N, d, val);
                     }
                 }
             }
         }
    */</span>

    <span class="cm">/* Here is an example of how to read/write 0's to  QK_t (N, N) using the 2D accessors

           for (int i = 0; i &lt; N; i++) {
	       for (int j = 0; j &lt; N; j++) {
	           float val = twoDimRead(QK_t, i, j, N);
               val = 0.0;
	           twoDimWrite(QK_t, i, j, N, val);
             }
         }
    */</span>
    
    <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
       <span class="c1">//loop over Heads</span>
       <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
           <span class="c1">//loop over Sequence Length</span>
           <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">seq_i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">seq_i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">seq_i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
             <span class="c1">//loop over Embedding Dimensionality</span>
              <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
               <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">q_row</span>  <span class="o">=</span> <span class="n">i</span><span class="p">;</span> 
                  <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                  <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                  <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">seq_i</span><span class="p">;</span>
                  <span class="c1">// float val = fourDimRead(Q, b, h, i, j, H, N, d);</span>
          <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>


                  <span class="c1">// val = 0.0;</span>
                  <span class="c1">// fourDimWrite(Q, b, h, i, j, H, N, d, val);</span>
               <span class="p">}</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq_i</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">val</span> <span class="p">);</span>

            <span class="p">}</span>

           <span class="p">}</span>
          <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">);</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

            <span class="kt">float</span> <span class="n">row_sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span><span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
               <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
              <span class="kt">float</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
              <span class="n">row_sum</span> <span class="o">+=</span> <span class="n">exp_val</span><span class="p">;</span>
              <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp_val</span><span class="p">;</span>

            <span class="p">}</span>

            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
              <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">row_sum</span><span class="p">;</span>
              <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">prob</span><span class="p">);</span>
            <span class="p">}</span>
          <span class="p">}</span>


        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">qkt_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">qkt_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">qkt_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">output_d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">output_d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">m_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">m_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">m_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">qkt_val</span> <span class="o">=</span>  <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">m_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
            <span class="kt">int</span> <span class="n">v_row</span> <span class="o">=</span> <span class="n">m_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_col</span> <span class="o">=</span> <span class="n">output_d_idx</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row</span><span class="p">,</span> <span class="n">v_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">qkt_val</span> <span class="o">*</span> <span class="n">v_val</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">output_d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="p">,</span><span class="n">val</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="p">}</span>
       <span class="p">}</span>
   <span class="p">}</span>




    
    <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
    <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-17 21:15:18 207308:207308 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0768, 0.0776, 0.0784, 0.0792, 0.0800, 0.0808, 0.0816, 0.0824, 0.0832,
        0.0840, 0.0848, 0.0856, 0.0864, 0.0872, 0.0880, 0.0888, 0.0896, 0.0904,
        0.0912, 0.0920, 0.0928, 0.0936, 0.0944, 0.0952, 0.0960, 0.0968, 0.0976,
        0.0984, 0.0992, 0.1000, 0.1008, 0.1016])
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 311, in main
    part1Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 221, in part1Test
    testTemplate(attentionModuleReference.myUnfusedAttention, params, "STUDENT - NAIVE ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Try to use double to store itermediate result to fix this problem.</p> <p>I have two issues in previous version of code</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
       <span class="c1">//loop over Heads</span>
       <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="c1">//loop over Sequence Length</span>
          <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">seq_i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">seq_i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">seq_i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
             <span class="c1">//loop over Embedding Dimensionality</span>
              <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
               <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">q_row</span>  <span class="o">=</span> <span class="n">i</span><span class="p">;</span> 
                  <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>

                    <span class="c1">// this is the correct indexing for the second matrix.</span>
                    <span class="c1">// Since K is not transposed.</span>
                     <span class="c1">// K should be indexed with (seq_i, j) instead of (j, seq_i) like normal matrix multiplciation</span>
                  <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">seq_i</span><span class="p">;</span>
                  <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                  <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>
               <span class="p">}</span>
                <span class="c1">// This is the second place that is fixed. </span>
                 <span class="c1">// QK_t is two dimenional. </span>
                 <span class="c1">// Should use twoDimWrite</span>
              <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq_i</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">val</span> <span class="p">);</span>
            <span class="p">}</span>

           <span class="p">}</span>
 
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/learning-based-memory-allocation-for-c-server-workloads-summary/">Learning-based memory allocation for C++ server workloads summary</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/my-question/">my question:</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/binary-search-algorithm-variant/">Binary search algorithm variant</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/docker-rocksdb-build/">Docker Rocksdb build</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/difference-between-dockerfile-and-docker-compose/">Difference between Dockerfile and Docker Compose</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Zhutao Zhuang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 18, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-stf-cs149-flash-attention",title:"Stf CS149 flash attention",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149gpt/"}},{id:"post-speed-up-matrix-multiplication-2",title:"Speed up matrix multiplication 2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-mul-comparison/"}},{id:"post-elf-loading",title:"Elf Loading",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/elf-loading/"}},{id:"post-stf-cs149-parallel-programming-assign3",title:"Stf CS149 Parallel Programming - Assign3",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign3/"}},{id:"post-stf-cs149-parallel-programming-lecture11-cache-coherence",title:"Stf CS149 Parallel Programming - Lecture11 - Cache coherence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture11-cache-coherence/"}},{id:"post-stf-cs149-parallel-programming-lecture-7-cuda-programming-model",title:"Stf CS149 Parallel Programming - Lecture 7 - Cuda programming model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture7-cuda/"}},{id:"post-stf-cs149-parallel-programming-assign2",title:"Stf CS149 Parallel Programming - Assign2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign2/"}},{id:"post-stf-cs149-parallel-programming-lecture-5-amp-6-performance-optimization",title:"Stf CS149 Parallel Programming - Lecture 5&amp;6 - Performance optimization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-lecture-takeaway/"}},{id:"post-stf-cs149-parallel-programming-assign1",title:"Stf CS149 Parallel Programming - Assign1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-assign1/"}},{id:"post-ssh-display-image-on-local-server",title:"ssh display image on local server",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/ssh-display-img/"}},{id:"post-c-compiler-single-letter-local-variable",title:"C compiler - single letter local variable",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-single-letter-ident/"}},{id:"post-c-compiler-parse-example-walkthrough",title:"C compiler - parse example walkthrough",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/statement-and-comparison/"}},{id:"post-linux-get-cpu-time-and-wall-clock-time",title:"Linux get cpu time and wall clock time",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/get-function-cpu-time/"}},{id:"post-simple-c-compiler-unary",title:"Simple c compiler unary",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-unary/"}},{id:"post-simple-c-compiler-gen-expr",title:"Simple c compiler gen expr",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-gen-expr/"}},{id:"post-python-pyplot-trick",title:"Python pyplot trick",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-plot-trick/"}},{id:"post-simple-lru-cache-cpp-implementation",title:"Simple lru cache cpp implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lru-cache/"}},{id:"post-python-capture-function-print-output",title:"Python capture function print output",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/capture-output-python/"}},{id:"post-simple-c-compiler",title:"Simple c compiler",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-compiler/"}},{id:"post-calloc-and-malloc",title:"calloc and malloc",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/calloc/"}},{id:"post-lightgbm-dataset",title:"LightGBM dataset",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-dataset/"}},{id:"post-difference-between-deep-copy-and-shallow-copy-in-python",title:"Difference between deep copy and shallow copy in python",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-deep-copy/"}},{id:"post-python-package-path",title:"Python Package Path",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-package-path/"}},{id:"post-micrograd",title:"micrograd",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/micrograd/"}},{id:"post-cpp-thread-local",title:"cpp thread local",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-thread-local/"}},{id:"post-cpp-async",title:"cpp async",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-async/"}},{id:"post-python-dataframe-drop-row",title:"python dataframe drop row",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/df-drop-row/"}},{id:"post-git-merge-file-from-another-branch",title:"Git merge file from another branch",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/git-merge-file-from-another-branch/"}},{id:"post-efficiency-tips",title:"Efficiency tips",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/efficiency-tips/"}},{id:"post-speed-up-matrix-multiplication",title:"Speed up matrix multiplication",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-multiplication/"}},{id:"post-how-to-write-research-paper",title:"How to write research paper",description:"How to write research paper",section:"Posts",handler:()=>{window.location.href="/blog/2024/how-to-write-research-paper/"}},{id:"post-fast-nano-gpt-training",title:"Fast nano-gpt training",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/gpt-fast/"}},{id:"post-system-for-machine-learning-papers",title:"System for machine learning papers",description:"sysml papers",section:"Posts",handler:()=>{window.location.href="/blog/2024/sysml-papers/"}},{id:"post-nano-gpt-and-transformer",title:"nano-gpt and Transformer",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer/"}},{id:"post-pytorch-tensor-to",title:"pytorch tensor.to",description:"pytorch",section:"Posts",handler:()=>{window.location.href="/blog/2024/pytorch/"}},{id:"post-install-neovim-with-old-glibc",title:"Install neovim with old glibc",description:"vim",section:"Posts",handler:()=>{window.location.href="/blog/2024/install-neovim/"}},{id:"post-llm-c",title:"llm.c",description:"llm minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/llm-c/"}},{id:"post-basic-digital-electronic",title:"Basic digital electronic",description:"transistor",section:"Posts",handler:()=>{window.location.href="/blog/2024/digital-electronic/"}},{id:"post-k8s-advance",title:"K8s Advance",description:"k8s minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/k8s-advance/"}},{id:"post-difference-between-dockerfile-and-docker-compose",title:"Difference between Dockerfile and Docker Compose",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/difference-between-dockerfile-and-docker-compose-d6ebdc687785?source=rss-da1663a42461------2","_blank")}},{id:"post-docker-rocksdb-build",title:"Docker Rocksdb build",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/docker-rocksdb-build-18a0bf0e0bb0?source=rss-da1663a42461------2","_blank")}},{id:"post-k3s-beginner",title:"K3s beginner",description:"k3s",section:"Posts",handler:()=>{window.location.href="/blog/2024/k3s/"}},{id:"post-docker-rocksdb",title:"Docker RocksDB",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-rocksdb/"}},{id:"post-docker-beginner",title:"Docker beginner",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-file-compose-diff/"}},{id:"post-git",title:"Git",description:"git",section:"Posts",handler:()=>{window.location.href="/blog/2024/git/"}},{id:"post-lightgbm-usage-and-implementation",title:"LightGBM usage and implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-usage/"}},{id:"post-backpropogation-c-implementation",title:"Backpropogation C++ Implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/back-propagation/"}},{id:"post-conda-usage",title:"Conda usage",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/conda/"}},{id:"post-install-k8s-cluster-with-3-ubuntu-nodes",title:"Install K8s cluster with 3 ubuntu nodes",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/cloud/"}},{id:"post-convert-svg-figures-to-pdf-latex-before-submitting-to-arxiv",title:"Convert SVG figures to pdf_latex before submitting to arxiv",description:"Convert SVG figures to pdf_latex before submitting to arxiv",section:"Posts",handler:()=>{window.location.href="/blog/2024/arxiv-cleaner/"}},{id:"post-binary-search-algorithm-variant",title:"Binary search algorithm variant",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/binary-search-algorithm-variant-9b5310473471?source=rss-da1663a42461------2","_blank")}},{id:"post-my-question",title:"my question:",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/my-question-a69930f167f0?source=rss-da1663a42461------2","_blank")}},{id:"post-learning-based-memory-allocation-for-c-server-workloads-summary",title:"Learning-based memory allocation for C++ server workloads summary",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/learning-based-memory-allocation-for-c-server-workloads-summary-479e9cd6d6f6?source=rss-da1663a42461------2","_blank")}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%68%75%61%6E%67%7A%68%75%74%61%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/BilyZ98","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>