<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Stf CS149 flash attention | Zhutao Zhuang </title> <meta name="author" content="Zhutao Zhuang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bilyz98.github.io/blog/2024/cs149gpt/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhutao</span> Zhuang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Stf CS149 flash attention</h1> <p class="post-meta"> Created in November 12, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="https://github.com/stanford-cs149/cs149gpt" rel="external nofollow noopener" target="_blank">Stf CS149 flash attention lab assignment repo</a></p> <h2 id="install-library-to-compile-code">Install library to compile code</h2> <p>It shows error about error loading shared object.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(cs149gpt) ➜  cs149gpt git:(main) ✗ python3 gpt149.py 4Daccess
/home/zt/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)                                                                                        return torch._C._cuda_getDeviceCount() &gt; 0
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 14, in &lt;module&gt;
    import module_ref as ms
ImportError: /home/zt/stf-cs149-pp/cs149gpt/module_ref.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE
</code></pre></div></div> <p>I tried to use conda to create new env and install low version of pytorch but conda always installs 2.3.x version of torch for me.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 cpuonly python=3.10 numpy=1.26 ninja tiktoken -c pytorch -c conda-forge
</code></pre></div></div> <p>What it is required is 2.1.x version of torch.</p> <p>Then I manually use pip to uninstall torch and reinstall 2.1.x version of torch</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> pip3 uninstall torch
pip3 install torch==2.1.2

</code></pre></div></div> <p>Got another issue saying that I should use numpy version &lt; 2.0.</p> <p>Then I uninstall numpy and reinstall it with version 1.2.x</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 uninstall numpy
 pip3 install numpy==1.26
</code></pre></div></div> <p>So now I can run the code successfully.</p> <h2 id="part-1-naive-attention">Part 1: naive attention</h2> <p>My code produce the value that is 0.0003 less than what solution produces for each element.</p> <p>I don’t know why.</p> <p>Should I use double ?</p> <p>First implementation:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myNaiveAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QK_tTensor</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>

    <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
    <span class="c1">//QK^t Intermediate Tensor has Shape (N, N)</span>
    
    <span class="c1">//Make O Tensor with Shape (B, H, N, d) </span>
    <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

    <span class="c1">//Format O, Q, K, and V tensors into 4D vectors</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>

    <span class="c1">//Format QK_t Tensor into a 2D vector.</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">QK_t</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QK_tTensor</span><span class="p">);</span>
    
    <span class="cm">/* Here is an example of how to read/write 0's to  Q (B, H, N, d) using the 4D accessors

        //loop over Batch Size
         for (int b = 0; b &lt; B; b++) {

             //loop over Heads
             for (int h = 0; h &lt; H; h++) {

                 //loop over Sequence Length
                 for (int i = 0; i &lt; N; i++) {

                     //loop over Embedding Dimensionality
                     for (int j = 0; j &lt; d; j++) {
                        float val = fourDimRead(Q, b, h, i, j, H, N, d);
                        val = 0.0;
                        fourDimWrite(Q, b, h, i, j, H, N, d, val);
                     }
                 }
             }
         }
    */</span>

    <span class="cm">/* Here is an example of how to read/write 0's to  QK_t (N, N) using the 2D accessors

           for (int i = 0; i &lt; N; i++) {
	       for (int j = 0; j &lt; N; j++) {
	           float val = twoDimRead(QK_t, i, j, N);
               val = 0.0;
	           twoDimWrite(QK_t, i, j, N, val);
             }
         }
    */</span>
    
    <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
       <span class="c1">//loop over Heads</span>
       <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
           <span class="c1">//loop over Sequence Length</span>
           <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">seq_i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">seq_i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">seq_i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
             <span class="c1">//loop over Embedding Dimensionality</span>
              <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
               <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">q_row</span>  <span class="o">=</span> <span class="n">i</span><span class="p">;</span> 
                  <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                  <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                  <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">seq_i</span><span class="p">;</span>
                  <span class="c1">// float val = fourDimRead(Q, b, h, i, j, H, N, d);</span>
          <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>


                  <span class="c1">// val = 0.0;</span>
                  <span class="c1">// fourDimWrite(Q, b, h, i, j, H, N, d, val);</span>
               <span class="p">}</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq_i</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">val</span> <span class="p">);</span>

            <span class="p">}</span>

           <span class="p">}</span>
          <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">);</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

            <span class="kt">float</span> <span class="n">row_sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span><span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
               <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
              <span class="kt">float</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
              <span class="n">row_sum</span> <span class="o">+=</span> <span class="n">exp_val</span><span class="p">;</span>
              <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp_val</span><span class="p">;</span>

            <span class="p">}</span>

            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
              <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">row_sum</span><span class="p">;</span>
              <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">prob</span><span class="p">);</span>
            <span class="p">}</span>
          <span class="p">}</span>


        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">qkt_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">qkt_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">qkt_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">output_d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">output_d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">m_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">m_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">m_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">qkt_val</span> <span class="o">=</span>  <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">m_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
            <span class="kt">int</span> <span class="n">v_row</span> <span class="o">=</span> <span class="n">m_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_col</span> <span class="o">=</span> <span class="n">output_d_idx</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row</span><span class="p">,</span> <span class="n">v_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">qkt_val</span> <span class="o">*</span> <span class="n">v_val</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">output_d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="p">,</span><span class="n">val</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="p">}</span>
       <span class="p">}</span>
   <span class="p">}</span>




    
    <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
    <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-17 21:15:18 207308:207308 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0768, 0.0776, 0.0784, 0.0792, 0.0800, 0.0808, 0.0816, 0.0824, 0.0832,
        0.0840, 0.0848, 0.0856, 0.0864, 0.0872, 0.0880, 0.0888, 0.0896, 0.0904,
        0.0912, 0.0920, 0.0928, 0.0936, 0.0944, 0.0952, 0.0960, 0.0968, 0.0976,
        0.0984, 0.0992, 0.1000, 0.1008, 0.1016])
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-17 21:15:19 207308:207308 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 311, in main
    part1Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 221, in part1Test
    testTemplate(attentionModuleReference.myUnfusedAttention, params, "STUDENT - NAIVE ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Try to use double to store itermediate result to fix this problem.</p> <p>I have two issues in previous version of code</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
       <span class="c1">//loop over Heads</span>
       <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="c1">//loop over Sequence Length</span>
          <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">seq_i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">seq_i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">seq_i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
             <span class="c1">//loop over Embedding Dimensionality</span>
              <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
               <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">q_row</span>  <span class="o">=</span> <span class="n">i</span><span class="p">;</span> 
                  <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>

                    <span class="c1">// this is the correct indexing for the second matrix.</span>
                    <span class="c1">// Since K is not transposed.</span>
                     <span class="c1">// K should be indexed with (seq_i, j) instead of (j, seq_i) like normal matrix multiplciation</span>
                  <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">seq_i</span><span class="p">;</span>
                  <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                  <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>
               <span class="p">}</span>
                <span class="c1">// This is the second place that is fixed. </span>
                 <span class="c1">// QK_t is two dimenional. </span>
                 <span class="c1">// Should use twoDimWrite</span>
              <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq_i</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">val</span> <span class="p">);</span>
            <span class="p">}</span>

           <span class="p">}</span>
 
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
REFERENCE - NAIVE ATTENTION statistics
cpu time:  293.585ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:10:17 1148267:1148267 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:10:17 1148267:1148267 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 13:10:17 1148267:1148267 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.2747969627380371

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::empty         0.01%      16.000us         0.01%      16.000us       2.000us       5.00 Mb       5.00 Mb             8
    STUDENT - NAIVE ATTENTION        99.49%     273.438ms        99.68%     273.946ms     273.946ms       4.50 Mb      -1.00 Mb             1
                  aten::zeros         0.01%      14.000us         0.05%     133.000us      66.500us       4.50 Mb           0 b             2
                  aten::clone         0.01%      20.000us         0.13%     346.000us     173.000us       1.00 Mb           0 b             2
          aten::empty_strided         0.00%      12.000us         0.00%      12.000us       2.400us     512.51 Kb     512.51 Kb             5
              model_inference         0.20%     549.000us       100.00%     274.838ms     274.838ms     512.00 Kb      -4.00 Mb             1
                aten::flatten         0.01%      19.000us         0.08%     209.000us      41.800us     512.00 Kb           0 b             5
             aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                     aten::to         0.00%       6.000us         0.01%      31.000us       5.167us         520 b           0 b             6
               aten::_to_copy         0.01%      14.000us         0.01%      25.000us       6.250us         520 b           0 b             4
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 274.838ms

STUDENT - NAIVE ATTENTION statistics
cpu time:  273.946ms
mem usage:  4718592 bytes
</code></pre></div></div> <h2 id="part2-blocked-matrix-multiplication">Part2: blocked matrix multiplication</h2> <p>Initial version</p> <p>Unlike that of code written in cuda, I have to manually iterate all rows and cols of input matrix.</p> <p>Don’t know if this is correct way to do that.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define TILE_SIZE 16
</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myUnfusedAttentionBlocked</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QK_tTensor</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>
  
  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
  <span class="c1">//QK^t Intermediate Tensor has Shape (N, N)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d) </span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

  <span class="c1">//Format O, Q, K, and V tensors into 4D vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>

  <span class="c1">//Format QK_t Tensor into a 2D vector.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">QK_t</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QK_tTensor</span><span class="p">);</span>

  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">q_row_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">q_row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">q_row_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// K is not transposed so we traverse k by row.</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_row_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k_row_tile_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_col_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_col_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">d_col_tile_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
              <span class="c1">// int out_row_idx = q_row_tile_idx * TILE_SIZE + tile_row_idx;</span>
              <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="c1">// int out_col_idx = k_row_tile_idx * TILE_SIZE + tile_col_idx;</span>
                <span class="kt">int</span> <span class="n">q_col_idx</span> <span class="o">=</span> <span class="n">d_col_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span><span class="n">q_row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span> 
                <span class="kt">int</span> <span class="n">k_row_idx</span> <span class="o">=</span> <span class="n">k_row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
                <span class="kt">int</span> <span class="n">k_col_idx</span> <span class="o">=</span> <span class="n">d_col_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="k">if</span><span class="p">(</span><span class="n">q_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">q_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span> <span class="o">&amp;&amp;</span> <span class="n">k_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">k_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">float</span> <span class="n">q_tile_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">q_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">k_tile_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row_idx</span><span class="p">,</span> <span class="n">k_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">orig_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">q_row_tile_idx</span><span class="p">,</span> <span class="n">k_row_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">q_tile_val</span> <span class="o">*</span> <span class="n">k_tile_val</span> <span class="o">+</span> <span class="n">orig_val</span><span class="p">;</span>
                  <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">q_row_tile_idx</span><span class="p">,</span> <span class="n">k_row_tile_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">val</span> <span class="p">);</span>
                <span class="p">}</span>
              <span class="p">}</span>
            <span class="p">}</span>
          <span class="p">}</span>

        <span class="p">}</span>
      <span class="p">}</span>

      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">);</span>
        <span class="kt">double</span> <span class="n">row_sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span><span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
           <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
          <span class="kt">double</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
          <span class="n">row_sum</span> <span class="o">+=</span> <span class="n">exp_val</span><span class="p">;</span>
          <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp_val</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">row_sum</span><span class="p">;</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">prob</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">qkt_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">qkt_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">qkt_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">output_d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">output_d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">m_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">m_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">m_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">qkt_val</span> <span class="o">=</span>  <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">m_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
            <span class="kt">int</span> <span class="n">v_row</span> <span class="o">=</span> <span class="n">m_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_col</span> <span class="o">=</span> <span class="n">output_d_idx</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row</span><span class="p">,</span> <span class="n">v_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">qkt_val</span> <span class="o">*</span> <span class="n">v_val</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">qkt_row_idx</span><span class="p">,</span> <span class="n">output_d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="p">,</span><span class="n">val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

    <span class="p">}</span>

  <span class="p">}</span>


  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <p>It’s not correct</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">-----</span><span class="n">RUNNING</span> <span class="n">STUDENT</span> <span class="n">IMPLEMENTATION</span><span class="o">-----</span>

<span class="n">first</span> <span class="n">row</span> <span class="n">value</span><span class="o">:</span>
 <span class="n">tensor</span><span class="p">([</span><span class="mf">0.0771</span><span class="p">,</span> <span class="mf">0.0779</span><span class="p">,</span> <span class="mf">0.0787</span><span class="p">,</span> <span class="mf">0.0795</span><span class="p">,</span> <span class="mf">0.0803</span><span class="p">,</span> <span class="mf">0.0811</span><span class="p">,</span> <span class="mf">0.0819</span><span class="p">,</span> <span class="mf">0.0827</span><span class="p">,</span> <span class="mf">0.0835</span><span class="p">,</span>
        <span class="mf">0.0843</span><span class="p">,</span> <span class="mf">0.0851</span><span class="p">,</span> <span class="mf">0.0859</span><span class="p">,</span> <span class="mf">0.0867</span><span class="p">,</span> <span class="mf">0.0875</span><span class="p">,</span> <span class="mf">0.0883</span><span class="p">,</span> <span class="mf">0.0891</span><span class="p">,</span> <span class="mf">0.0899</span><span class="p">,</span> <span class="mf">0.0907</span><span class="p">,</span>
        <span class="mf">0.0915</span><span class="p">,</span> <span class="mf">0.0923</span><span class="p">,</span> <span class="mf">0.0931</span><span class="p">,</span> <span class="mf">0.0939</span><span class="p">,</span> <span class="mf">0.0947</span><span class="p">,</span> <span class="mf">0.0955</span><span class="p">,</span> <span class="mf">0.0963</span><span class="p">,</span> <span class="mf">0.0971</span><span class="p">,</span> <span class="mf">0.0979</span><span class="p">,</span>
        <span class="mf">0.0987</span><span class="p">,</span> <span class="mf">0.0995</span><span class="p">,</span> <span class="mf">0.1003</span><span class="p">,</span> <span class="mf">0.1011</span><span class="p">,</span> <span class="mf">0.1019</span><span class="p">])</span>
<span class="n">STAGE</span><span class="o">:</span><span class="mi">2024</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">18</span> <span class="mi">11</span><span class="o">:</span><span class="mi">29</span><span class="o">:</span><span class="mi">24</span> <span class="mi">607233</span><span class="o">:</span><span class="mi">607233</span> <span class="n">ActivityProfilerController</span><span class="p">.</span><span class="n">cpp</span><span class="o">:</span><span class="mi">312</span><span class="p">]</span> <span class="n">Completed</span> <span class="n">Stage</span><span class="o">:</span> <span class="n">Warm</span> <span class="n">Up</span>
<span class="n">student</span> <span class="n">first</span> <span class="n">row</span><span class="o">:</span>
 <span class="n">tensor</span><span class="p">([</span><span class="mf">0.0767</span><span class="p">,</span> <span class="mf">0.0775</span><span class="p">,</span> <span class="mf">0.0783</span><span class="p">,</span> <span class="mf">0.0791</span><span class="p">,</span> <span class="mf">0.0799</span><span class="p">,</span> <span class="mf">0.0807</span><span class="p">,</span> <span class="mf">0.0815</span><span class="p">,</span> <span class="mf">0.0823</span><span class="p">,</span> <span class="mf">0.0831</span><span class="p">,</span>
        <span class="mf">0.0839</span><span class="p">,</span> <span class="mf">0.0847</span><span class="p">,</span> <span class="mf">0.0855</span><span class="p">,</span> <span class="mf">0.0863</span><span class="p">,</span> <span class="mf">0.0871</span><span class="p">,</span> <span class="mf">0.0879</span><span class="p">,</span> <span class="mf">0.0887</span><span class="p">,</span> <span class="mf">0.0895</span><span class="p">,</span> <span class="mf">0.0903</span><span class="p">,</span>
        <span class="mf">0.0911</span><span class="p">,</span> <span class="mf">0.0919</span><span class="p">,</span> <span class="mf">0.0927</span><span class="p">,</span> <span class="mf">0.0935</span><span class="p">,</span> <span class="mf">0.0943</span><span class="p">,</span> <span class="mf">0.0951</span><span class="p">,</span> <span class="mf">0.0959</span><span class="p">,</span> <span class="mf">0.0967</span><span class="p">,</span> <span class="mf">0.0975</span><span class="p">,</span>
        <span class="mf">0.0983</span><span class="p">,</span> <span class="mf">0.0991</span><span class="p">,</span> <span class="mf">0.0999</span><span class="p">,</span> <span class="mf">0.1007</span><span class="p">,</span> <span class="mf">0.1015</span><span class="p">])</span>
<span class="n">STAGE</span><span class="o">:</span><span class="mi">2024</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">18</span> <span class="mi">11</span><span class="o">:</span><span class="mi">29</span><span class="o">:</span><span class="mi">24</span> <span class="mi">607233</span><span class="o">:</span><span class="mi">607233</span> <span class="n">ActivityProfilerController</span><span class="p">.</span><span class="n">cpp</span><span class="o">:</span><span class="mi">318</span><span class="p">]</span> <span class="n">Completed</span> <span class="n">Stage</span><span class="o">:</span> <span class="n">Collection</span>
<span class="n">STAGE</span><span class="o">:</span><span class="mi">2024</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">18</span> <span class="mi">11</span><span class="o">:</span><span class="mi">29</span><span class="o">:</span><span class="mi">24</span> <span class="mi">607233</span><span class="o">:</span><span class="mi">607233</span> <span class="n">ActivityProfilerController</span><span class="p">.</span><span class="n">cpp</span><span class="o">:</span><span class="mi">322</span><span class="p">]</span> <span class="n">Completed</span> <span class="n">Stage</span><span class="o">:</span> <span class="n">Post</span> <span class="n">Processing</span>
<span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span><span class="o">:</span>
  <span class="n">File</span> <span class="s">"/home/zt/stf-cs149-pp/cs149gpt/gpt149.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">329</span><span class="p">,</span> <span class="n">in</span> <span class="o">&lt;</span><span class="k">module</span><span class="o">&gt;</span>
    <span class="n">main</span><span class="p">()</span>
  <span class="n">File</span> <span class="s">"/home/zt/stf-cs149-pp/cs149gpt/gpt149.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">313</span><span class="p">,</span> <span class="n">in</span> <span class="n">main</span>
    <span class="n">part2Test</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
  <span class="n">File</span> <span class="s">"/home/zt/stf-cs149-pp/cs149gpt/gpt149.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">233</span><span class="p">,</span> <span class="n">in</span> <span class="n">part2Test</span>
    <span class="n">testTemplate</span><span class="p">(</span><span class="n">attentionModuleReference</span><span class="p">.</span><span class="n">myUnfusedAttentionBlocked</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="s">"STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX"</span><span class="p">)</span>
  <span class="n">File</span> <span class="s">"/home/zt/stf-cs149-pp/cs149gpt/gpt149.py"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">182</span><span class="p">,</span> <span class="n">in</span> <span class="n">testTemplate</span>
    <span class="n">assert</span> <span class="n">torch</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">QKV</span><span class="p">,</span><span class="n">QKS1</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">),</span> <span class="n">correctness_error_message</span>
<span class="n">AssertionError</span><span class="o">:</span>
<span class="o">-------------------------------------------</span>
 <span class="n">YOUR</span> <span class="n">ATTENTION</span> <span class="n">PRODUCED</span> <span class="n">INCORRECT</span> <span class="n">RESULTS</span>
</code></pre></div></div> <p>Fixed the two indexing issue in original issue.</p> <p>This is the correct implementation given by gpt. However, it’s too slow</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      for (int q_row_tile_idx = 0; q_row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; q_row_tile_idx++) {
    for (int k_row_tile_idx = 0; k_row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; k_row_tile_idx++) {
        for (int d_col_tile_idx = 0; d_col_tile_idx &lt; (d + TILE_SIZE - 1) / TILE_SIZE; d_col_tile_idx++) {
            for (int tile_row_idx = 0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {
                for (int tile_col_idx = 0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {
                    int q_row_idx = q_row_tile_idx * TILE_SIZE + tile_row_idx;
                    int k_row_idx = k_row_tile_idx * TILE_SIZE + tile_col_idx; // Fix indexing
                    for (int d_idx = d_col_tile_idx * TILE_SIZE; d_idx &lt; (d_col_tile_idx + 1) * TILE_SIZE; d_idx++) {
                        if (q_row_idx &lt; N &amp;&amp; k_row_idx &lt; N &amp;&amp; d_idx &lt; d) {
                            float q_tile_val = fourDimRead(Q, b, h, q_row_idx, d_idx, H, N, d);
                            float k_tile_val = fourDimRead(K, b, h, k_row_idx, d_idx, H, N, d);
                            float orig_val = twoDimRead(QK_t, q_row_idx, k_row_idx, N);
                            float val = q_tile_val * k_tile_val + orig_val;
                            twoDimWrite(QK_t, q_row_idx, k_row_idx, N, val);
                        }
                    }
                }
            }
        }
    }
}
</code></pre></div></div> <p>Explanation from gpt</p> <p>You’re right to question the necessity of traversing the dimension ( d ) fully. Let’s clarify why it’s important in some cases and whether it applies here.</p> <h3 id="understanding-the-role-of-dimension--d-">Understanding the Role of Dimension ( d )</h3> <p>In block matrix multiplication:</p> <ol> <li> <strong>Matrix Multiplication Basics</strong>: <ul> <li>To compute an element ((i, j)) of the resulting matrix ( QK_t ), you need to calculate the dot product of the (i)-th row of (Q) and the (j)-th row of (K).</li> <li>This involves summing the product of corresponding elements from both rows over the shared dimension ( d ):<br> [ QK_t[i, j] = \sum_{k=0}^{d-1} Q[i, k] \times K[j, k] ]</li> </ul> </li> <li> <strong>Tiling</strong>: <ul> <li>In tiled matrix multiplication, the computation of a block of ( QK_t ) (e.g., a ( \text{TILE_SIZE} \times \text{TILE_SIZE} ) block) is split into smaller sub-blocks to improve memory locality.</li> <li>Each sub-block computation iterates over the shared dimension ( d ) in chunks.</li> </ul> </li> </ol> <h3 id="why-traverse--d-">Why Traverse ( d )?</h3> <p>If you don’t traverse ( d ), the computation will only consider partial contributions to the dot product. This would leave the values in ( QK_t ) incomplete.</p> <p>For example, if ( d = 6 ), and your block size is ( \text{TILE_SIZE} = 3 ):</p> <ul> <li>You compute the dot product for ( d \in [0, 2] ), but ignore ( d \in [3, 5] ), leading to incorrect results.</li> </ul> <h3 id="why-the-code-needs-it">Why the Code Needs It</h3> <p>Your original code suggests you’re updating ( QK_t ) incrementally:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">q_tile_val</span> <span class="o">*</span> <span class="n">k_tile_val</span> <span class="o">+</span> <span class="n">orig_val</span><span class="p">;</span>
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">orig_val</code> accumulates partial results. Without iterating fully over ( d ), the result will be incomplete because only a subset of the dot product contributions is added.</p> <h3 id="when--d--traversal-isnt-necessary">When ( d ) Traversal Isn’t Necessary</h3> <p>In rare cases, you might not need to traverse ( d ) if:</p> <ol> <li> <strong>Each block fully covers ( d )</strong>: <ul> <li>This is possible only when ( \text{TILE_SIZE} ) equals ( d ).</li> <li>In this case, the tiling logic aligns perfectly with the shared dimension, and a single traversal suffices.</li> </ul> </li> <li> <strong>Precomputed Values</strong>: <ul> <li>If ( Q ) or ( K ) has been preprocessed (e.g., multiplied previously), the shared dimension ( d ) might not need iteration.</li> </ul> </li> </ol> <h3 id="conclusion">Conclusion</h3> <p>For the general case (and your code):</p> <ul> <li>Traversing ( d ) fully ensures the correctness of the dot product computation across the shared dimension.</li> <li>If you’re sure ( d ) traversal isn’t necessary, it usually means ( Q ) and ( K ) are already structured to encapsulate the full dot product for each tile.</li> </ul> <p>If you’re still unsure about why it’s needed in your case, we can examine your problem more closely to confirm!</p> <p>I can view the outside for loop from two kind of perspective.</p> <p>One is from input matrix.</p> <p>One is from output matrix. This one makes more sense after reading code generated by gpt</p> <p>Two outer for loop traverse row and column tiles of output matrix. The third for loop traverse the row in input matrix and the column in second input matrix The fourth and fifth for loop traverse the row and column elements in current tile. The sixth for loop does dot product for a single row in input tile and single column in output tile and accumulate the sum. And then write back the sum to output matrix.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for (int row_tile_idx = 0; row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; row_tile_idx++) {
    for (int col_tile_idx = 0; col_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; col_tile_idx++) {
        // Initialize the current block of the output matrix
        for (int i = 0; i &lt; TILE_SIZE; i++) {
            for (int j = 0; j &lt; TILE_SIZE; j++) {
                int row_idx = row_tile_idx * TILE_SIZE + i;
                int col_idx = col_tile_idx * TILE_SIZE + j;
                if (row_idx &lt; N &amp;&amp; col_idx &lt; N) {
                    twoDimWrite(C, row_idx, col_idx, N, 0.0); // Initialize to zero
                }
            }
        }

        for (int k_tile_idx = 0; k_tile_idx &lt; (d + TILE_SIZE - 1) / TILE_SIZE; k_tile_idx++) {
            for (int i = 0; i &lt; TILE_SIZE; i++) {
                for (int j = 0; j &lt; TILE_SIZE; j++) {
                    int row_idx = row_tile_idx * TILE_SIZE + i;
                    int col_idx = col_tile_idx * TILE_SIZE + j;
                    if (row_idx &gt;= N || col_idx &gt;= N) continue;

                    float sum = twoDimRead(C, row_idx, col_idx, N);

                    for (int k = 0; k &lt; TILE_SIZE; k++) {
                        int k_idx = k_tile_idx * TILE_SIZE + k;
                        if (k_idx &gt;= d) break;

                        float a_val = twoDimRead(A, row_idx, k_idx, d);
                        float b_val = twoDimRead(B, col_idx, k_idx, d); // Column index now indexes rows in B
                        sum += a_val * b_val;
                    }

                    twoDimWrite(C, row_idx, col_idx, N, sum);
                }
            }
        }
    }
}

</code></pre></div></div> <p>The code above use global buffer.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      for(int row_tile_idx=0; row_tile_idx &lt; (N+TILE_SIZE-1)/TILE_SIZE; row_tile_idx++) {
        for(int col_tile_idx=0; col_tile_idx &lt; (N+TILE_SIZE-1)/TILE_SIZE; col_tile_idx++) {
          for(int k_tile_idx=0; k_tile_idx &lt; (d+TILE_SIZE-1)/TILE_SIZE; k_tile_idx++  ) {
            for(int tile_row_idx=0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {
              for(int tile_col_idx=0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                if(row_idx &gt;= N || col_idx &gt;= N) {
                  continue;
                }
                float sum = twoDimRead(QK_t, row_idx, col_idx, N);

                for(int k=0; k &lt; TILE_SIZE; k++) {
                  int k_idx = k_tile_idx * TILE_SIZE + k;
                  if(k_idx &gt;= d) break;
                  float q_val =  fourDimRead(Q,b, h, row_idx, k_idx, H, N, d);
                  float k_val = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);
                  sum += q_val * k_val;
                }
                twoDimWrite(QK_t, row_idx, col_idx, N, sum);
              }

            }
          }
        }
      }
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  238.938ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:39:56 1318038:1318038 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:39:56 1318038:1318038 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 13:39:56 1318038:1318038 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.29826903343200684

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.01%      21.000us         0.01%      21.000us       2.625us       5.00 Mb       5.00 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.35%     296.386ms        99.70%     297.443ms     297.443ms       4.50 Mb      -1.00 Mb             1
                                   aten::zeros         0.01%      15.000us         0.24%     706.000us     353.000us       4.50 Mb           0 b             2
                                   aten::clone         0.01%      19.000us         0.11%     328.000us     164.000us       1.00 Mb           0 b             2
                           aten::empty_strided         0.00%      11.000us         0.00%      11.000us       2.200us     512.51 Kb     512.51 Kb             5
                               model_inference         0.18%     550.000us       100.00%     298.329ms     298.329ms     512.00 Kb      -4.00 Mb             1
                                 aten::flatten         0.01%      15.000us         0.07%     200.000us      40.000us     512.00 Kb           0 b             5
                              aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                                      aten::to         0.00%       6.000us         0.01%      31.000us       5.167us         520 b           0 b             6
                                aten::_to_copy         0.00%      14.000us         0.01%      25.000us       6.250us         520 b           0 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 298.329ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  297.443ms
mem usage:  4718592 bytes
</code></pre></div></div> <p>This code use local buffer that is allocated. A little bit faster but not a lot.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#pragma omp parallel for collapse(2) // Parallelize the two outermost loops
for (int row_tile_idx = 0; row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; row_tile_idx++) {
    for (int col_tile_idx = 0; col_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; col_tile_idx++) {
        for (int k_tile_idx = 0; k_tile_idx &lt; (d + TILE_SIZE - 1) / TILE_SIZE; k_tile_idx++) {

            // Buffers for tile data
            float Q_tile[TILE_SIZE][TILE_SIZE];
            float K_tile[TILE_SIZE][TILE_SIZE];

            // Preload Q and K tiles into local buffers
            for (int tile_row_idx = 0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                if (row_idx &gt;= N) continue; // Skip out-of-bound rows

                for (int k = 0; k &lt; TILE_SIZE; k++) {
                    int k_idx = k_tile_idx * TILE_SIZE + k;
                    if (k_idx &lt; d) {
                        Q_tile[tile_row_idx][k] = fourDimRead(Q, b, h, row_idx, k_idx, H, N, d);
                    } else {
                        Q_tile[tile_row_idx][k] = 0.0f; // Fill with zero if out-of-bounds
                    }
                }
            }

            for (int tile_col_idx = 0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {
                int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                if (col_idx &gt;= N) continue; // Skip out-of-bound columns

                for (int k = 0; k &lt; TILE_SIZE; k++) {
                    int k_idx = k_tile_idx * TILE_SIZE + k;
                    if (k_idx &lt; d) {
                        K_tile[tile_col_idx][k] = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);
                    } else {
                        K_tile[tile_col_idx][k] = 0.0f; // Fill with zero if out-of-bounds
                    }
                }
            }

            // Compute the dot product for the current tile
            for (int tile_row_idx = 0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {
                int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;
                if (row_idx &gt;= N) continue; // Skip out-of-bound rows

                for (int tile_col_idx = 0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {
                    int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;
                    if (col_idx &gt;= N) continue; // Skip out-of-bound columns

                    float sum = twoDimRead(QK_t, row_idx, col_idx, N);

                    // Unrolled loop for vectorized dot product
                    for (int k = 0; k &lt; TILE_SIZE; k++) {
                        sum += Q_tile[tile_row_idx][k] * K_tile[tile_col_idx][k];
                    }

                    twoDimWrite(QK_t, row_idx, col_idx, N, sum);
                }
            }
        }
    }
}

</code></pre></div></div> <p>OUtput:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  226.667ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:44:02 1342423:1342423 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 13:44:02 1342423:1342423 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 13:44:02 1342423:1342423 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.2852001190185547

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.00%      13.000us         0.00%      13.000us       1.625us       5.00 Mb       5.00 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.52%     283.895ms        99.68%     284.350ms     284.350ms       4.50 Mb      -1.00 Mb             1
                                   aten::zeros         0.01%      17.000us         0.05%     131.000us      65.500us       4.50 Mb           0 b             2
                                   aten::clone         0.01%      23.000us         0.11%     300.000us     150.000us       1.00 Mb           0 b             2
                           aten::empty_strided         0.00%      12.000us         0.00%      12.000us       2.400us     512.26 Kb     512.26 Kb             5
                               model_inference         0.20%     565.000us       100.00%     285.254ms     285.254ms     512.00 Kb      -4.00 Mb             1
                                 aten::flatten         0.01%      16.000us         0.06%     159.000us      31.800us     512.00 Kb           0 b             5
                              aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                                      aten::to         0.00%      10.000us         0.01%      31.000us       5.167us         520 b         256 b             6
                                aten::_to_copy         0.01%      16.000us         0.01%      26.000us       6.500us         520 b         256 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 285.254ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  284.35ms
mem usage:  4718592 bytes
</code></pre></div></div> <p>Code that do block matrix multiplication for both (Q * K_t) and (softmax(QK_t ) * V):</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define TILE_SIZE 16
</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myUnfusedAttentionBlocked</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QK_tTensor</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>
  
  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
  <span class="c1">//QK^t Intermediate Tensor has Shape (N, N)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d) </span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

  <span class="c1">//Format O, Q, K, and V tensors into 4D vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>

  <span class="c1">//Format QK_t Tensor into a 2D vector.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">QK_t</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QK_tTensor</span><span class="p">);</span>

  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

      <span class="c1">//</span>
      <span class="c1">// correct</span>
      <span class="c1">// for(int row_tile_idx=0; row_tile_idx &lt; (N+TILE_SIZE-1)/TILE_SIZE; row_tile_idx++) {</span>
      <span class="c1">//   for(int col_tile_idx=0; col_tile_idx &lt; (N+TILE_SIZE-1)/TILE_SIZE; col_tile_idx++) {</span>
      <span class="c1">//     for(int k_tile_idx=0; k_tile_idx &lt; (d+TILE_SIZE-1)/TILE_SIZE; k_tile_idx++  ) {</span>
      <span class="c1">//       for(int tile_row_idx=0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {</span>
      <span class="c1">//         for(int tile_col_idx=0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {</span>
      <span class="c1">//           int row_idx = row_tile_idx * TILE_SIZE + tile_row_idx;</span>
      <span class="c1">//           int col_idx = col_tile_idx * TILE_SIZE + tile_col_idx;</span>
      <span class="c1">//           if(row_idx &gt;= N || col_idx &gt;= N) {</span>
      <span class="c1">//             continue;</span>
      <span class="c1">//           }</span>
      <span class="c1">//           float sum = twoDimRead(QK_t, row_idx, col_idx, N);</span>

      <span class="c1">//           for(int k=0; k &lt; TILE_SIZE; k++) {</span>
      <span class="c1">//             int k_idx = k_tile_idx * TILE_SIZE + k;</span>
      <span class="c1">//             if(k_idx &gt;= d) break;</span>
      <span class="c1">//             float q_val =  fourDimRead(Q,b, h, row_idx, k_idx, H, N, d);</span>
      <span class="c1">//             float k_val = fourDimRead(K, b, h, col_idx, k_idx, H, N, d);</span>
      <span class="c1">//             sum += q_val * k_val;</span>
      <span class="c1">//           }</span>
      <span class="c1">//           twoDimWrite(QK_t, row_idx, col_idx, N, sum);</span>
      <span class="c1">//         }</span>

      <span class="c1">//       }</span>
      <span class="c1">//     }</span>
      <span class="c1">//   }</span>
      <span class="c1">// }</span>

      <span class="c1">// correct with local buffer</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">row_tile_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">TILE_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">row_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">col_tile_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">col_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">TILE_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">col_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k_tile_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="n">TILE_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

            <span class="c1">// Buffers for tile data</span>
            <span class="kt">float</span> <span class="n">Q_tile</span><span class="p">[</span><span class="n">TILE_SIZE</span><span class="p">][</span><span class="n">TILE_SIZE</span><span class="p">];</span>
            <span class="kt">float</span> <span class="n">K_tile</span><span class="p">[</span><span class="n">TILE_SIZE</span><span class="p">][</span><span class="n">TILE_SIZE</span><span class="p">];</span>

            <span class="c1">// Preload Q and K tiles into local buffers</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip out-of-bound rows</span>

                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                    <span class="kt">int</span> <span class="n">k_idx</span> <span class="o">=</span> <span class="n">k_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">k</span><span class="p">;</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">k_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">Q_tile</span><span class="p">[</span><span class="n">tile_row_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                        <span class="n">Q_tile</span><span class="p">[</span><span class="n">tile_row_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">// Fill with zero if out-of-bounds</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>

            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">col_idx</span> <span class="o">=</span> <span class="n">col_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">col_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip out-of-bound columns</span>

                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                    <span class="kt">int</span> <span class="n">k_idx</span> <span class="o">=</span> <span class="n">k_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">k</span><span class="p">;</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">k_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">K_tile</span><span class="p">[</span><span class="n">tile_col_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                        <span class="n">K_tile</span><span class="p">[</span><span class="n">tile_col_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">// Fill with zero if out-of-bounds</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>

            <span class="c1">// Compute the dot product for the current tile</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip out-of-bound rows</span>

                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                    <span class="kt">int</span> <span class="n">col_idx</span> <span class="o">=</span> <span class="n">col_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">col_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip out-of-bound columns</span>

                    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>

                    <span class="c1">// Unrolled loop for vectorized dot product</span>
                    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">sum</span> <span class="o">+=</span> <span class="n">Q_tile</span><span class="p">[</span><span class="n">tile_row_idx</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">K_tile</span><span class="p">[</span><span class="n">tile_col_idx</span><span class="p">][</span><span class="n">k</span><span class="p">];</span>
                    <span class="p">}</span>

                    <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>



      <span class="c1">// also correct</span>
<span class="c1">//       for (int q_row_tile_idx = 0; q_row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; q_row_tile_idx++) {</span>
<span class="c1">//     for (int k_row_tile_idx = 0; k_row_tile_idx &lt; (N + TILE_SIZE - 1) / TILE_SIZE; k_row_tile_idx++) {</span>
<span class="c1">//         for (int d_col_tile_idx = 0; d_col_tile_idx &lt; (d + TILE_SIZE - 1) / TILE_SIZE; d_col_tile_idx++) {</span>
<span class="c1">//             for (int tile_row_idx = 0; tile_row_idx &lt; TILE_SIZE; tile_row_idx++) {</span>
<span class="c1">//                 for (int tile_col_idx = 0; tile_col_idx &lt; TILE_SIZE; tile_col_idx++) {</span>
<span class="c1">//                     int q_row_idx = q_row_tile_idx * TILE_SIZE + tile_row_idx;</span>
<span class="c1">//                     int k_row_idx = k_row_tile_idx * TILE_SIZE + tile_col_idx; // Fix indexing</span>
<span class="c1">//                     for (int d_idx = d_col_tile_idx * TILE_SIZE; d_idx &lt; (d_col_tile_idx + 1) * TILE_SIZE; d_idx++) {</span>
<span class="c1">//                         if (q_row_idx &lt; N &amp;&amp; k_row_idx &lt; N &amp;&amp; d_idx &lt; d) {</span>
<span class="c1">//                             float q_tile_val = fourDimRead(Q, b, h, q_row_idx, d_idx, H, N, d);</span>
<span class="c1">//                             float k_tile_val = fourDimRead(K, b, h, k_row_idx, d_idx, H, N, d);</span>
<span class="c1">//                             float orig_val = twoDimRead(QK_t, q_row_idx, k_row_idx, N);</span>
<span class="c1">//                             float val = q_tile_val * k_tile_val + orig_val;</span>
<span class="c1">//                             twoDimWrite(QK_t, q_row_idx, k_row_idx, N, val);</span>
<span class="c1">//                         }</span>
<span class="c1">//                     }</span>
<span class="c1">//                 }</span>
<span class="c1">//             }</span>
<span class="c1">//         }</span>
<span class="c1">//     }</span>
<span class="c1">// }</span>




      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">);</span>
        <span class="kt">double</span> <span class="n">row_sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span><span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
           <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
          <span class="kt">double</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
          <span class="n">row_sum</span> <span class="o">+=</span> <span class="n">exp_val</span><span class="p">;</span>
          <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp_val</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">cold_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">cold_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">cold_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">tmp_row_res</span><span class="p">[</span><span class="n">cold_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">row_sum</span><span class="p">;</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">cold_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">prob</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>


      
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">qkt_row_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">qkt_row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">qkt_row_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_d_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">output_d_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">output_d_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">out_row_idx</span> <span class="o">=</span> <span class="n">qkt_row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
              <span class="k">if</span><span class="p">(</span><span class="n">out_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
              <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">out_col_idx</span> <span class="o">=</span> <span class="n">output_d_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="k">if</span><span class="p">(</span> <span class="n">out_col_idx</span> <span class="o">&gt;=</span> <span class="n">d</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>

                <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="p">);</span>
                <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">k_idx</span> <span class="o">=</span> <span class="n">k_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">k</span><span class="p">;</span>
                  <span class="kt">float</span> <span class="n">qkt_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="n">sum</span> <span class="o">+=</span> <span class="n">qkt_val</span> <span class="o">*</span> <span class="n">v_val</span><span class="p">;</span> 
                <span class="p">}</span>
                <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
              <span class="p">}</span>
            <span class="p">}</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">}</span>


    <span class="p">}</span>

  <span class="p">}</span>


  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>

</code></pre></div></div> <p>Output:</p> <p>Achieve the same cpu time as ref solution.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Self CPU time total: 215.420ms

REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  214.494ms
mem usage:  4718592 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 20:06:22 3461555:3461555 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-18 20:06:23 3461555:3461555 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 20:06:23 3461555:3461555 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.2183218002319336

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.01%      12.000us         0.01%      12.000us       1.500us       5.00 Mb       5.00 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.38%     217.015ms        99.61%     217.504ms     217.504ms       4.50 Mb      -1.00 Mb             1
                                   aten::zeros         0.01%      17.000us         0.07%     142.000us      71.000us       4.50 Mb           0 b             2
                                   aten::clone         0.01%      19.000us         0.14%     313.000us     156.500us       1.00 Mb           0 b             2
                           aten::empty_strided         0.00%       9.000us         0.00%       9.000us       1.800us     512.25 Kb     512.25 Kb             5
                               model_inference         0.24%     532.000us       100.00%     218.359ms     218.359ms     512.00 Kb      -4.00 Mb             1
                                 aten::flatten         0.01%      25.000us         0.09%     189.000us      37.800us     512.00 Kb           0 b             5
                              aten::empty_like         0.00%       4.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                                      aten::to         0.00%       9.000us         0.01%      30.000us       5.000us         520 b           4 b             6
                                aten::_to_copy         0.01%      14.000us         0.01%      25.000us       6.250us         520 b         260 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 218.359ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  217.504ms
mem usage:  4718592 bytes
</code></pre></div></div> <p>Another issue of code above.</p> <p>It’s not correct when output matrix size is not power of 2</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(myenv) ➜  cs149gpt git:(main) python3 gpt149.py part2 -N 103
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0077, 0.0085, 0.0093, 0.0101, 0.0109, 0.0117, 0.0125, 0.0133, 0.0141,
        0.0149, 0.0157, 0.0165, 0.0173, 0.0181, 0.0189, 0.0197, 0.0205, 0.0213,
        0.0221, 0.0229, 0.0237, 0.0245, 0.0253, 0.0261, 0.0269, 0.0277, 0.0285,
        0.0293, 0.0301, 0.0309, 0.0317, 0.0325])
STAGE:2024-11-18 23:12:28 271968:271968 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0077, 0.0086, 0.0094, 0.0103, 0.0112, 0.0121, 0.0129, 0.0138, 0.0147,
        0.0155, 0.0164, 0.0173, 0.0181, 0.0190, 0.0199, 0.0208, 0.0216, 0.0225,
        0.0234, 0.0242, 0.0251, 0.0260, 0.0268, 0.0277, 0.0286, 0.0295, 0.0303,
        0.0312, 0.0321, 0.0329, 0.0338, 0.0347])
STAGE:2024-11-18 23:12:28 271968:271968 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-18 23:12:28 271968:271968 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 313, in main
    part2Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 233, in part2Test
    testTemplate(attentionModuleReference.myUnfusedAttentionBlocked, params, "STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Found the root cause. It’s because there is out of bound check for softmax(QK_t) * V</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">qkt_row_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">qkt_row_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">qkt_row_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">output_d_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">output_d_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">output_d_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_tile_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_tile_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">TILE_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k_tile_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_row_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">out_row_idx</span> <span class="o">=</span> <span class="n">qkt_row_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_row_idx</span><span class="p">;</span>
              <span class="k">if</span><span class="p">(</span><span class="n">out_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
              <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">tile_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">tile_col_idx</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">tile_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">int</span> <span class="n">out_col_idx</span> <span class="o">=</span> <span class="n">output_d_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tile_col_idx</span><span class="p">;</span>
                <span class="k">if</span><span class="p">(</span> <span class="n">out_col_idx</span> <span class="o">&gt;=</span> <span class="n">d</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>

                <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="p">);</span>
                <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="kt">int</span> <span class="n">k_idx</span> <span class="o">=</span> <span class="n">k_tile_idx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">k</span><span class="p">;</span>
                <span class="c1">// Needto add this out of bound check</span>
                  <span class="k">if</span><span class="p">(</span><span class="n">k_idx</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">break</span><span class="p">;</span>
                  <span class="kt">float</span> <span class="n">qkt_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">QK_t</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
                  <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="n">sum</span> <span class="o">+=</span> <span class="n">qkt_val</span> <span class="o">*</span> <span class="n">v_val</span><span class="p">;</span> 
                <span class="p">}</span>
                <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">out_row_idx</span><span class="p">,</span> <span class="n">out_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
              <span class="p">}</span>
            <span class="p">}</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">}</span>
</code></pre></div></div> <p>Now the result is correct when N is not power of 2. The cpu time is higher than aht of ref solution. I think this is because I add this out of bound check ?</p> <p>Output:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>myenv<span class="o">)</span> ➜  cs149gpt git:<span class="o">(</span>main<span class="o">)</span> ✗ python3 gpt149.py part2 <span class="nt">-N</span> 1023
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Self CPU time total: 223.154ms

REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  222.105ms
mem usage:  4709892 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0770, 0.0778, 0.0786, 0.0794, 0.0802, 0.0810, 0.0818, 0.0826, 0.0834,
        0.0842, 0.0850, 0.0858, 0.0866, 0.0874, 0.0882, 0.0890, 0.0898, 0.0906,
        0.0914, 0.0922, 0.0930, 0.0938, 0.0946, 0.0954, 0.0962, 0.0970, 0.0978,
        0.0986, 0.0994, 0.1002, 0.1010, 0.1018])
STAGE:2024-11-19 10:48:32 4016683:4016683 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0770, 0.0778, 0.0786, 0.0794, 0.0802, 0.0810, 0.0818, 0.0826, 0.0834,
        0.0842, 0.0850, 0.0858, 0.0866, 0.0874, 0.0882, 0.0890, 0.0898, 0.0906,
        0.0914, 0.0922, 0.0930, 0.0938, 0.0946, 0.0954, 0.0962, 0.0970, 0.0978,
        0.0986, 0.0994, 0.1002, 0.1010, 0.1018])
STAGE:2024-11-19 10:48:32 4016683:4016683 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-19 10:48:32 4016683:4016683 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.23152732849121094

----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   aten::empty         0.01%      13.000us         0.01%      13.000us       1.625us       4.99 Mb       4.99 Mb             8
    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.41%     230.208ms        99.62%     230.695ms     230.695ms       4.49 Mb   -1023.00 Kb             1
                                   aten::zeros         0.01%      32.000us         0.06%     145.000us      72.500us       4.49 Mb           0 b             2
                                   aten::clone         0.01%      18.000us         0.14%     318.000us     159.000us    1023.00 Kb           0 b             2
                           aten::empty_strided         0.00%       9.000us         0.00%       9.000us       1.800us     511.75 Kb     511.75 Kb             5
                               model_inference         0.24%     546.000us       100.00%     231.569ms     231.569ms     511.50 Kb      -3.99 Mb             1
                                 aten::flatten         0.01%      16.000us         0.07%     170.000us      34.000us     511.50 Kb           0 b             5
                              aten::empty_like         0.00%       4.000us         0.00%       6.000us       6.000us     511.50 Kb           0 b             1
                                      aten::to         0.00%       6.000us         0.01%      30.000us       5.000us         552 b          32 b             6
                                aten::_to_copy         0.01%      15.000us         0.01%      24.000us       6.000us         520 b         260 b             4
----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 231.569ms

STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics
cpu time:  230.695ms
mem usage:  4709892 bytes
</code></pre></div></div> <h2 id="part3-fused-attention">Part3: Fused attention</h2> <p>Problematic implementation: Output is all zero. Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myFusedAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">temp</span><span class="p">,</span>
              <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>

  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d)</span>
  <span class="c1">//and O Row Tensor with Shape (N)</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">ORowTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">N</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

  <span class="c1">//Format Y, Q, K, and V tensors into 4D vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>
  
  <span class="c1">//Format ORow Tensor into a 1D vector</span>
  <span class="c1">// You can simply access this as ORow[i]</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">ORow</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">ORowTensor</span><span class="p">);</span>


  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="c1">// We give you a template of the first three loops for your convenience</span>
  <span class="c1">//loop over batch</span>

  <span class="cp">#pragma omp parallel for collapse(3)
</span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">){</span>
    <span class="c1">//loop over heads</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">){</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">q_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">q_row_idx</span><span class="o">++</span><span class="p">){</span>

  <span class="c1">// YRow is moved inside so each OpenMP thread gets a local copy.</span>
            <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">ORowTensor</span> <span class="o">=</span> <span class="n">temp</span><span class="p">.</span><span class="n">index</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">Slice</span><span class="p">(</span><span class="n">omp_get_thread_num</span><span class="p">(),</span> <span class="n">torch</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">None</span><span class="p">)});</span>      
            <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">ORow</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">ORowTensor</span><span class="p">);</span>
  <span class="c1">//YOUR CODE HERE</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">k_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span> <span class="p">;</span><span class="n">d_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">q_row</span> <span class="o">=</span> <span class="n">q_row_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">k_row_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="p">;</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">k_row_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>

        <span class="p">}</span>
        <span class="c1">// softmax</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">);</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">;</span>
          <span class="n">sum</span> <span class="o">+=</span> <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
          <span class="c1">// tmp_row_res[i] = exp_val;</span>
        <span class="p">}</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">/</span>  <span class="n">sum</span><span class="p">;</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">prob</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">v_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">v_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span> <span class="p">,</span><span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">v_val</span> <span class="o">*</span> <span class="n">ORow</span><span class="p">[</span><span class="n">v_row_idx</span><span class="p">];</span>
          <span class="p">}</span>
          <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"vcold_idx"</span> <span class="o">&lt;&lt;</span> <span class="n">v_col_idx</span> <span class="o">&lt;&lt;</span> <span class="s">"val: "</span> <span class="o">&lt;&lt;</span> <span class="n">sum</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
          <span class="c1">// tmp_row_res[v_col_idx] = sum;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
        <span class="p">}</span>
          
        
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
    

  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE - FUSED ATTENTION statistics
cpu time:  56.506ms
mem usage:  557056 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 11:53:42 171611:171611 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])
STAGE:2024-11-19 11:53:42 171611:171611 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-19 11:53:42 171611:171611 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 315, in main
    part3Test(N, d, B, H)
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 245, in part3Test
    testTemplate(attentionModuleReference.myFusedAttention, params, "STUDENT - FUSED ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Found the root cause . It’s becaue of wrong type definition.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">v_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">v_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// This is the correct definition</span>
            <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span> <span class="p">,</span><span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">v_val</span> <span class="o">*</span> <span class="n">ORow</span><span class="p">[</span><span class="n">v_row_idx</span><span class="p">];</span>
          <span class="p">}</span>
          <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"vcold_idx"</span> <span class="o">&lt;&lt;</span> <span class="n">v_col_idx</span> <span class="o">&lt;&lt;</span> <span class="s">"val: "</span> <span class="o">&lt;&lt;</span> <span class="n">sum</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
        <span class="p">}</span>
</code></pre></div></div> <p>Output:</p> <p>It’s too slow. I don’t know why</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE - FUSED ATTENTION statistics
cpu time:  59.932ms
mem usage:  557056 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 13:07:43 578930:578930 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 13:07:43 578930:578930 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-19 13:07:43 578930:578930 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.3012988567352295

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::empty         0.01%      17.000us         0.01%      17.000us       1.889us       1.04 Mb       1.04 Mb             9
                  aten::clone         0.00%      15.000us         0.08%     247.000us     123.500us       1.00 Mb           0 b             2
                  aten::zeros         0.01%      19.000us         0.02%      65.000us      21.667us     548.00 Kb           0 b             3
    STUDENT - FUSED ATTENTION        95.37%     287.422ms        99.68%     300.382ms     300.382ms     544.00 Kb      -1.00 Mb             1
          aten::empty_strided         0.00%       9.000us         0.00%       9.000us       1.800us     512.00 Kb     512.00 Kb             5
              model_inference         0.21%     636.000us       100.00%     301.361ms     301.361ms     512.00 Kb     -32.63 Kb             1
                aten::flatten         1.42%       4.278ms         1.48%       4.467ms       1.089us     512.00 Kb           0 b          4101
             aten::empty_like         0.00%       3.000us         0.00%       5.000us       5.000us     512.00 Kb           0 b             1
                     aten::to         0.00%       6.000us         0.01%      32.000us       5.333us         520 b           0 b             6
               aten::_to_copy         0.01%      18.000us         0.01%      26.000us       6.500us         520 b         516 b             4
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 301.361ms

STUDENT - FUSED ATTENTION statistics
cpu time:  300.382ms
mem usage:  557056 bytes
</code></pre></div></div> <p>Found the root cause. I don’t enable openmp in original code</p> <p>The speed is comparable after enabling openmp Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myFusedAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">temp</span><span class="p">,</span>
              <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">){</span>

  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d)</span>
  <span class="c1">//and O Row Tensor with Shape (N)</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">ORowTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">N</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>

  <span class="c1">//Format Y, Q, K, and V tensors into 4D vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>
  
  <span class="c1">//Format ORow Tensor into a 1D vector</span>
  <span class="c1">// You can simply access this as ORow[i]</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">ORow</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">ORowTensor</span><span class="p">);</span>


  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="c1">// We give you a template of the first three loops for your convenience</span>
  <span class="c1">//loop over batch</span>

<span class="c1">// Need to enable this openmp for parallel execution for all rows in batches and heads.</span>
  <span class="cp">#pragma omp parallel for collapse(3)
</span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">){</span>
    <span class="c1">//loop over heads</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">){</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">q_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="p">;</span> <span class="n">q_row_idx</span><span class="o">++</span><span class="p">){</span>

  <span class="c1">// YRow is moved inside so each OpenMP thread gets a local copy.</span>
            <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">ORowTensor</span> <span class="o">=</span> <span class="n">temp</span><span class="p">.</span><span class="n">index</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">Slice</span><span class="p">(</span><span class="n">omp_get_thread_num</span><span class="p">(),</span> <span class="n">torch</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">None</span><span class="p">)});</span>      
            <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">ORow</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">ORowTensor</span><span class="p">);</span>
  <span class="c1">//YOUR CODE HERE</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">k_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span> <span class="p">;</span><span class="n">d_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">q_row</span> <span class="o">=</span> <span class="n">q_row_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">q_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">k_row</span> <span class="o">=</span> <span class="n">k_row_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">k_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row</span><span class="p">,</span> <span class="n">q_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">k_row</span><span class="p">,</span> <span class="n">k_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="p">;</span>
            <span class="n">val</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">k_row_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
          <span class="c1">// std::cout &lt;&lt; "krowidx: " &lt;&lt; k_row_idx &lt;&lt; " val: " &lt;&lt; ORow[k_row_idx] &lt;&lt; std::endl;</span>

        <span class="p">}</span>
        <span class="c1">// softmax</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">tmp_row_res</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">);</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">;</span>
          <span class="n">sum</span> <span class="o">+=</span> <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
          <span class="c1">// tmp_row_res[i] = exp_val;</span>
        <span class="p">}</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">/</span>  <span class="n">sum</span><span class="p">;</span>
          <span class="n">ORow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">prob</span><span class="p">;</span>
          <span class="c1">// std::cout &lt;&lt; "softmax col: "  &lt;&lt; i &lt;&lt; " val: " &lt;&lt; ORow[i] &lt;&lt; std::endl;</span>
        <span class="p">}</span>

        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_col_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_col_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">v_col_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">v_row_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">v_row_idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">v_row_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">v_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span> <span class="p">,</span><span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">v_val</span> <span class="o">*</span> <span class="n">ORow</span><span class="p">[</span><span class="n">v_row_idx</span><span class="p">];</span>
          <span class="p">}</span>
          <span class="c1">// std::cout &lt;&lt; "vcold_idx" &lt;&lt; v_col_idx &lt;&lt; "val: " &lt;&lt; sum &lt;&lt; std::endl;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">v_col_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
        <span class="p">}</span>
          
        
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
    

  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE - FUSED ATTENTION statistics
cpu time:  56.526ms
mem usage:  557056 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 17:17:41 1945272:1945272 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-19 17:17:41 1945272:1945272 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-19 17:17:41 1945272:1945272 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
manual attention == pytorch attention True
Manual Execution Time:  0.04986262321472168

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::empty         0.02%      12.000us         0.02%      12.000us       1.333us       1.04 Mb       1.04 Mb             9
                  aten::clone         0.04%      19.000us         0.42%     208.000us     104.000us       1.00 Mb           0 b             2
                  aten::zeros         0.04%      19.000us         0.14%      68.000us      22.667us     548.00 Kb           0 b             3
    STUDENT - FUSED ATTENTION        94.35%      47.083ms        98.27%      49.043ms      49.043ms     544.00 Kb      -1.00 Mb             1
          aten::empty_strided         0.02%       8.000us         0.02%       8.000us       1.600us     512.00 Kb     512.00 Kb             5
              model_inference         1.11%     555.000us       100.00%      49.904ms      49.904ms     512.00 Kb     -32.63 Kb             1
                aten::flatten         1.10%     549.000us         1.50%     748.000us       1.447us     512.00 Kb           0 b           517
             aten::empty_like         0.01%       3.000us         0.01%       4.000us       4.000us     512.00 Kb           0 b             1
                     aten::to         0.01%       6.000us         0.06%      29.000us       4.833us         520 b           0 b             6
               aten::_to_copy         0.03%      14.000us         0.05%      23.000us       5.750us         520 b         260 b             4
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 49.904ms

STUDENT - FUSED ATTENTION statistics
cpu time:  49.043ms
mem usage:  557056 bytes
</code></pre></div></div> <h2 id="part4-flash-attention">Part4: Flash attention</h2> <p>gpt gives good explanation about how Oi forumala is obtained.</p> <p>Basically previous O_i is divided by l_i . So we need to multiply O_i by l_i when calculating new O_i with new l_i according to this expression.</p> <p>[ O_i = \frac{\sum_{\text{old } j} \alpha_{ij} V_j + \sum_{\text{new } j} \alpha_{ij} V_j}{\sum_{\text{old } j} \alpha_{ij} + \sum_{\text{new } j} \alpha_{ij}}. ]</p> <p>The formula</p> <p>[ O_i = \frac{l_i O_i + P_{ij} V_j}{l_{\text{new}}} ]</p> <p>and its update mechanism for the cumulative sum ( l_i O_i ) and normalization factor ( l_i ) works because of the <strong>linearity of the softmax-weighted sum</strong> and the <strong>incremental construction of softmax normalization</strong>. Let’s derive and explain why this works step-by-step.</p> <hr> <h3 id="1-standard-attention-mechanism">1. <strong>Standard Attention Mechanism</strong> </h3> <p>In the standard attention mechanism, the output ( O_i ) for query ( Q_i ) is defined as:</p> <p>[ O_i = \frac{\sum_{j=1}^N \alpha_{ij} V_j}{\sum_{j=1}^N \alpha_{ij}}, ]</p> <p>where:</p> <ul> <li>( \alpha_{ij} = \exp(Q_i K_j^T) ): the unnormalized attention score between query ( Q_i ) and key ( K_j ).</li> <li>( P_{ij} = \frac{\alpha_{ij}}{\sum_{k=1}^N \alpha_{ik}} ): the normalized attention score (softmax).</li> <li>( V_j ): the value vector associated with key ( K_j ).</li> </ul> <p>Rewriting ( O_i ) explicitly:</p> <p>[ O_i = \frac{\sum_{j=1}^N \alpha_{ij} V_j}{Z_i}, ]</p> <p>where ( Z_i = \sum_{j=1}^N \alpha_{ij} ) is the normalization constant for row ( i ).</p> <hr> <h3 id="2-chunked-computation-in-flash-attention">2. <strong>Chunked Computation in Flash Attention</strong> </h3> <p>To handle memory constraints, Flash Attention processes the attention computation in chunks of ( j ). Suppose we process a chunk of keys/values indexed by ( j ) in each step. Let:</p> <ul> <li>( l_i^{\text{old}} ) be the normalization constant ( Z_i ) for the previously processed chunks.</li> <li>( O_i^{\text{old}} ) be the partial output for the previously processed chunks.</li> </ul> <p>The goal is to update ( O_i ) and ( l_i ) incrementally when a new chunk is processed.</p> <p>For the new chunk, compute:</p> <ul> <li>( \alpha_{ij} ) for the current chunk.</li> <li>The sum of unnormalized scores for the chunk: ( l_{\text{chunk}} = \sum_{j \in \text{chunk}} \alpha_{ij} ).</li> <li>The weighted value contribution: ( O_{\text{chunk}} = \sum_{j \in \text{chunk}} \alpha_{ij} V_j ).</li> </ul> <p>The updated values are:</p> <p>[ l_i^{\text{new}} = l_i^{\text{old}} + l_{\text{chunk}} ] [ O_i^{\text{new}} = \frac{l_i^{\text{old}} O_i^{\text{old}} + O_{\text{chunk}}}{l_i^{\text{new}}}. ]</p> <hr> <h3 id="3-why-this-works-derivation">3. <strong>Why This Works: Derivation</strong> </h3> <h4 id="a-weighted-combination-of-outputs"><strong>a. Weighted Combination of Outputs</strong></h4> <p>The full output ( O_i ) over all chunks can be expressed as a weighted combination of the contributions from the old and new chunks:</p> <p>[ O_i = \frac{\sum_{\text{all } j} \alpha_{ij} V_j}{\sum_{\text{all } j} \alpha_{ij}}. ]</p> <p>Breaking this into previously processed chunks (old) and the current chunk:</p> <p>[ O_i = \frac{\sum_{\text{old } j} \alpha_{ij} V_j + \sum_{\text{new } j} \alpha_{ij} V_j}{\sum_{\text{old } j} \alpha_{ij} + \sum_{\text{new } j} \alpha_{ij}}. ]</p> <p>Let:</p> <ul> <li>( l_i^{\text{old}} = \sum_{\text{old } j} \alpha_{ij} ),</li> <li>( l_{\text{chunk}} = \sum_{\text{new } j} \alpha_{ij} ),</li> <li>( O_i^{\text{old}} = \frac{\sum_{\text{old } j} \alpha_{ij} V_j}{l_i^{\text{old}}} ),</li> <li>( O_{\text{chunk}} = \sum_{\text{new } j} \alpha_{ij} V_j ).</li> </ul> <p>Substituting:</p> <p>[ O_i = \frac{l_i^{\text{old}} O_i^{\text{old}} + O_{\text{chunk}}}{l_i^{\text{old}} + l_{\text{chunk}}}. ]</p> <p>This matches the Flash Attention formula:</p> <p>[ O_i = \frac{l_i O_i + P_{ij} V_j}{l_{\text{new}}}. ]</p> <hr> <h4 id="b-incremental-normalization"><strong>b. Incremental Normalization</strong></h4> <p>The normalization factor ( l_i ) (equivalent to ( Z_i ) in standard attention) is computed incrementally:</p> <p>[ l_i^{\text{new}} = l_i^{\text{old}} + \sum_{j \in \text{chunk}} \alpha_{ij}. ]</p> <p>This ensures that the normalization is consistent across all processed chunks, allowing the final ( O_i ) to represent the output for the full sequence.</p> <hr> <h3 id="4-benefits-of-this-approach">4. <strong>Benefits of This Approach</strong> </h3> <ol> <li> <strong>Memory Efficiency</strong>: <ul> <li>By processing chunks incrementally, there is no need to store the full ( N \times N ) attention matrix or intermediate results.</li> </ul> </li> <li> <strong>Numerical Stability</strong>: <ul> <li>The incremental updates prevent large sums from growing uncontrollably, which could cause overflow/underflow in standard softmax computations.</li> </ul> </li> <li> <strong>Linearity of Weighted Sum</strong>: <ul> <li>The linearity of summation ensures that partial results (( l_i^{\text{old}} O_i^{\text{old}} )) can be combined directly with new contributions without recomputation.</li> </ul> </li> </ol> <hr> <h3 id="5-conclusion">5. <strong>Conclusion</strong> </h3> <p>The Flash Attention update formula works because:</p> <ul> <li>It incrementally combines contributions from each chunk using the linearity of the softmax-weighted sum.</li> <li>The normalization factor ( l_i ) ensures consistency across chunks, so the final result is equivalent to the full softmax computation.</li> <li>This approach trades memory for compute, making it feasible to handle large inputs while maintaining mathematical equivalence to standard attention.</li> </ul> <p>Incorrect implementation:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myFlashAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QiTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KjTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VjTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">SijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">PijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">PVTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OiTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LTensor</span><span class="p">,</span>  <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LiTensor</span><span class="p">,</span> 
	       <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LnewTensor</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Bc</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Br</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
      
  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
  <span class="c1">// Sij, Pij are passed in with Shape: (Br, Bc)</span>
  <span class="c1">// Kj, Vj are passed in with Shape: (Bc, d)</span>
  <span class="c1">// Qi, Oi, and PV  are passed in with Shape: (Br, d)</span>
  <span class="c1">// L in passed in with Shape: (N)</span>
  <span class="c1">// Li, Lij, and Lnew are passed in with shape (Br)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d)</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
 
  <span class="c1">//Format All Tensors into Vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Sij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">SijTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Pij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">PijTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Kj</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KjTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Vj</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VjTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Qi</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QiTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Oi</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OiTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">l</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">PV</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">PVTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">li</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LiTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LijTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lnew</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LnewTensor</span><span class="p">);</span>

  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_block_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_block_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">Bc</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">Bc</span><span class="p">;</span> <span class="n">k_block_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// load Kj, Vj into local memory blocks.</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">j_row</span> <span class="o">=</span> <span class="n">k_block_idx</span> <span class="o">*</span> <span class="n">Bc</span> <span class="o">+</span> <span class="n">j</span><span class="p">;</span>
      <span class="k">if</span><span class="p">(</span><span class="n">j_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">j_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
        <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">j_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
        <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Kj</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">k_val</span><span class="p">);</span>
        <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Vj</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">v_val</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">q_block_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">q_block_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">Br</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">Br</span><span class="p">;</span> <span class="n">q_block_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">// load Qi, Oi, li into local memory blocks</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span> 
        <span class="k">if</span><span class="p">(</span><span class="n">q_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span> <span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">o_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span> <span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Qi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">q_val</span><span class="p">);</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">o_val</span><span class="p">);</span>

        <span class="p">}</span>
        <span class="kt">float</span> <span class="n">l_val</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="n">q_row_idx</span><span class="p">];</span>
        <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">l_val</span><span class="p">;</span>

      <span class="p">}</span>

      <span class="c1">// compute Sij  = Qi * Kj_T (Br x Bc) </span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Qi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Kj</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>

          <span class="p">}</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Compute Pij = exp(Sij) of size (Br x Bc)</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">twoDimRead</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">));</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">exp_val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Compute lij = rowsum(Pij) of size (Br)</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>

      <span class="p">}</span>

      <span class="c1">// compute lnew = li + lij</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>
      <span class="p">}</span>

      <span class="c1">// Compute Oi = (liOi + PijVj)/ lnew</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">pv_sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">p_row</span> <span class="o">=</span> <span class="n">br_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">p_col</span> <span class="o">=</span> <span class="n">bc_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_row</span> <span class="o">=</span> <span class="n">bc_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="n">pv_sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">p_row</span><span class="p">,</span> <span class="n">p_col</span><span class="p">,</span> <span class="n">Bc</span><span class="p">)</span> <span class="o">*</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Vj</span><span class="p">,</span> <span class="n">v_row</span><span class="p">,</span> <span class="n">v_col</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>

          <span class="p">}</span>

          <span class="kt">float</span> <span class="n">li_Oi_val</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">*</span> <span class="nf">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">new_sum</span> <span class="o">=</span> <span class="n">pv_sum</span> <span class="o">+</span> <span class="n">li_Oi_val</span><span class="p">;</span>
          <span class="kt">float</span> <span class="n">new_Oi_val</span> <span class="o">=</span> <span class="n">new_sum</span> <span class="o">/</span> <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">new_Oi_val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Write Oi and lnew back to O and l in main memory</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">O_row</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span>
        <span class="k">if</span><span class="p">(</span><span class="n">O_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">break</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">Oi_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">int</span> <span class="n">O_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">O_row</span><span class="p">,</span> <span class="n">O_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">Oi_val</span><span class="p">);</span>

        <span class="p">}</span>

        <span class="n">l</span><span class="p">[</span><span class="n">O_row</span><span class="p">]</span> <span class="o">=</span> <span class="n">lnew</span><span class="p">[</span><span class="n">O_row</span><span class="p">];</span>

      <span class="p">}</span>


    <span class="p">}</span>
  <span class="p">}</span>

    <span class="p">}</span>

  <span class="p">}</span>


  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Self CPU time total: 589.890ms

REFERENCE - FLASH ATTENTION statistics
cpu time:  588.591ms
mem usage:  524288 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([0.0771, 0.0779, 0.0787, 0.0795, 0.0803, 0.0811, 0.0819, 0.0827, 0.0835,
        0.0843, 0.0851, 0.0859, 0.0867, 0.0875, 0.0883, 0.0891, 0.0899, 0.0907,
        0.0915, 0.0923, 0.0931, 0.0939, 0.0947, 0.0955, 0.0963, 0.0971, 0.0979,
        0.0987, 0.0995, 0.1003, 0.1011, 0.1019])
STAGE:2024-11-20 14:25:23 442505:442505 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
student first row:
 tensor([0.0773, 0.0781, 0.0789, 0.0797, 0.0805, 0.0813, 0.0821, 0.0829, 0.0837,
        0.0845, 0.0853, 0.0861, 0.0869, 0.0877, 0.0885, 0.0893, 0.0901, 0.0909,
        0.0917, 0.0925, 0.0933, 0.0941, 0.0949, 0.0957, 0.0965, 0.0973, 0.0981,
        0.0989, 0.0997, 0.1005, 0.1013, 0.1021])
STAGE:2024-11-20 14:25:23 442505:442505 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-11-20 14:25:23 442505:442505 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 329, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 317, in main
    part4Test(N, d, B, H, int(args.bc), int(args.br))
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 257, in part4Test
    testTemplate(attentionModuleReference.myFlashAttention, params, "STUDENT - FLASH ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 182, in testTemplate
    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Asked gpt to find bug in my previous code</p> <p>Some are corret , some are not.</p> <p>There are several potential bugs and inefficiencies in the provided implementation of Flash Attention. Below is an analysis of common issues based on the code structure.</p> <hr> <h3 id="1-bug-in-writing-lnew-to-global-memory">1. <strong>Bug in Writing <code class="language-plaintext highlighter-rouge">lnew</code> to Global Memory</strong> </h3> <p>At the end of the block:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span><span class="p">[</span><span class="n">O_row</span><span class="p">]</span> <span class="o">=</span> <span class="n">lnew</span><span class="p">[</span><span class="n">O_row</span><span class="p">];</span>
</code></pre></div></div> <p>This line is problematic because <code class="language-plaintext highlighter-rouge">lnew[O_row]</code> is incorrect. You are trying to write from <code class="language-plaintext highlighter-rouge">lnew</code> (a local array for the current query block) to the global <code class="language-plaintext highlighter-rouge">l</code>. The index <code class="language-plaintext highlighter-rouge">O_row</code> does not map directly to <code class="language-plaintext highlighter-rouge">lnew</code>, as <code class="language-plaintext highlighter-rouge">lnew</code> uses local indices within a chunk.</p> <p><strong>Fix</strong>:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span><span class="p">[</span><span class="n">O_row</span><span class="p">]</span> <span class="o">=</span> <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">br_idx</code> is the correct index for <code class="language-plaintext highlighter-rouge">lnew</code> in the current query block.</p> <hr> <h3 id="2-mismatch-in-twodimread-and-twodimwrite-for-sij">2. <strong>Mismatch in <code class="language-plaintext highlighter-rouge">twoDimRead</code> and <code class="language-plaintext highlighter-rouge">twoDimWrite</code> for <code class="language-plaintext highlighter-rouge">Sij</code></strong> </h3> <p>In the computation of ( S_{ij} ):</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
</code></pre></div></div> <p>You are using <code class="language-plaintext highlighter-rouge">d</code> as the stride instead of <code class="language-plaintext highlighter-rouge">Bc</code>. <code class="language-plaintext highlighter-rouge">Sij</code> is declared as having a shape <code class="language-plaintext highlighter-rouge">(Br, Bc)</code>, so the stride in the <code class="language-plaintext highlighter-rouge">twoDimWrite</code> call should be <code class="language-plaintext highlighter-rouge">Bc</code>.</p> <p><strong>Fix</strong>:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
</code></pre></div></div> <p>Similarly, update other uses of <code class="language-plaintext highlighter-rouge">Sij</code> to use the correct stride.</p> <hr> <h3 id="3-incorrect-update-of-lioi">3. <strong>Incorrect Update of <code class="language-plaintext highlighter-rouge">liOi</code></strong> </h3> <p>In the line:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">li_Oi_val</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">*</span> <span class="nf">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
</code></pre></div></div> <p>This reads the old <code class="language-plaintext highlighter-rouge">Oi</code> value from <code class="language-plaintext highlighter-rouge">Oi</code>, but <code class="language-plaintext highlighter-rouge">Oi</code> is already being updated in the loop. This introduces unintended feedback into the computation.</p> <p><strong>Fix</strong>: Make a copy of <code class="language-plaintext highlighter-rouge">Oi</code> at the start of each block before updates, and use that copy for the computation:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Oi_copy</span> <span class="o">=</span> <span class="n">Oi</span><span class="p">;</span> <span class="c1">// Create a copy of Oi</span>
<span class="kt">float</span> <span class="n">li_Oi_val</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">*</span> <span class="nf">twoDimRead</span><span class="p">(</span><span class="n">Oi_copy</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
</code></pre></div></div> <hr> <h3 id="4-improper-memory-initialization">4. <strong>Improper Memory Initialization</strong> </h3> <p>Variables like <code class="language-plaintext highlighter-rouge">Kj</code>, <code class="language-plaintext highlighter-rouge">Vj</code>, <code class="language-plaintext highlighter-rouge">Qi</code>, <code class="language-plaintext highlighter-rouge">Oi</code>, <code class="language-plaintext highlighter-rouge">Sij</code>, etc., are written to without ensuring their contents are initialized correctly. This might cause unexpected behavior if these tensors are not zeroed out.</p> <p><strong>Fix</strong>: Explicitly zero out these tensors before use:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Kj</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Kj</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Vj</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Vj</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Qi</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Qi</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Oi</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Oi</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Sij</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Sij</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
</code></pre></div></div> <hr> <h3 id="5-incorrect-stride-in-twodimread-and-twodimwrite-for-block-matrices">5. <strong>Incorrect Stride in <code class="language-plaintext highlighter-rouge">twoDimRead</code> and <code class="language-plaintext highlighter-rouge">twoDimWrite</code> for Block Matrices</strong> </h3> <p>Some block matrices (e.g., <code class="language-plaintext highlighter-rouge">Pij</code>, <code class="language-plaintext highlighter-rouge">Kj</code>, <code class="language-plaintext highlighter-rouge">Vj</code>, <code class="language-plaintext highlighter-rouge">Qi</code>) use strides inconsistently. Double-check the stride parameter in each <code class="language-plaintext highlighter-rouge">twoDimRead</code> and <code class="language-plaintext highlighter-rouge">twoDimWrite</code> call to match the matrix dimensions.</p> <hr> <h3 id="6-inefficient-computation-of-lij-and-lnew">6. <strong>Inefficient Computation of <code class="language-plaintext highlighter-rouge">lij</code> and <code class="language-plaintext highlighter-rouge">lnew</code></strong> </h3> <p>The computation of <code class="language-plaintext highlighter-rouge">lij</code> (row sum of <code class="language-plaintext highlighter-rouge">Pij</code>) and <code class="language-plaintext highlighter-rouge">lnew</code> involves multiple sequential loops that can be combined for better performance. For example:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
  <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>This eliminates one loop.</p> <hr> <h3 id="7-numerical-stability-in-softmax">7. <strong>Numerical Stability in Softmax</strong> </h3> <p>The computation of ( P_{ij} = \exp(S_{ij}) ) can suffer from numerical instability if ( S_{ij} ) contains large values. This can lead to overflow or inaccurate results.</p> <p><strong>Fix</strong>: Subtract the maximum value in each row of ( S_{ij} ) before applying the exponential function:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">float</span> <span class="n">max_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">std</span><span class="o">::</span><span class="n">numeric_limits</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;::</span><span class="n">infinity</span><span class="p">();</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">sij_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">);</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">sij_val</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">sij_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">)</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">;</span>
    <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">sij_val</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>This ensures the softmax computation is numerically stable.</p> <hr> <h3 id="8-inefficient-loop-nesting">8. <strong>Inefficient Loop Nesting</strong> </h3> <p>The loop order might lead to poor memory access patterns, especially for <code class="language-plaintext highlighter-rouge">Kj</code>, <code class="language-plaintext highlighter-rouge">Vj</code>, and <code class="language-plaintext highlighter-rouge">Qi</code>. Consider transposing these matrices in local memory to improve cache efficiency.</p> <hr> <h3 id="9-potential-misalignment-in-block-indexing">9. <strong>Potential Misalignment in Block Indexing</strong> </h3> <p>When processing blocks of size <code class="language-plaintext highlighter-rouge">Br</code> and <code class="language-plaintext highlighter-rouge">Bc</code>, ensure that the indexing properly handles edge cases where <code class="language-plaintext highlighter-rouge">N</code> is not evenly divisible by <code class="language-plaintext highlighter-rouge">Br</code> or <code class="language-plaintext highlighter-rouge">Bc</code>. Currently, edge handling appears inconsistent.</p> <p><strong>Fix</strong>: Use bounds checking consistently:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">q_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span> <span class="o">||</span> <span class="n">j_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
</code></pre></div></div> <hr> <h3 id="10-output-tensor-not-updated-properly">10. <strong>Output Tensor Not Updated Properly</strong> </h3> <p>The output tensor <code class="language-plaintext highlighter-rouge">OTensor</code> may not be updated properly due to indexing or memory mismanagement.</p> <p><strong>Fix</strong>: Ensure that updates to <code class="language-plaintext highlighter-rouge">O</code> directly correspond to <code class="language-plaintext highlighter-rouge">OTensor</code>:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">O_row</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">O_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">Oi_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
    <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">O_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">Oi_val</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <hr> <h3 id="summary-of-key-fixes">Summary of Key Fixes</h3> <ol> <li>Correct indexing when writing <code class="language-plaintext highlighter-rouge">lnew</code> to <code class="language-plaintext highlighter-rouge">l</code>.</li> <li>Fix strides in <code class="language-plaintext highlighter-rouge">twoDimRead</code> and <code class="language-plaintext highlighter-rouge">twoDimWrite</code> for block matrices.</li> <li>Zero-initialize temporary tensors.</li> <li>Apply numerical stabilization for the softmax computation.</li> <li>Combine loops for <code class="language-plaintext highlighter-rouge">lij</code> and <code class="language-plaintext highlighter-rouge">lnew</code>.</li> <li>Address potential misalignment when <code class="language-plaintext highlighter-rouge">N</code> is not a multiple of block sizes.</li> </ol> <p>These fixes ensure correctness, improve numerical stability, and enhance efficiency.</p> <p>Add initialization of 0.0 to Kj and Vj, to avoid impact of leftover numnber in last iteration</p> <p>I realize this problem from python diff check and found that value is not equal at the last row of matrix.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mismatch at index (0, 3, 1023, 22): QKV=0.13594280183315277, QKS1=0.03398570418357849
Mismatch at index (0, 3, 1023, 23): QKV=0.13674280047416687, QKS1=0.03418571129441261
Mismatch at index (0, 3, 1023, 24): QKV=0.13754287362098694, QKS1=0.03438570350408554
Mismatch at index (0, 3, 1023, 25): QKV=0.13834281265735626, QKS1=0.03458569571375847
Mismatch at index (0, 3, 1023, 26): QKV=0.13914281129837036, QKS1=0.03478570282459259
Mismatch at index (0, 3, 1023, 27): QKV=0.13994282484054565, QKS1=0.034985702484846115
Mismatch at index (0, 3, 1023, 28): QKV=0.14074280858039856, QKS1=0.03518570587038994
Mismatch at index (0, 3, 1023, 29): QKV=0.14154279232025146, QKS1=0.035385698080062866
Mismatch at index (0, 3, 1023, 30): QKV=0.14234280586242676, QKS1=0.03558569401502609
Mismatch at index (0, 3, 1023, 31): QKV=0.14314278960227966, QKS1=0.03578570485115051
Traceback (most recent call last):
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 339, in &lt;module&gt;
    main()
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 327, in main
    part4Test(N, d, B, H, int(args.bc), int(args.br))
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 267, in part4Test
    testTemplate(attentionModuleReference.myFlashAttention, params, "STUDENT - FLASH ATTENTION")
  File "/home/zt/stf-cs149-pp/cs149gpt/gpt149.py", line 190, in testTemplate
    raise AssertionError(correctness_error_message)
AssertionError:
-------------------------------------------
 YOUR ATTENTION PRODUCED INCORRECT RESULTS
</code></pre></div></div> <p>Found that result is incorrect starting from second head.</p> <p>I think it’s becase that some itermediate variable is not cleared before entering next head which leads to incorrect answer.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mismatch at index (0, 1, 0, 0): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 1): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 2): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 3): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 4): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 5): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 6): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 7): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 8): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 9): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 10): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 11): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 12): QKV=1.0, QKS1=0.5
Mismatch at index (0, 1, 0, 13): QKV=1.0, QKS1=0.5
M
</code></pre></div></div> <p>Found the root cause and fix it.</p> <p>I need to clear <code class="language-plaintext highlighter-rouge">l</code> for each head. This is because <code class="language-plaintext highlighter-rouge">l</code> is read to <code class="language-plaintext highlighter-rouge">li</code> for each <code class="language-plaintext highlighter-rouge">Q-</code> block</p> <p>Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">myFlashAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">QiTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">KjTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">VjTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">SijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">PijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">PVTensor</span><span class="p">,</span>
               <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OiTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LTensor</span><span class="p">,</span>  <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LiTensor</span><span class="p">,</span> 
	       <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LijTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">LnewTensor</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Bc</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Br</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">H</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
      
  <span class="c1">// Q, K, V are passed in with Shape: (B, H, N, d)</span>
  <span class="c1">// Sij, Pij are passed in with Shape: (Br, Bc)</span>
  <span class="c1">// Kj, Vj are passed in with Shape: (Bc, d)</span>
  <span class="c1">// Qi, Oi, and PV  are passed in with Shape: (Br, d)</span>
  <span class="c1">// L in passed in with Shape: (N)</span>
  <span class="c1">// Li, Lij, and Lnew are passed in with shape (Br)</span>

  <span class="c1">//Make O Tensor with Shape (B, H, N, d)</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">OTensor</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">at</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
 
  <span class="c1">//Format All Tensors into Vectors</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">O</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">K</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">V</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Sij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">SijTensor</span><span class="p">);</span> <span class="c1">//clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Pij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">PijTensor</span><span class="p">);</span> <span class="c1">//clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Kj</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">KjTensor</span><span class="p">);</span> <span class="c1">// clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Vj</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">VjTensor</span><span class="p">);</span> <span class="c1">// clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Qi</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">QiTensor</span><span class="p">);</span> <span class="c1">// clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">Oi</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">OiTensor</span><span class="p">);</span> <span class="c1">//clear</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">l</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LTensor</span><span class="p">);</span> <span class="c1">// This should be cleared</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">PV</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">PVTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">li</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LiTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lij</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LijTensor</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">lnew</span> <span class="o">=</span> <span class="n">formatTensor</span><span class="p">(</span><span class="n">LnewTensor</span><span class="p">);</span>

  <span class="c1">// std::cout &lt;&lt; "br:" &lt;&lt; Br &lt;&lt; " bc:" &lt;&lt; Bc &lt;&lt;std::endl;</span>
  <span class="c1">// -------- YOUR CODE HERE  -------- //</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>

    <span class="c1">// This line is essential to correctness.</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">l</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">lnew</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">lnew</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">lij</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">lij</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">k_block_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">k_block_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">Bc</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">Bc</span><span class="p">;</span> <span class="n">k_block_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Kj</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Kj</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Vj</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Vj</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="c1">// load Kj, Vj into local memory blocks.</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">j_row</span> <span class="o">=</span> <span class="n">k_block_idx</span> <span class="o">*</span> <span class="n">Bc</span> <span class="o">+</span> <span class="n">j</span><span class="p">;</span>
      <span class="k">if</span><span class="p">(</span><span class="n">j_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">j_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
        <span class="kt">float</span> <span class="n">v_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">j_row</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
        <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Kj</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">k_val</span><span class="p">);</span>
        <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Vj</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">v_val</span><span class="p">);</span>
          <span class="c1">// std::cout&lt;&lt; "j:" &lt;&lt; j_row &lt;&lt; " col:" &lt;&lt; d_idx &lt;&lt; "kj:" &lt;&lt; k_val &lt;&lt; " vj:" &lt;&lt; v_val &lt;&lt; std::endl;</span>
      <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">q_block_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">q_block_idx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">Br</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">Br</span><span class="p">;</span> <span class="n">q_block_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Qi</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Qi</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
      <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Oi</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Oi</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
      <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Sij</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Sij</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
      <span class="n">std</span><span class="o">::</span><span class="n">fill</span><span class="p">(</span><span class="n">Pij</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Pij</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>


      <span class="c1">// load Qi, Oi, li into local memory blocks</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">q_row_idx</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span> 
        <span class="k">if</span><span class="p">(</span><span class="n">q_row_idx</span> <span class="o">&gt;=</span> <span class="n">N</span> <span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">o_val</span> <span class="o">=</span> <span class="n">fourDimRead</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_row_idx</span> <span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Qi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">q_val</span><span class="p">);</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">o_val</span><span class="p">);</span>
            <span class="c1">// std::cout &lt;&lt; "q_row_idx:" &lt;&lt; q_row_idx &lt;&lt; " d_idx:" &lt;&lt; d_idx &lt;&lt; " Qi:" &lt;&lt; q_val &lt;&lt; " Oi:" &lt;&lt; o_val &lt;&lt;std::endl;</span>

        <span class="p">}</span>
        <span class="kt">float</span> <span class="n">l_val</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="n">q_row_idx</span><span class="p">];</span>
        <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">l_val</span><span class="p">;</span>
            <span class="c1">// std::cout &lt;&lt; "li:" &lt;&lt; l_val &lt;&lt; std::endl;</span>

      <span class="p">}</span>

      <span class="c1">// compute Sij  = Qi * Kj_T (Br x Bc) </span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">q_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Qi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="kt">float</span> <span class="n">k_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Kj</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">q_val</span> <span class="o">*</span> <span class="n">k_val</span><span class="p">;</span>

          <span class="p">}</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
              <span class="c1">// std::cout &lt;&lt; "sij, br:" &lt;&lt; br_idx &lt;&lt; " bc:" &lt;&lt; bc_idx &lt;&lt; " val:" &lt;&lt; sum &lt;&lt; std::endl;</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Compute Pij = exp(Sij) of size (Br x Bc)</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">exp_val</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">twoDimRead</span><span class="p">(</span><span class="n">Sij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">));</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">,</span> <span class="n">exp_val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Compute lij = rowsum(Pij) of size (Br)</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">bc_idx</span><span class="p">,</span> <span class="n">Bc</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
        <span class="c1">// compute lnew = li + lij</span>
        <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">lij</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>

      <span class="p">}</span>



      <span class="c1">// Compute Oi = (liOi + PijVj)/ lnew</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">pv_sum</span> <span class="o">=</span><span class="mf">0.0</span><span class="p">;</span>
          <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">bc_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">bc_idx</span> <span class="o">&lt;</span> <span class="n">Bc</span><span class="p">;</span> <span class="n">bc_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">p_row</span> <span class="o">=</span> <span class="n">br_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">p_col</span> <span class="o">=</span> <span class="n">bc_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_row</span> <span class="o">=</span> <span class="n">bc_idx</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">v_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
            <span class="n">pv_sum</span> <span class="o">+=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Pij</span><span class="p">,</span> <span class="n">p_row</span><span class="p">,</span> <span class="n">p_col</span><span class="p">,</span> <span class="n">Bc</span><span class="p">)</span> <span class="o">*</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Vj</span><span class="p">,</span> <span class="n">v_row</span><span class="p">,</span> <span class="n">v_col</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>

          <span class="p">}</span>
          <span class="c1">// twoDimWrite(PV, br_idx, d_idx, d, pv_sum);</span>

          <span class="kt">float</span> <span class="n">li_Oi_val</span> <span class="o">=</span> <span class="n">li</span><span class="p">[</span><span class="n">br_idx</span><span class="p">]</span> <span class="o">*</span> <span class="nf">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
          <span class="kt">float</span> <span class="n">new_sum</span> <span class="o">=</span> <span class="n">pv_sum</span> <span class="o">+</span> <span class="n">li_Oi_val</span><span class="p">;</span>
          <span class="kt">float</span> <span class="n">new_Oi_val</span> <span class="o">=</span> <span class="n">new_sum</span> <span class="o">/</span> <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>
          <span class="n">twoDimWrite</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">new_Oi_val</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>

      <span class="c1">// Write Oi and lnew back to O and l in main memory</span>
      <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">br_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">br_idx</span> <span class="o">&lt;</span> <span class="n">Br</span><span class="p">;</span> <span class="n">br_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">O_row</span> <span class="o">=</span> <span class="n">q_block_idx</span> <span class="o">*</span> <span class="n">Br</span> <span class="o">+</span> <span class="n">br_idx</span><span class="p">;</span>
        <span class="k">if</span><span class="p">(</span><span class="n">O_row</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">d_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">d_idx</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">;</span> <span class="n">d_idx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
          <span class="kt">float</span> <span class="n">Oi_val</span> <span class="o">=</span> <span class="n">twoDimRead</span><span class="p">(</span><span class="n">Oi</span><span class="p">,</span> <span class="n">br_idx</span><span class="p">,</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>
                  <span class="kt">int</span> <span class="n">O_col</span> <span class="o">=</span> <span class="n">d_idx</span><span class="p">;</span>
          <span class="n">fourDimWrite</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">O_row</span><span class="p">,</span> <span class="n">O_col</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">Oi_val</span><span class="p">);</span>

        <span class="p">}</span>

        <span class="n">l</span><span class="p">[</span><span class="n">O_row</span><span class="p">]</span> <span class="o">=</span> <span class="n">lnew</span><span class="p">[</span><span class="n">br_idx</span><span class="p">];</span>

      <span class="p">}</span>


    <span class="p">}</span>
  <span class="p">}</span>

    <span class="p">}</span>

  <span class="p">}</span>


  <span class="c1">// DO NOT EDIT THIS RETURN STATEMENT //</span>
  <span class="c1">// It formats your C++ Vector O back into a Tensor of Shape (B, H, N, d) and returns it //</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">O</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">},</span> <span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)).</span><span class="n">clone</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div> <p>OUtput:</p> <p>My implementation is 3x faster than ref implementation. I don’t know why. Is it because that I don’t materialize <code class="language-plaintext highlighter-rouge">PV</code>?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>               model_inference         0.11%     771.000us       100.00%     721.392ms     721.392ms     512.00 Kb     -10.53 Kb             1
    REFERENCE - FLASH ATTENTION        99.56%     718.217ms        99.83%     720.142ms     720.142ms     512.00 Kb      -8.00 Mb             1
                       aten::to         0.00%       5.000us         0.00%      33.000us       5.500us         520 b           0 b             6
                 aten::_to_copy         0.00%      16.000us         0.00%      28.000us       7.000us         520 b           8 b             4
                      aten::abs         0.00%      30.000us         0.01%      54.000us      13.500us         512 b         256 b             4
            aten::empty_strided         0.00%       3.000us         0.00%       3.000us       0.750us         512 b         512 b             4
                 aten::isfinite         0.00%      12.000us         0.01%      89.000us      89.000us         224 b           0 b             1
            aten::masked_select         0.00%      14.000us         0.01%      43.000us      43.000us         128 b         120 b             1
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 721.392ms

REFERENCE - FLASH ATTENTION statistics
cpu time:  720.142ms
mem usage:  524288 bytes
-----RUNNING STUDENT IMPLEMENTATION-----

first row value:
 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
student first row:
 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
manual attention == pytorch attention True
Manual Execution Time:  0.21289777755737305

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::empty         0.01%      16.000us         0.01%      16.000us       0.889us       1.00 Mb       1.00 Mb            18
                  aten::clone         0.01%      25.000us         0.16%     332.000us     166.000us       1.00 Mb           0 b             2
                  aten::zeros         0.02%      36.000us         0.04%      83.000us       6.917us     521.59 Kb       5.00 Kb            12
          aten::empty_strided         0.00%      10.000us         0.00%      10.000us       2.000us     512.51 Kb     512.51 Kb             5
              model_inference         0.34%     722.000us       100.00%     212.937ms     212.937ms     512.00 Kb     -15.47 Kb             1
    STUDENT - FLASH ATTENTION        99.25%     211.341ms        99.44%     211.746ms     211.746ms     512.00 Kb      -1.00 Mb             1
                aten::flatten         0.01%      26.000us         0.10%     216.000us      14.400us     512.00 Kb           0 b            15
             aten::empty_like         0.00%       4.000us         0.00%       6.000us       6.000us     512.00 Kb           0 b             1
                  aten::zero_         0.00%       5.000us         0.02%      35.000us       2.917us       5.00 Kb       5.00 Kb            12
                     aten::to         0.01%      16.000us         0.01%      29.000us       4.833us         520 b           4 b             6
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 212.937ms

STUDENT - FLASH ATTENTION statistics
cpu time:  211.746ms
mem usage:  524288 bytes
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/learning-based-memory-allocation-for-c-server-workloads-summary/">Learning-based memory allocation for C++ server workloads summary</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/my-question/">my question:</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/binary-search-algorithm-variant/">Binary search algorithm variant</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/docker-rocksdb-build/">Docker Rocksdb build</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/difference-between-dockerfile-and-docker-compose/">Difference between Dockerfile and Docker Compose</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Zhutao Zhuang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 26, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-imperative-programming-vs-declarative-programming",title:"Imperative programming vs. Declarative programming",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/imperative-programming/"}},{id:"post-hash-in-cpp",title:"Hash in cpp",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hash-in-cpp/"}},{id:"post-computer-basics",title:"Computer basics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/some-basic-computer-knowledge/"}},{id:"post-rdma",title:"Rdma",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/rdma/"}},{id:"post-autodiff-implementation-kernel-and-memory-management",title:"Autodiff implementation - kernel and memory management",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign2/"}},{id:"post-topo-sort",title:"Topo sort",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/topo-sort/"}},{id:"post-autodiff-implementation",title:"Autodiff implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign/"}},{id:"post-palindrome-substring-partition",title:"palindrome substring partition",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/palindrome-substring-partition/"}},{id:"post-stf-cs149-flash-attention",title:"Stf CS149 flash attention",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149gpt/"}},{id:"post-speed-up-matrix-multiplication-2",title:"Speed up matrix multiplication 2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-mul-comparison/"}},{id:"post-elf-loading",title:"Elf Loading",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/elf-loading/"}},{id:"post-stf-cs149-parallel-programming-assign3",title:"Stf CS149 Parallel Programming - Assign3",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign3/"}},{id:"post-stf-cs149-parallel-programming-lecture11-cache-coherence",title:"Stf CS149 Parallel Programming - Lecture11 - Cache coherence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture11-cache-coherence/"}},{id:"post-stf-cs149-parallel-programming-lecture-7-cuda-programming-model",title:"Stf CS149 Parallel Programming - Lecture 7 - Cuda programming model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture7-cuda/"}},{id:"post-stf-cs149-parallel-programming-assign2",title:"Stf CS149 Parallel Programming - Assign2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign2/"}},{id:"post-stf-cs149-parallel-programming-lecture-5-amp-6-performance-optimization",title:"Stf CS149 Parallel Programming - Lecture 5&amp;6 - Performance optimization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-lecture-takeaway/"}},{id:"post-stf-cs149-parallel-programming-assign1",title:"Stf CS149 Parallel Programming - Assign1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-assign1/"}},{id:"post-ssh-display-image-on-local-server",title:"ssh display image on local server",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/ssh-display-img/"}},{id:"post-c-compiler-single-letter-local-variable",title:"C compiler - single letter local variable",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-single-letter-ident/"}},{id:"post-c-compiler-parse-example-walkthrough",title:"C compiler - parse example walkthrough",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/statement-and-comparison/"}},{id:"post-linux-get-cpu-time-and-wall-clock-time",title:"Linux get cpu time and wall clock time",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/get-function-cpu-time/"}},{id:"post-simple-c-compiler-unary",title:"Simple c compiler unary",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-unary/"}},{id:"post-simple-c-compiler-gen-expr",title:"Simple c compiler gen expr",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-gen-expr/"}},{id:"post-python-pyplot-trick",title:"Python pyplot trick",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-plot-trick/"}},{id:"post-simple-lru-cache-cpp-implementation",title:"Simple lru cache cpp implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lru-cache/"}},{id:"post-python-capture-function-print-output",title:"Python capture function print output",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/capture-output-python/"}},{id:"post-simple-c-compiler",title:"Simple c compiler",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-compiler/"}},{id:"post-calloc-and-malloc",title:"calloc and malloc",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/calloc/"}},{id:"post-lightgbm-dataset",title:"LightGBM dataset",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-dataset/"}},{id:"post-difference-between-deep-copy-and-shallow-copy-in-python",title:"Difference between deep copy and shallow copy in python",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-deep-copy/"}},{id:"post-python-package-path",title:"Python Package Path",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-package-path/"}},{id:"post-micrograd",title:"micrograd",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/micrograd/"}},{id:"post-cpp-thread-local",title:"cpp thread local",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-thread-local/"}},{id:"post-cpp-async",title:"cpp async",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-async/"}},{id:"post-python-dataframe-drop-row",title:"python dataframe drop row",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/df-drop-row/"}},{id:"post-git-merge-file-from-another-branch",title:"Git merge file from another branch",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/git-merge-file-from-another-branch/"}},{id:"post-efficiency-tips",title:"Efficiency tips",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/efficiency-tips/"}},{id:"post-speed-up-matrix-multiplication",title:"Speed up matrix multiplication",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-multiplication/"}},{id:"post-how-to-write-research-paper",title:"How to write research paper",description:"How to write research paper",section:"Posts",handler:()=>{window.location.href="/blog/2024/how-to-write-research-paper/"}},{id:"post-fast-nano-gpt-training",title:"Fast nano-gpt training",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/gpt-fast/"}},{id:"post-system-for-machine-learning-papers",title:"System for machine learning papers",description:"sysml papers",section:"Posts",handler:()=>{window.location.href="/blog/2024/sysml-papers/"}},{id:"post-nano-gpt-and-transformer",title:"nano-gpt and Transformer",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer/"}},{id:"post-pytorch-tensor-to",title:"pytorch tensor.to",description:"pytorch",section:"Posts",handler:()=>{window.location.href="/blog/2024/pytorch/"}},{id:"post-install-neovim-with-old-glibc",title:"Install neovim with old glibc",description:"vim",section:"Posts",handler:()=>{window.location.href="/blog/2024/install-neovim/"}},{id:"post-llm-c",title:"llm.c",description:"llm minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/llm-c/"}},{id:"post-basic-digital-electronic",title:"Basic digital electronic",description:"transistor",section:"Posts",handler:()=>{window.location.href="/blog/2024/digital-electronic/"}},{id:"post-k8s-advance",title:"K8s Advance",description:"k8s minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/k8s-advance/"}},{id:"post-difference-between-dockerfile-and-docker-compose",title:"Difference between Dockerfile and Docker Compose",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/difference-between-dockerfile-and-docker-compose-d6ebdc687785?source=rss-da1663a42461------2","_blank")}},{id:"post-docker-rocksdb-build",title:"Docker Rocksdb build",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/docker-rocksdb-build-18a0bf0e0bb0?source=rss-da1663a42461------2","_blank")}},{id:"post-k3s-beginner",title:"K3s beginner",description:"k3s",section:"Posts",handler:()=>{window.location.href="/blog/2024/k3s/"}},{id:"post-docker-rocksdb",title:"Docker RocksDB",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-rocksdb/"}},{id:"post-docker-beginner",title:"Docker beginner",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-file-compose-diff/"}},{id:"post-git",title:"Git",description:"git",section:"Posts",handler:()=>{window.location.href="/blog/2024/git/"}},{id:"post-lightgbm-usage-and-implementation",title:"LightGBM usage and implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-usage/"}},{id:"post-backpropogation-c-implementation",title:"Backpropogation C++ Implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/back-propagation/"}},{id:"post-conda-usage",title:"Conda usage",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/conda/"}},{id:"post-install-k8s-cluster-with-3-ubuntu-nodes",title:"Install K8s cluster with 3 ubuntu nodes",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/cloud/"}},{id:"post-convert-svg-figures-to-pdf-latex-before-submitting-to-arxiv",title:"Convert SVG figures to pdf_latex before submitting to arxiv",description:"Convert SVG figures to pdf_latex before submitting to arxiv",section:"Posts",handler:()=>{window.location.href="/blog/2024/arxiv-cleaner/"}},{id:"post-binary-search-algorithm-variant",title:"Binary search algorithm variant",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/binary-search-algorithm-variant-9b5310473471?source=rss-da1663a42461------2","_blank")}},{id:"post-my-question",title:"my question:",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/my-question-a69930f167f0?source=rss-da1663a42461------2","_blank")}},{id:"post-learning-based-memory-allocation-for-c-server-workloads-summary",title:"Learning-based memory allocation for C++ server workloads summary",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/learning-based-memory-allocation-for-c-server-workloads-summary-479e9cd6d6f6?source=rss-da1663a42461------2","_blank")}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-paper-dumpkv-accepted-by-vldb-39-25",title:"A paper(DumpKV) accepted by VLDB&#39;25",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%68%75%61%6E%67%7A%68%75%74%61%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/BilyZ98","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>