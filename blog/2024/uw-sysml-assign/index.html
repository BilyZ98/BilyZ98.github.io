<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Autodiff implementation | Zhutao Zhuang </title> <meta name="author" content="Zhutao Zhuang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bilyz98.github.io/blog/2024/uw-sysml-assign/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhutao</span> Zhuang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Autodiff implementation</h1> <p class="post-meta"> Created in November 29, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/framework"> <i class="fa-solid fa-hashtag fa-sm"></i> framework</a>   ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a>   <a href="/blog/category/framework"> <i class="fa-solid fa-tag fa-sm"></i> framework</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="https://github.com/dlsys-course/assignment1-2018" rel="external nofollow noopener" target="_blank">Source assignment</a></p> <p>Wrong code I wrote:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    for node in reverse_topo_order:
        cur_node_grad = node_to_output_grads_list[node]
        sum_grad = np.sum(cur_node_grad, axis=0)
        node_grad = node.op.gradient(node, sum_grad)
        for input in node.inputs:
            input_grads = node.op.gradient(input, sum_grad)
            if input not in node_to_output_grads_list:
                node_to_output_grads_list[input] = []
            node_to_output_grads_list[input].append(input_grads)


</code></pre></div></div> <p><strong>Understanding the Code and Identifying the Issue</strong></p> <p>The provided code implements a backpropagation algorithm to compute gradients of an output node with respect to a list of input nodes in a computational graph. The core idea is to traverse the graph in reverse topological order, calculating gradients at each node and propagating them backward.</p> <p>However, the current implementation has a potential issue:</p> <ol> <li> <strong>Incorrect Gradient Calculation:</strong> <ul> <li>The line <code class="language-plaintext highlighter-rouge">node_to_output_grad[node] = sum_grad</code> directly assigns the sum of gradients to the node’s gradient. This is incorrect, as it doesn’t account for the specific gradient contribution of each input to the node’s output.</li> <li>The subsequent gradient calculation for input nodes using <code class="language-plaintext highlighter-rouge">node.op.gradient</code> might also be affected by this incorrect sum.</li> </ul> </li> </ol> <p><strong>Corrected Code</strong></p> <p>Here’s a corrected implementation that addresses the issue:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">output_node</span><span class="p">,</span> <span class="n">node_list</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Take gradient of output node with respect to each node in node_list.

    Parameters
    ----------
    output_node: output node that we are taking derivative of.
    node_list: list of nodes that we are taking derivative wrt.

    Returns
    -------
    A list of gradient values, one for each node in node_list respectively.
    </span><span class="sh">"""</span>

    <span class="n">node_to_output_grads_list</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">node_to_output_grad</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Initialize gradient of output node as oneslike_op(output_node)
</span>    <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">output_node</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nf">oneslike_op</span><span class="p">(</span><span class="n">output_node</span><span class="p">)]</span>

    <span class="n">reverse_topo_order</span> <span class="o">=</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">find_topo_sort</span><span class="p">([</span><span class="n">output_node</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">reverse_topo_order</span><span class="p">:</span>
        <span class="n">cur_node_grads</span> <span class="o">=</span> <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
        <span class="n">sum_grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">cur_node_grads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Sum gradients across different output nodes
</span>
        <span class="c1"># Calculate gradient for the current node
</span>        <span class="n">node_grad</span> <span class="o">=</span> <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">sum_grad</span><span class="p">)</span>
        <span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_grad</span>

        <span class="c1"># Propagate gradients to input nodes
</span>        <span class="k">for</span> <span class="n">input_node</span><span class="p">,</span> <span class="n">input_grad</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">node_grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">input_node</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node_to_output_grads_list</span><span class="p">:</span>
                <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">input_node</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">input_node</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">input_grad</span><span class="p">)</span>

    <span class="c1"># Collect results for gradients requested
</span>    <span class="n">grad_node_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_list</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">grad_node_list</span>
</code></pre></div></div> <p><strong>Explanation of the Correction</strong></p> <ol> <li> <strong>Correct Gradient Calculation:</strong> <ul> <li>The <code class="language-plaintext highlighter-rouge">sum_grad</code> is now calculated by summing the gradients across different output nodes, ensuring that the total gradient for the current node is correctly computed.</li> <li>The <code class="language-plaintext highlighter-rouge">node.op.gradient</code> method is used to calculate the gradient of the node with respect to its inputs, considering the current sum of gradients.</li> </ul> </li> <li> <strong>Gradient Propagation:</strong> <ul> <li>The gradients are propagated to input nodes using <code class="language-plaintext highlighter-rouge">zip</code> to iterate over input nodes and their corresponding gradients, ensuring correct association.</li> </ul> </li> </ol> <p>By addressing the incorrect gradient calculation, this revised code provides a more accurate and robust implementation of the backpropagation algorithm.</p> <h2 id="nontype-not-iterable-error">nontype not iterable error</h2> <p>Code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def gradients(output_node, node_list):
    """Take gradient of output node with respect to each node in node_list.

    Parameters
    ----------
    output_node: output node that we are taking derivative of.
    node_list: list of nodes that we are taking derivative wrt.

    Returns
    -------
    A list of gradient values, one for each node in node_list respectively.

    """

    # a map from node to a list of gradient contributions from each output node
    node_to_output_grads_list = {}
    # Special note on initializing gradient of output_node as oneslike_op(output_node):
    # We are really taking a derivative of the scalar reduce_sum(output_node)
    # instead of the vector output_node. But this is the common case for loss function.
    node_to_output_grads_list[output_node] = [oneslike_op(output_node)]
    # a map from node to the gradient of that node
    node_to_output_grad = {}
    # Traverse graph in reverse topological order given the output_node that we are taking gradient wrt.
    reverse_topo_order = reversed(find_topo_sort([output_node]))

    """TODO: Your code here"""
    for node in reverse_topo_order:
        cur_node_grad = node_to_output_grads_list[node]
        sum_grad = np.sum(cur_node_grad, axis=0)
        # gradient for each input node with gradient from current node
        node_grad = node.op.gradient(node, sum_grad)
        node_to_output_grad[node] = node_grad

        if node.inputs is not None:
            print('type node.inputs', type(node.inputs), 'node name', node.name,  'type node_grad', type(node_grad))
            for input_node, input_grad in zip(node.inputs, node_grad):
                if input_node not in node_to_output_grads_list:
                    node_to_output_grads_list[input_node] = []
                # This is wrong
                node_to_output_grads_list[input_node] = input_grad

        # node_grad = node.op.gradient(node, sum_grad)
        # for input in node.inputs:
        #     input_grads = node.op.gradient(input, sum_grad)
        #     if input not in node_to_output_grads_list:
        #         node_to_output_grads_list[input] = []
        #     node_to_output_grads_list[input].append(input_grads)

        # for i in range(len(node.inputs)):
        #     input_node = node.inputs[i]
        #     if input_node not in node_to_output_grads_list:
        #         node_to_output_grads_list[input_node] = []
        #     input_node_grad = node_grad[i]
        #     node_to_output_grads_list[input_node].append(input_node_grad)

    # Collect results for gradients requested.
    grad_node_list = [node_to_output_grad[node] for node in node_list]
    return grad_node_list


</code></pre></div></div> <p>Error:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>======================================================================                                               ERROR: autodiff_test.test_add_mul_mix_1                                                                              ----------------------------------------------------------------------                                               Traceback (most recent call last):                                                                                     File "/home/zt/miniconda3/lib/python3.12/site-packages/nose/case.py", line 189, in runTest                             self.test(*self.arg)                                                                                               File "/mnt/nvme1n1/zt/assignment1-2018/autodiff_test.py", line 86, in test_add_mul_mix_1                               grad_x1, grad_x2, grad_x3 = ad.gradients(y, [x1, x2, x3])                                                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                          File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 353, in gradients                                            for input_node, input_grad in zip(node.inputs, node_grad):                                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                        TypeError: 'NoneType' object is not iterable                                                                         -------------------- &gt;&gt; begin captured stdout &lt;&lt; ---------------------
type node.inputs &lt;class 'list'&gt; node name (x1+((x2*x3)*x1)) type node_grad &lt;class 'list'&gt;
type node.inputs &lt;class 'list'&gt; node name ((x2*x3)*x1) type node_grad &lt;class 'list'&gt;
type node.inputs &lt;class 'list'&gt; node name (x2*x3) type node_grad &lt;class 'list'&gt;
type node.inputs &lt;class 'list'&gt; node name x3 type node_grad &lt;class 'NoneType'&gt;
                                                                                                                     --------------------- &gt;&gt; end captured stdout &lt;&lt; ----------------------
-------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------
--------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------

</code></pre></div></div> <p>I think it’s because I don’t add placeholder op to input x value.</p> <p>Get another error</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ERROR: autodiff_test.test_add_by_const                                                                               ----------------------------------------------------------------------                                               Traceback (most recent call last):                                                                                     File "/home/zt/miniconda3/lib/python3.12/site-packages/nose/case.py", line 189, in runTest                             self.test(*self.arg)                                                                                               File "/mnt/nvme1n1/zt/assignment1-2018/autodiff_test.py", line 26, in test_add_by_const                                y_val, grad_x2_val= executor.run(feed_dict = {x2 : x2_val})                                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                        File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 306, in run                                                  topo_order = find_topo_sort(self.eval_node_list)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                   File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 392, in find_topo_sort                                       topo_sort_dfs(node, visited, topo_order)                                                                           File "/mnt/nvme1n1/zt/assignment1-2018/autodiff.py", line 400, in topo_sort_dfs                                        for n in node.inputs:                                                                                                         ^^^^^^^^^^^                                                                                             AttributeError: 'NoneType' object has no attribute 'inputs'                                                          -------------------- &gt;&gt; begin captured stdout &lt;&lt; ---------------------                                               type node.inputs &lt;class 'list'&gt; node name (x2+5) type node_grad &lt;class 'list'&gt;                                                                                                                                                            --------------------- &gt;&gt; end captured stdout &lt;&lt; ----------------------                                               -------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------                                               --------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------
</code></pre></div></div> <p>Correct code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">output_node</span><span class="p">,</span> <span class="n">node_list</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Take gradient of output node with respect to each node in node_list.

    Parameters
    ----------
    output_node: output node that we are taking derivative of.
    node_list: list of nodes that we are taking derivative wrt.

    Returns
    -------
    A list of gradient values, one for each node in node_list respectively.

    </span><span class="sh">"""</span>

    <span class="c1"># a map from node to a list of gradient contributions from each output node
</span>    <span class="n">node_to_output_grads_list</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Special note on initializing gradient of output_node as oneslike_op(output_node):
</span>    <span class="c1"># We are really taking a derivative of the scalar reduce_sum(output_node)
</span>    <span class="c1"># instead of the vector output_node. But this is the common case for loss function.
</span>    <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">output_node</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nf">oneslike_op</span><span class="p">(</span><span class="n">output_node</span><span class="p">)]</span>
    <span class="c1"># a map from node to the gradient of that node
</span>    <span class="n">node_to_output_grad</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Traverse graph in reverse topological order given the output_node that we are taking gradient wrt.
</span>    <span class="n">reverse_topo_order</span> <span class="o">=</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">find_topo_sort</span><span class="p">([</span><span class="n">output_node</span><span class="p">]))</span>

    <span class="sh">"""</span><span class="s">TODO: Your code here</span><span class="sh">"""</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">reverse_topo_order</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_to_output_grads_list</span><span class="p">:</span>
            <span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="nf">sum_node_list</span><span class="p">(</span><span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
        <span class="c1"># cur_node_grad = node_to_output_grads_list[node]
</span>        <span class="c1"># sum_grad = np.sum(cur_node_grad, axis=0)
</span>        <span class="c1"># gradient for each input node with gradient from current node
</span>        <span class="c1"># node_grad = node.op.gradient(node, sum_grad)
</span>        <span class="c1"># node_to_output_grad[node] = node_grad
</span>
        <span class="c1"># if node_grad is not None:
</span>        <span class="c1">#     print('type node.inputs', type(node.inputs), 'node name', node.name,  'type node_grad', type(node_grad))
</span>        <span class="c1">#     for input_node, input_grad in zip(node.inputs, node_grad):
</span>        <span class="c1">#         if input_node not in node_to_output_grads_list:
</span>        <span class="c1">#             node_to_output_grads_list[input_node] = []
</span>        <span class="c1">#         node_to_output_grads_list[input_node] = input_grad
</span>            <span class="k">for</span> <span class="nb">id</span> <span class="p">,</span> <span class="n">in_nodes</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">inputs</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">in_nodes</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node_to_output_grads_list</span><span class="p">:</span>
                    <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">in_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">node_to_output_grads_list</span><span class="p">[</span><span class="n">in_nodes</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="nb">id</span><span class="p">])</span>

        <span class="c1"># node_grad = node.op.gradient(node, sum_grad)
</span>        <span class="c1"># for input in node.inputs:
</span>        <span class="c1">#     input_grads = node.op.gradient(input, sum_grad)
</span>        <span class="c1">#     if input not in node_to_output_grads_list:
</span>        <span class="c1">#         node_to_output_grads_list[input] = []
</span>        <span class="c1">#     node_to_output_grads_list[input].append(input_grads)
</span>
        <span class="c1"># for i in range(len(node.inputs)):
</span>        <span class="c1">#     input_node = node.inputs[i]
</span>        <span class="c1">#     if input_node not in node_to_output_grads_list:
</span>        <span class="c1">#         node_to_output_grads_list[input_node] = []
</span>        <span class="c1">#     input_node_grad = node_grad[i]
</span>        <span class="c1">#     node_to_output_grads_list[input_node].append(input_node_grad)
</span>
    <span class="c1"># Collect results for gradients requested.
</span>    <span class="n">grad_node_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_to_output_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_list</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">grad_node_list</span>


</code></pre></div></div> <p>I don’t know why this is the correct code.</p> <p>What is the difference between my previous code and this code ?</p> <p>This is very different from pytorch like auto gradient calculation.</p> <p><a href="https://dlsys.cs.washington.edu/pdf/lecture4.pdf" rel="external nofollow noopener" target="_blank">Autodiff algorithm lecture</a></p> <p><strong>Yes, PyTorch leverages automatic differentiation (autodiff) to calculate gradients automatically.</strong> This powerful feature significantly simplifies the process of training neural networks.</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>Computational Graph:</strong> When you perform operations on PyTorch tensors, a computational graph is created. This graph represents the sequence of operations involved in calculating the output.</li> <li> <strong>Forward Pass:</strong> During the forward pass, the input data is fed into the graph, and calculations are performed to obtain the output.</li> <li> <strong>Backward Pass:</strong> In the backward pass, the gradients of the output with respect to the input parameters are computed using the chain rule. PyTorch automatically traverses the computational graph in reverse order, calculating gradients for each node.</li> <li> <strong>Gradient Update:</strong> The calculated gradients are then used to update the model’s parameters using an optimization algorithm like stochastic gradient descent (SGD).</li> </ol> <p><strong>Key Benefits of PyTorch’s Autodiff:</strong></p> <ul> <li> <strong>Simplified Training:</strong> Autodiff eliminates the need for manual gradient calculation, making the training process more efficient.</li> <li> <strong>Complex Models:</strong> It enables the training of complex deep learning models with numerous layers and parameters.</li> <li> <strong>Flexibility:</strong> PyTorch’s dynamic computational graph allows for more flexibility in defining and training neural networks.</li> </ul> <p>By understanding how PyTorch’s autodiff works, you can effectively leverage this powerful tool to build and train sophisticated deep learning models.</p> <p><a href="https://github.com/yzh119/UW-dlsys-assignment1/blob/master/autodiff.py" rel="external nofollow noopener" target="_blank">reference repo</a></p> <p>This test code gives good example about how to use autodiff to do forward and gradient calculation.</p> <p>First we define variable in the graph.</p> <p>We do some calculations with the node in the graph. And then we first call <code class="language-plaintext highlighter-rouge">gradient()</code> to get gradient we are interested to get. And then we call <code class="language-plaintext highlighter-rouge">Excutor()</code> to create a instance of Executor to do forward pass to get the value want.</p> <p>In the constructor arg we give all the variables in graph we are interested to get value of when <code class="language-plaintext highlighter-rouge">run()</code> is called.</p> <p>To actually run and get values we need to call <code class="language-plaintext highlighter-rouge">executor.run()</code> by giving values for all inputs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_matmul_two_vars</span><span class="p">():</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">x3</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x3</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nf">matmul_op</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">)</span>

    <span class="n">grad_x2</span><span class="p">,</span> <span class="n">grad_x3</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">])</span>
    
    <span class="n">executor</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">Executor</span><span class="p">([</span><span class="n">y</span><span class="p">,</span> <span class="n">grad_x2</span><span class="p">,</span> <span class="n">grad_x3</span><span class="p">])</span>
    <span class="n">x2_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span> <span class="c1"># 3x2
</span>    <span class="n">x3_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]])</span> <span class="c1"># 2x3
</span>
    <span class="n">y_val</span><span class="p">,</span> <span class="n">grad_x2_val</span><span class="p">,</span> <span class="n">grad_x3_val</span> <span class="o">=</span> <span class="n">executor</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x2</span><span class="p">:</span> <span class="n">x2_val</span><span class="p">,</span> <span class="n">x3</span><span class="p">:</span> <span class="n">x3_val</span><span class="p">})</span>

    <span class="n">expected_yval</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x2_val</span><span class="p">,</span> <span class="n">x3_val</span><span class="p">)</span>
    <span class="n">expected_grad_x2_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">expected_yval</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">x3_val</span><span class="p">))</span>
    <span class="n">expected_grad_x3_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">x2_val</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">expected_yval</span><span class="p">))</span>

    <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ad</span><span class="p">.</span><span class="n">Node</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">expected_yval</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">grad_x2_val</span><span class="p">,</span> <span class="n">expected_grad_x2_val</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">grad_x3_val</span><span class="p">,</span> <span class="n">expected_grad_x3_val</span><span class="p">)</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/learning-based-memory-allocation-for-c-server-workloads-summary/">Learning-based memory allocation for C++ server workloads summary</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/my-question/">my question:</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/binary-search-algorithm-variant/">Binary search algorithm variant</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/docker-rocksdb-build/">Docker Rocksdb build</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/difference-between-dockerfile-and-docker-compose/">Difference between Dockerfile and Docker Compose</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Zhutao Zhuang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 31, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-imperative-programming-vs-declarative-programming",title:"Imperative programming vs. Declarative programming",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/imperative-programming/"}},{id:"post-hash-in-cpp",title:"Hash in cpp",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hash-in-cpp/"}},{id:"post-computer-basics",title:"Computer basics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/some-basic-computer-knowledge/"}},{id:"post-rdma",title:"Rdma",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/rdma/"}},{id:"post-autodiff-implementation-kernel-and-memory-management",title:"Autodiff implementation - kernel and memory management",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign2/"}},{id:"post-topo-sort",title:"Topo sort",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/topo-sort/"}},{id:"post-autodiff-implementation",title:"Autodiff implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign/"}},{id:"post-palindrome-substring-partition",title:"palindrome substring partition",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/palindrome-substring-partition/"}},{id:"post-stf-cs149-flash-attention",title:"Stf CS149 flash attention",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149gpt/"}},{id:"post-speed-up-matrix-multiplication-2",title:"Speed up matrix multiplication 2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-mul-comparison/"}},{id:"post-elf-loading",title:"Elf Loading",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/elf-loading/"}},{id:"post-stf-cs149-parallel-programming-assign3",title:"Stf CS149 Parallel Programming - Assign3",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign3/"}},{id:"post-stf-cs149-parallel-programming-lecture11-cache-coherence",title:"Stf CS149 Parallel Programming - Lecture11 - Cache coherence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture11-cache-coherence/"}},{id:"post-stf-cs149-parallel-programming-lecture-7-cuda-programming-model",title:"Stf CS149 Parallel Programming - Lecture 7 - Cuda programming model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture7-cuda/"}},{id:"post-stf-cs149-parallel-programming-assign2",title:"Stf CS149 Parallel Programming - Assign2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign2/"}},{id:"post-stf-cs149-parallel-programming-lecture-5-amp-6-performance-optimization",title:"Stf CS149 Parallel Programming - Lecture 5&amp;6 - Performance optimization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-lecture-takeaway/"}},{id:"post-stf-cs149-parallel-programming-assign1",title:"Stf CS149 Parallel Programming - Assign1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-assign1/"}},{id:"post-ssh-display-image-on-local-server",title:"ssh display image on local server",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/ssh-display-img/"}},{id:"post-c-compiler-single-letter-local-variable",title:"C compiler - single letter local variable",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-single-letter-ident/"}},{id:"post-c-compiler-parse-example-walkthrough",title:"C compiler - parse example walkthrough",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/statement-and-comparison/"}},{id:"post-linux-get-cpu-time-and-wall-clock-time",title:"Linux get cpu time and wall clock time",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/get-function-cpu-time/"}},{id:"post-simple-c-compiler-unary",title:"Simple c compiler unary",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-unary/"}},{id:"post-simple-c-compiler-gen-expr",title:"Simple c compiler gen expr",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-gen-expr/"}},{id:"post-python-pyplot-trick",title:"Python pyplot trick",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-plot-trick/"}},{id:"post-simple-lru-cache-cpp-implementation",title:"Simple lru cache cpp implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lru-cache/"}},{id:"post-python-capture-function-print-output",title:"Python capture function print output",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/capture-output-python/"}},{id:"post-simple-c-compiler",title:"Simple c compiler",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-compiler/"}},{id:"post-calloc-and-malloc",title:"calloc and malloc",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/calloc/"}},{id:"post-lightgbm-dataset",title:"LightGBM dataset",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-dataset/"}},{id:"post-difference-between-deep-copy-and-shallow-copy-in-python",title:"Difference between deep copy and shallow copy in python",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-deep-copy/"}},{id:"post-python-package-path",title:"Python Package Path",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-package-path/"}},{id:"post-micrograd",title:"micrograd",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/micrograd/"}},{id:"post-cpp-thread-local",title:"cpp thread local",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-thread-local/"}},{id:"post-cpp-async",title:"cpp async",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-async/"}},{id:"post-python-dataframe-drop-row",title:"python dataframe drop row",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/df-drop-row/"}},{id:"post-git-merge-file-from-another-branch",title:"Git merge file from another branch",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/git-merge-file-from-another-branch/"}},{id:"post-efficiency-tips",title:"Efficiency tips",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/efficiency-tips/"}},{id:"post-speed-up-matrix-multiplication",title:"Speed up matrix multiplication",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-multiplication/"}},{id:"post-how-to-write-research-paper",title:"How to write research paper",description:"How to write research paper",section:"Posts",handler:()=>{window.location.href="/blog/2024/how-to-write-research-paper/"}},{id:"post-fast-nano-gpt-training",title:"Fast nano-gpt training",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/gpt-fast/"}},{id:"post-system-for-machine-learning-papers",title:"System for machine learning papers",description:"sysml papers",section:"Posts",handler:()=>{window.location.href="/blog/2024/sysml-papers/"}},{id:"post-nano-gpt-and-transformer",title:"nano-gpt and Transformer",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer/"}},{id:"post-pytorch-tensor-to",title:"pytorch tensor.to",description:"pytorch",section:"Posts",handler:()=>{window.location.href="/blog/2024/pytorch/"}},{id:"post-install-neovim-with-old-glibc",title:"Install neovim with old glibc",description:"vim",section:"Posts",handler:()=>{window.location.href="/blog/2024/install-neovim/"}},{id:"post-llm-c",title:"llm.c",description:"llm minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/llm-c/"}},{id:"post-basic-digital-electronic",title:"Basic digital electronic",description:"transistor",section:"Posts",handler:()=>{window.location.href="/blog/2024/digital-electronic/"}},{id:"post-k8s-advance",title:"K8s Advance",description:"k8s minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/k8s-advance/"}},{id:"post-difference-between-dockerfile-and-docker-compose",title:"Difference between Dockerfile and Docker Compose",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/difference-between-dockerfile-and-docker-compose-d6ebdc687785?source=rss-da1663a42461------2","_blank")}},{id:"post-docker-rocksdb-build",title:"Docker Rocksdb build",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/docker-rocksdb-build-18a0bf0e0bb0?source=rss-da1663a42461------2","_blank")}},{id:"post-k3s-beginner",title:"K3s beginner",description:"k3s",section:"Posts",handler:()=>{window.location.href="/blog/2024/k3s/"}},{id:"post-docker-rocksdb",title:"Docker RocksDB",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-rocksdb/"}},{id:"post-docker-beginner",title:"Docker beginner",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-file-compose-diff/"}},{id:"post-git",title:"Git",description:"git",section:"Posts",handler:()=>{window.location.href="/blog/2024/git/"}},{id:"post-lightgbm-usage-and-implementation",title:"LightGBM usage and implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-usage/"}},{id:"post-backpropogation-c-implementation",title:"Backpropogation C++ Implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/back-propagation/"}},{id:"post-conda-usage",title:"Conda usage",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/conda/"}},{id:"post-install-k8s-cluster-with-3-ubuntu-nodes",title:"Install K8s cluster with 3 ubuntu nodes",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/cloud/"}},{id:"post-convert-svg-figures-to-pdf-latex-before-submitting-to-arxiv",title:"Convert SVG figures to pdf_latex before submitting to arxiv",description:"Convert SVG figures to pdf_latex before submitting to arxiv",section:"Posts",handler:()=>{window.location.href="/blog/2024/arxiv-cleaner/"}},{id:"post-binary-search-algorithm-variant",title:"Binary search algorithm variant",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/binary-search-algorithm-variant-9b5310473471?source=rss-da1663a42461------2","_blank")}},{id:"post-my-question",title:"my question:",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/my-question-a69930f167f0?source=rss-da1663a42461------2","_blank")}},{id:"post-learning-based-memory-allocation-for-c-server-workloads-summary",title:"Learning-based memory allocation for C++ server workloads summary",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/learning-based-memory-allocation-for-c-server-workloads-summary-479e9cd6d6f6?source=rss-da1663a42461------2","_blank")}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-paper-dumpkv-accepted-by-vldb-39-25",title:"A paper(DumpKV) accepted by VLDB&#39;25",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%68%75%61%6E%67%7A%68%75%74%61%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/BilyZ98","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>