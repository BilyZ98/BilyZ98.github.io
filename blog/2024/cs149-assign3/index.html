<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Stf CS149 Parallel Programming - Assign3 | Zhutao Zhuang </title> <meta name="author" content="Zhutao Zhuang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bilyz98.github.io/blog/2024/cs149-assign3/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhutao</span> Zhuang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Stf CS149 Parallel Programming - Assign3</h1> <p class="post-meta"> Created in November 03, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/parallel"> <i class="fa-solid fa-hashtag fa-sm"></i> parallel</a>   <a href="/blog/tag/programming"> <i class="fa-solid fa-hashtag fa-sm"></i> programming</a>   ·   <a href="/blog/category/parallel"> <i class="fa-solid fa-tag fa-sm"></i> parallel</a>   <a href="/blog/category/programming"> <i class="fa-solid fa-tag fa-sm"></i> programming</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="part1">Part1</h2> <p>Code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code></code></pre></div></div> <p>saxpy serial cpu output from assign1 prog5:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(base) ➜  prog5_saxpy git:(master) ✗ ./saxpy
[saxpy serial]:         [20.605] ms     [14.464] GB/s   [1.941] GFLOPS
[saxpy ispc]:           [17.866] ms     [16.681] GB/s   [2.239] GFLOPS
[saxpy task ispc]:      [3.122] ms      [95.446] GB/s   [12.810] GFLOPS                                                                                                                                                           (5.72x speedup from use of tasks)
</code></pre></div></div> <p>saxpy gpu cuda output</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Found 4 CUDA devices
Device 0: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 1: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 2: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 3: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
---------------------------------------------------------
Running 3 timing tests:
Effective BW by CUDA saxpy: 225.263 ms          [4.961 GB/s]
kernel execution time: 1.503ms
Effective BW by CUDA saxpy: 247.816 ms          [4.510 GB/s]
kernel execution time: 1.504ms
Effective BW by CUDA saxpy: 245.998 ms          [4.543 GB/s]
kernel execution time: 1.506ms
</code></pre></div></div> <p>Looks like gpu bandwidth is lower than cpu</p> <p>kernel execution time is super short and all the time is taken for memory copy.</p> <p>I am a little bit confused about the two bandwidths listed in this doc https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-datasheet-951643.pdf</p> <p>gpu memory bandwidth is 300GB/sec and interconnect bandwidth is 32 GB/sec.</p> <p>I guess gpu memory bandwidth is the bandwidth that is used in internal SMs in gpu</p> <p>And interconnect bandwidth is the bandwidth during transfer data between cpu and gp</p> <p>command to run when on A800</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./cudaSaxpy: error while loading shared libraries: libcudart.so.12: cannot open shared object file: No such file or directory

[nsccgz_qylin_1@gpu72%tianhe2-K saxpy]$ echo $LD_LIBRARY_PATH | grep dart
[nsccgz_qylin_1@gpu72%tianhe2-K saxpy]$ export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
[nsccgz_qylin_1@gpu72%tianhe2-K saxpy]$ export LD_LIBRARY_PATH=/usr/local/cuda-12.0/lib64:$LD_LIBRARY_PATH
</code></pre></div></div> <h2 id="part2-parallel-prefix-sum">Part2: parallel prefix sum</h2> <p>Get this libstd lib version issue when running execution binary</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@gpu72%tianhe2-K scan]$ ./cudaScan ./cudaScan: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by ./cudaScan) ./cudaScan: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by ./cudaScan) ./cudaScan: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by ./cudaScan)
</code></pre></div></div> <p>Solution: Use conda to install libstdcxx and include it in <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate myenv
conda install -c conda-forge libstdcxx-ng
find $CONDA_PREFIX -name "libstdc++.so.6"

 export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Found 4 CUDA devices
Device 0: NVIDIA A800 80GB PCIe
   SMs:        108
   Global mem: 81229 MB
   CUDA Cap:   8.0
Device 1: NVIDIA A800 80GB PCIe
   SMs:        108
   Global mem: 81229 MB
   CUDA Cap:   8.0
Device 2: NVIDIA A800 80GB PCIe
   SMs:        108
   Global mem: 81229 MB
   CUDA Cap:   8.0
Device 3: NVIDIA A800 80GB PCIe
   SMs:        108
   Global mem: 81229 MB
   CUDA Cap:   8.0
---------------------------------------------------------
Array size: 64
Student GPU time: 0.069 ms
Scan outputs are correct!
</code></pre></div></div> <h3 id="round-input-length-power-of-2-for-cudascan">Round input length power of 2 for cudaScan</h3> <h3 id="find-repeats">find repeats</h3> <p>I did not know why find repeats could be parallelized and how it can be done until I read code from others.</p> <p>The idea is that the return result of find repeats is the indices of <code class="language-plaintext highlighter-rouge">A[i]==A[i+1]</code></p> <p>This find repeats process can be parallelized with exclusive scan but we first need to generate a intermetidate representation of input arr.</p> <p>So basically we first generate a indices array which runs on cuda that can be parallelized. The output indices array is that <code class="language-plaintext highlighter-rouge">arr[i] = 1 if A[i]==A[i+1] else = 0</code>.</p> <p>This flags array is then passed to cudascan function for parallel exclusive scan to get a new array flags_sum_arr where <code class="language-plaintext highlighter-rouge">flags_sum_arr[i]</code> indicates how many repeated elements are accumulated so far.</p> <p>And then this <code class="language-plaintext highlighter-rouge">flags_sum_arr</code> is passed to another cuda kernel which is also parallelized to generated the final indices array <code class="language-plaintext highlighter-rouge">indices[i]</code>.</p> <p>This is the fun part that shows the core of parallel programming which is that each subtask has not dependency on each other. The output writing is totally independent.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// cuda kernal code</span>
<span class="k">if</span><span class="p">(</span><span class="n">flags_sum_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">flags_sum_arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="p">{</span>
    <span class="c1">// this indicates this is a repeated element in input array</span>
    <span class="c1">// The position of output value we need to write is flags_sum_arr[i]</span>
    <span class="c1">// The repeated element index is i.</span>
    <span class="n">indices</span><span class="p">[</span><span class="n">flags_sum_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span> 
<span class="p">}</span>
</code></pre></div></div> <p>Problem:</p> <p>Input array in cpu and input array in gpu is not the same</p> <p>Why is that?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input arr
0:1 1:1 2:1 3:1 4:1 5:1 6:1 7:1
input arr
0:1 1:1 2:337500088 3:10935 4:0 5:0 6:0 7:0
flags arr
0:1 1:1 2:337500088 3:10935 4:0 5:0 6:0 7:0
</code></pre></div></div> <p>This is because I did not copy all bytes of intput element</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> cudaMemcpy(arr, device_input, length*sizeof(int), cudaMemcpyDeviceToHost);
</code></pre></div></div> <p>and</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
cudaMemcpy(arr, device_input, length, cudaMemcpyDeviceToHost);
</code></pre></div></div> <p>Need to copy length of <code class="language-plaintext highlighter-rouge">length*sizeof(int)</code> instead of <code class="language-plaintext highlighter-rouge">length</code></p> <p>memcpy copies number of bytes specified.</p> <p>Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> 
<span class="nf">flag_repeats_kernel</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">index</span> <span class="o">&lt;</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">==</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="p">{</span>
    <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="p">}</span>


<span class="p">}</span>

<span class="n">__global__</span> <span class="kt">void</span> 
<span class="nf">flags_extract_indices</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">input</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span> <span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">index</span> <span class="o">&lt;</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="p">{</span>
    <span class="n">output</span><span class="p">[</span><span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span> <span class="o">=</span> <span class="n">index</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// find_repeats --</span>
<span class="c1">//</span>
<span class="c1">// Given an array of integers `device_input`, returns an array of all</span>
<span class="c1">// indices `i` for which `device_input[i] == device_input[i+1]`.</span>
<span class="c1">//</span>
<span class="c1">// Returns the total number of pairs found</span>
<span class="kt">int</span> <span class="nf">find_repeats</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">device_input</span><span class="p">,</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">device_output</span><span class="p">)</span> <span class="p">{</span>

    <span class="c1">// CS149 TODO:</span>
    <span class="c1">//</span>
    <span class="c1">// Implement this function. You will probably want to</span>
    <span class="c1">// make use of one or more calls to exclusive_scan(), as well as</span>
    <span class="c1">// additional CUDA kernel launches.</span>
    <span class="c1">//    </span>
    <span class="c1">// Note: As in the scan code, the calling code ensures that</span>
    <span class="c1">// allocated arrays are a power of 2 in size, so you can use your</span>
    <span class="c1">// exclusive_scan function with them. However, your implementation</span>
    <span class="c1">// must ensure that the results of find_repeats are correct given</span>
    <span class="c1">// the actual array length.</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">flags_arr</span><span class="p">;</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">flags_sum_arr</span><span class="p">;</span>

  <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">flags_arr</span><span class="p">,</span> <span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
  <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">flags_sum_arr</span><span class="p">,</span> <span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

  <span class="k">const</span> <span class="kt">int</span> <span class="n">threadsPerBlock</span> <span class="o">=</span> <span class="mi">512</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">length</span> <span class="o">+</span> <span class="n">threadsPerBlock</span> <span class="o">-</span><span class="mi">1</span> <span class="p">)</span> <span class="o">/</span> <span class="n">threadsPerBlock</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">repeat_indices_count</span><span class="p">;</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">arr</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
  <span class="c1">// cudaMemcpy(arr, device_input, length*sizeof(int), cudaMemcpyDeviceToHost);</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
  <span class="c1">// printf("input arr2\n");</span>
  <span class="c1">// print_arr(arr, length);</span>
  <span class="n">flag_repeats_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">device_input</span><span class="p">,</span> <span class="n">flags_arr</span><span class="p">,</span> <span class="n">length</span><span class="p">);</span>
  <span class="c1">// cudaMemcpy(arr, flags_arr, length*sizeof(int), cudaMemcpyDeviceToHost);</span>
  <span class="c1">// printf("flags arr\n");</span>
  <span class="c1">// print_arr(arr, length);</span>
  <span class="n">cudaScan</span><span class="p">(</span><span class="n">flags_arr</span><span class="p">,</span> <span class="n">flags_arr</span><span class="o">+</span><span class="n">length</span><span class="p">,</span> <span class="n">flags_sum_arr</span><span class="p">)</span> <span class="p">;</span>
  <span class="n">flags_extract_indices</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">flags_sum_arr</span><span class="p">,</span> <span class="n">device_output</span><span class="p">,</span> <span class="n">length</span><span class="p">);</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">repeat_indices_count</span><span class="p">,</span> <span class="n">flags_sum_arr</span><span class="o">+</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
  <span class="n">free</span><span class="p">(</span><span class="n">arr</span><span class="p">);</span>

  <span class="k">return</span> <span class="n">repeat_indices_count</span><span class="p">;</span> 

<span class="p">}</span>


<span class="c1">//</span>
<span class="c1">// cudaFindRepeats --</span>
<span class="c1">//</span>
<span class="c1">// Timing wrapper around find_repeats. You should not modify this function.</span>
<span class="kt">double</span> <span class="nf">cudaFindRepeats</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">input</span><span class="p">,</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">output_length</span><span class="p">)</span> <span class="p">{</span>

    <span class="kt">int</span> <span class="o">*</span><span class="n">device_input</span><span class="p">;</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">device_output</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">rounded_length</span> <span class="o">=</span> <span class="n">nextPow2</span><span class="p">(</span><span class="n">length</span><span class="p">);</span>
    
    <span class="c1">// printf("input arr1\n");</span>
    <span class="c1">// print_arr(input, length);  </span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_input</span><span class="p">,</span> <span class="n">rounded_length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_output</span><span class="p">,</span> <span class="n">rounded_length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">device_input</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="kt">double</span> <span class="n">startTime</span> <span class="o">=</span> <span class="n">CycleTimer</span><span class="o">::</span><span class="n">currentSeconds</span><span class="p">();</span>
    
    <span class="kt">int</span> <span class="n">result</span> <span class="o">=</span> <span class="n">find_repeats</span><span class="p">(</span><span class="n">device_input</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">device_output</span><span class="p">);</span>

    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="kt">double</span> <span class="n">endTime</span> <span class="o">=</span> <span class="n">CycleTimer</span><span class="o">::</span><span class="n">currentSeconds</span><span class="p">();</span>

    <span class="c1">// set output count and results array</span>
    <span class="o">*</span><span class="n">output_length</span> <span class="o">=</span> <span class="n">result</span><span class="p">;</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">device_output</span><span class="p">,</span> <span class="n">length</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

    <span class="c1">// printf("output length:%d\n", *output_length);</span>
    <span class="c1">// printf("output indices\n");</span>
    <span class="c1">// print_arr(output, length);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_input</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_output</span><span class="p">);</span>

    <span class="kt">float</span> <span class="n">duration</span> <span class="o">=</span> <span class="n">endTime</span> <span class="o">-</span> <span class="n">startTime</span><span class="p">;</span> 
    <span class="k">return</span> <span class="n">duration</span><span class="p">;</span>
<span class="p">}</span>

</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ yhrun -p gpu_v100 ./cudaScan -m find_repeats -n 8  -i ones
---------------------------------------------------------
Found 4 CUDA devices
Device 0: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 1: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 2: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 3: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
---------------------------------------------------------
Array size: 8
flags arr
0:1 1:1 2:1 3:1 4:1 5:1 6:1 7:0
output length:7
output indices
0:0 1:1 2:2 3:3 4:4 5:5 6:6 7:0
flags arr
0:1 1:1 2:1 3:1 4:1 5:1 6:1 7:0
output length:7
output indices
0:0 1:1 2:2 3:3 4:4 5:5 6:6 7:0
flags arr
0:1 1:1 2:1 3:1 4:1 5:1 6:1 7:0
output length:7
output indices
0:0 1:1 2:2 3:3 4:4 5:5 6:6 7:0
Student GPU time: 0.199 ms
Find_repeats outputs are correct!
</code></pre></div></div> <h2 id="part-3-renderer">Part 3: renderer</h2> <p>Install opengl library before compile the project</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install -c anaconda pyopengl
conda install -c anaconda freeglut
</code></pre></div></div> <p>Install opengl library using conda and then compile c code that use opengl library with Makefile</p> <p>The thing to note about is that we need to include conda library path in CFLAGS and LDFLAGS</p> <h3 id="step-by-step-guide">Step-by-Step Guide</h3> <ol> <li> <strong>Install OpenGL using conda</strong>: <div class="language-sh highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> opengl_env
conda activate opengl_env
conda <span class="nb">install</span> <span class="nt">-c</span> anaconda pyopengl
conda <span class="nb">install</span> <span class="nt">-c</span> anaconda freeglut
</code></pre></div> </div> </li> <li> <strong>Write Your C Code</strong>: <ul> <li>Create a file named <code class="language-plaintext highlighter-rouge">main.c</code> with the following content: <div class="language-c highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;GL/glut.h&gt;</span><span class="cp">
</span>
<span class="kt">void</span> <span class="nf">display</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">glClear</span><span class="p">(</span><span class="n">GL_COLOR_BUFFER_BIT</span><span class="p">);</span>
    <span class="n">glBegin</span><span class="p">(</span><span class="n">GL_TRIANGLES</span><span class="p">);</span>
    <span class="n">glVertex2f</span><span class="p">(</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">);</span>
    <span class="n">glVertex2f</span><span class="p">(</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">);</span>
    <span class="n">glVertex2f</span><span class="p">(</span><span class="mi">0</span><span class="p">.</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">);</span>
    <span class="n">glEnd</span><span class="p">();</span>
    <span class="n">glFlush</span><span class="p">();</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">glutInit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">);</span>
    <span class="n">glutCreateWindow</span><span class="p">(</span><span class="s">"OpenGL Setup Test"</span><span class="p">);</span>
    <span class="n">glutDisplayFunc</span><span class="p">(</span><span class="n">display</span><span class="p">);</span>
    <span class="n">glutMainLoop</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> </div> </li> </ul> </li> <li> <strong>Create a Makefile</strong>: <ul> <li>Create a file named <code class="language-plaintext highlighter-rouge">Makefile</code> with the following content: <div class="language-makefile highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="nv">CC</span> <span class="o">=</span> gcc
<span class="nv">CFLAGS</span> <span class="o">=</span> <span class="nt">-I</span><span class="p">$(</span>CONDA_PREFIX<span class="p">)</span>/include
<span class="nv">LDFLAGS</span> <span class="o">=</span> <span class="nt">-L</span><span class="p">$(</span>CONDA_PREFIX<span class="p">)</span>/lib <span class="nt">-lGL</span> <span class="nt">-lGLU</span> <span class="nt">-lglut</span>

<span class="nl">all</span><span class="o">:</span> <span class="nf">main</span>

<span class="nl">main</span><span class="o">:</span> <span class="nf">main.o</span>
    <span class="err">$(CC)</span> <span class="err">-o</span> <span class="err">main</span> <span class="err">main.o</span> <span class="err">$(LDFLAGS)</span>

<span class="nl">main.o</span><span class="o">:</span> <span class="nf">main.c</span>
    <span class="err">$(CC)</span> <span class="err">-c</span> <span class="err">main.c</span> <span class="err">$(CFLAGS)</span>

<span class="nl">clean</span><span class="o">:</span>
    <span class="err">rm</span> <span class="err">-f</span> <span class="err">main</span> <span class="err">main.o</span>
</code></pre></div> </div> </li> </ul> </li> <li> <strong>Compile and Run Your Code</strong>: <ul> <li>Run the following commands in your terminal: <div class="language-sh highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>make
./main
</code></pre></div> </div> </li> </ul> </li> </ol> <h3 id="explanation">Explanation</h3> <ul> <li> <strong>CC</strong>: Specifies the compiler to use (gcc in this case).</li> <li> <strong>CFLAGS</strong>: Specifies the include directory for the OpenGL headers.</li> <li> <strong>LDFLAGS</strong>: Specifies the library directory and the libraries to link against (<code class="language-plaintext highlighter-rouge">-lGL</code>, <code class="language-plaintext highlighter-rouge">-lGLU</code>, <code class="language-plaintext highlighter-rouge">-lglut</code>).</li> <li> <strong>all</strong>: The default target that builds the <code class="language-plaintext highlighter-rouge">main</code> executable.</li> <li> <strong>main</strong>: The target that links the object file to create the executable.</li> <li> <strong>main.o</strong>: The target that compiles the source file into an object file.</li> <li> <strong>clean</strong>: A target to clean up the compiled files.</li> </ul> <p>This Makefile ensures that the compiler and linker use the correct paths for the OpenGL headers and libraries installed via conda.</p> <p>One problem is that how to get orders of drawing a same pixel when multiple circles overlap at the same pixel.</p> <p>The hints say that I can use prefix sum to help with this assignment but I don’t know how to do that.</p> <p>I know that once we have caculation order array for each pixel then we can parallize the image drawing for eall pixels in parallel.</p> <p>Take a look at shadePixel</p> <p>Run pixel rendering in parallel instead of rendering circles in parllel. <a href="https://github.com/ClaudiaRaffaelli/CUDA-Renderer/blob/master/cudaRenderer.cu" rel="external nofollow noopener" target="_blank">Ref repo</a></p> <p>The naive solution is slow because for each pixel thread it has to iterate all circles to see if each circle contributes to current pixel.</p> <p>The good news is that we don’t need to worry about the order issue and the correctness is guaranteed.</p> <p>I don’t know how prefix sum can be used to solve this problem yet.</p> <p><a href="https://github.com/MizukiCry/CS149/blob/c257598c102e438b9744ea133417e213188cd7ee/asst3/render/cudaRenderer.cu" rel="external nofollow noopener" target="_blank">Ref repo that use prefix sum to create unique offset for pixels of all circles so that each circles thread can draw pixels in correct order covered by all circles </a></p> <h3 id="first-naive-solution-render-all-pixels-in-parallel">First naive solution: render all pixels in parallel</h3> <p><a href="https://github.com/kykim0/asst3/blob/d0550cbd3a2037895f7a0bb7d1a52c3a40131fb1/render/cudaRenderer.cu" rel="external nofollow noopener" target="_blank">Ref implementation that uses similar idea to render pixel in parallel</a></p> <p>The naive solution is slow because for each pixel thread it has to iterate all circles to see if each circle contributes to current pixel.</p> <p>Code:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span>
<span class="n">CudaRenderer</span><span class="o">::</span><span class="n">render</span><span class="p">()</span> <span class="p">{</span>

    <span class="c1">// 256 threads per block is a healthy number</span>
    <span class="n">dim3</span> <span class="n">blockDim</span><span class="p">(</span><span class="n">THREADS_PER_BLOCK_X</span><span class="p">,</span> <span class="n">THREADS_PER_BLOCK_Y</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">gridDim</span><span class="p">((</span><span class="n">image</span><span class="o">-&gt;</span><span class="n">width</span><span class="o">+</span><span class="n">THREADS_PER_BLOCK_X</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">THREADS_PER_BLOCK_X</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="o">-&gt;</span><span class="n">height</span> <span class="o">+</span> <span class="n">THREADS_PER_BLOCK_Y</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">THREADS_PER_BLOCK_Y</span> <span class="p">);</span>

    <span class="n">kernelRenderPixels</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridDim</span><span class="p">,</span> <span class="n">blockDim</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>


<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernelRenderPixels</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">pixel_X</span> <span class="o">=</span>  <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">pixel_Y</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>


  <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">imageWidth</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">height</span> <span class="o">=</span> <span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">imageHeight</span><span class="p">;</span>

  <span class="c1">// float boxL = pixel_X -1;</span>
  <span class="c1">// float boxR = pixel_X +1;</span>
  <span class="c1">// float boxT = pixel_Y - 1;</span>
  <span class="c1">// float boxB = pixel_Y + 1;</span>


  <span class="k">if</span> <span class="p">(</span><span class="n">pixel_X</span> <span class="o">&gt;=</span> <span class="n">width</span> <span class="o">||</span> <span class="n">pixel_Y</span> <span class="o">&gt;=</span> <span class="n">height</span><span class="p">)</span>
      <span class="k">return</span><span class="p">;</span>

  <span class="n">float4</span><span class="o">*</span> <span class="n">imgPtr</span> <span class="o">=</span> <span class="p">(</span><span class="n">float4</span><span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">imageData</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">pixel_Y</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">pixel_X</span><span class="p">)]);</span>
  <span class="kt">int</span> <span class="n">num_circles</span> <span class="o">=</span> <span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">numCircles</span> <span class="p">;</span>

  <span class="kt">float</span> <span class="n">invWidth</span> <span class="o">=</span> <span class="mf">1.</span><span class="n">f</span> <span class="o">/</span> <span class="n">width</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">invHeight</span> <span class="o">=</span> <span class="mf">1.</span><span class="n">f</span> <span class="o">/</span> <span class="n">height</span><span class="p">;</span>

  <span class="n">float2</span> <span class="n">pixelCenterNorm</span> <span class="o">=</span> <span class="n">make_float2</span><span class="p">(</span><span class="n">invWidth</span> <span class="o">*</span> <span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">pixel_X</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="n">f</span><span class="p">),</span>
                                       <span class="n">invHeight</span> <span class="o">*</span> <span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">pixel_Y</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="n">f</span><span class="p">));</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">circle_index</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">circle_index</span><span class="o">&lt;</span> <span class="n">num_circles</span><span class="p">;</span> <span class="n">circle_index</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">circle_index3</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">circle_index</span><span class="p">;</span>  

    <span class="n">float3</span> <span class="n">p</span> <span class="o">=</span> <span class="o">*</span><span class="p">(</span><span class="n">float3</span><span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">cuConstRendererParams</span><span class="p">.</span><span class="n">position</span><span class="p">[</span><span class="n">circle_index3</span><span class="p">]);</span>
    <span class="c1">// float  rad = cuConstRendererParams.radius[circle_index];</span>
    <span class="n">shadePixel</span><span class="p">(</span><span class="n">circle_index</span><span class="p">,</span> <span class="n">pixelCenterNorm</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">imgPtr</span><span class="p">);</span>

  <span class="p">}</span>

<span class="p">}</span>
</code></pre></div></div> <p>Output:</p> <p>The output shows that render time reduces 17x with gpu.</p> <p>And it’s correct and passes the correctness check..</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@ln101%tianhe2-K render]$ yhrun -p gpu_v100 ./render -r cpuref rand10k
Rendering to 1024x1024 image
Loaded scene with 10000 circles

Running benchmark, 1 frames, beginning at frame 0 ...
Dumping frames to output_xxx.ppm
Wrote image file output_0000.ppm
Clear:    661.9973 ms
Advance:  0.0079 ms
Render:   390.4090 ms
Total:    1052.4143 ms
File IO:  61.6616 ms

Overall:  1.1314 sec (note units are seconds)



[nsccgz_qylin_1@ln101%tianhe2-K render]$ yhrun -p gpu_v100 ./render -r cuda rand10k
Rendering to 1024x1024 image
Loaded scene with 10000 circles
---------------------------------------------------------
Initializing CUDA for CudaRenderer
Found 4 CUDA devices
Device 0: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 1: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 2: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 3: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
---------------------------------------------------------

Running benchmark, 1 frames, beginning at frame 0 ...
Dumping frames to output_xxx.ppm
Copying image data from device
Wrote image file output_0000.ppm
Clear:    0.1329 ms
Advance:  0.0057 ms
Render:   23.3001 ms
Total:    23.4387 ms
File IO:  99.4054 ms

Overall:  0.1410 sec (note units are seconds)
</code></pre></div></div> <p>Correctness check</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nsccgz_qylin_1@ln101%tianhe2-K render]$ yhrun -p gpu_v100 ./render -r cuda rand10k -c
Rendering to 1024x1024 image
Loaded scene with 10000 circles
Loaded scene with 10000 circles
---------------------------------------------------------
Initializing CUDA for CudaRenderer
Found 4 CUDA devices
Device 0: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 1: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 2: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
Device 3: Tesla V100-SXM2-16GB
   SMs:        80
   Global mem: 16160 MB
   CUDA Cap:   7.0
---------------------------------------------------------

Running benchmark, 1 frames, beginning at frame 0 ...
Dumping frames to output_xxx.ppm
Copying image data from device
Wrote image file output_0000.ppm
Copying image data from device
***************** Correctness check passed **************************
Clear:    0.1450 ms
Advance:  0.0058 ms
Render:   23.3115 ms
Total:    23.4624 ms
File IO:  69.5518 ms

Overall:  0.4857 sec (note units are seconds)
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/learning-based-memory-allocation-for-c-server-workloads-summary/">Learning-based memory allocation for C++ server workloads summary</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/my-question/">my question:</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/binary-search-algorithm-variant/">Binary search algorithm variant</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/docker-rocksdb-build/">Docker Rocksdb build</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/difference-between-dockerfile-and-docker-compose/">Difference between Dockerfile and Docker Compose</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhutao Zhuang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: January 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-chibicc-simple-c-compiler-return-keyword",title:"chibicc - Simple c compiler return keyword",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-return-keyword/"}},{id:"post-learned-idnex-survey",title:"Learned idnex survey",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/learned-index-survery/"}},{id:"post-chibicc-c-compiler-multi-char-variable-name",title:"chibicc C compiler - multi char variable name",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-multi-char-var-name/"}},{id:"post-c-and-linux-kernel-memory-allocation",title:"C++ and linux kernel memory allocation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/cpp-interview-questions/"}},{id:"post-chibicc-c-compiler-parser-review-and-expression-evaluator",title:"chibicc C compiler - parser review and expression evaluator",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chibicc-parser-review-and-calculator/"}},{id:"post-imperative-programming-vs-declarative-programming",title:"Imperative programming vs. Declarative programming",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/imperative-programming/"}},{id:"post-autodiff-implementation-kernel-and-memory-management",title:"Autodiff implementation - kernel and memory management",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign2/"}},{id:"post-hash-in-cpp",title:"Hash in cpp",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hash-in-cpp/"}},{id:"post-computer-basics",title:"Computer basics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/some-basic-computer-knowledge/"}},{id:"post-rdma",title:"Rdma",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/rdma/"}},{id:"post-topo-sort",title:"Topo sort",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/topo-sort/"}},{id:"post-autodiff-implementation",title:"Autodiff implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/uw-sysml-assign/"}},{id:"post-palindrome-substring-partition",title:"palindrome substring partition",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/palindrome-substring-partition/"}},{id:"post-stf-cs149-flash-attention",title:"Stf CS149 flash attention",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149gpt/"}},{id:"post-speed-up-matrix-multiplication-2",title:"Speed up matrix multiplication 2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-mul-comparison/"}},{id:"post-elf-loading",title:"Elf Loading",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/elf-loading/"}},{id:"post-stf-cs149-parallel-programming-assign3",title:"Stf CS149 Parallel Programming - Assign3",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign3/"}},{id:"post-stf-cs149-parallel-programming-lecture11-cache-coherence",title:"Stf CS149 Parallel Programming - Lecture11 - Cache coherence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture11-cache-coherence/"}},{id:"post-stf-cs149-parallel-programming-lecture-7-cuda-programming-model",title:"Stf CS149 Parallel Programming - Lecture 7 - Cuda programming model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-lecture7-cuda/"}},{id:"post-stf-cs149-parallel-programming-assign2",title:"Stf CS149 Parallel Programming - Assign2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs149-assign2/"}},{id:"post-stf-cs149-parallel-programming-lecture-5-amp-6-performance-optimization",title:"Stf CS149 Parallel Programming - Lecture 5&amp;6 - Performance optimization",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-lecture-takeaway/"}},{id:"post-stf-cs149-parallel-programming-assign1",title:"Stf CS149 Parallel Programming - Assign1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/stf-cs149-assign1/"}},{id:"post-ssh-display-image-on-local-server",title:"ssh display image on local server",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/ssh-display-img/"}},{id:"post-c-compiler-single-letter-local-variable",title:"C compiler - single letter local variable",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-single-letter-ident/"}},{id:"post-c-compiler-parse-example-walkthrough",title:"C compiler - parse example walkthrough",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/statement-and-comparison/"}},{id:"post-linux-get-cpu-time-and-wall-clock-time",title:"Linux get cpu time and wall clock time",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/get-function-cpu-time/"}},{id:"post-simple-c-compiler-unary",title:"Simple c compiler unary",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-unary/"}},{id:"post-simple-c-compiler-gen-expr",title:"Simple c compiler gen expr",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-gen-expr/"}},{id:"post-python-pyplot-trick",title:"Python pyplot trick",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-plot-trick/"}},{id:"post-simple-lru-cache-cpp-implementation",title:"Simple lru cache cpp implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lru-cache/"}},{id:"post-python-capture-function-print-output",title:"Python capture function print output",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/capture-output-python/"}},{id:"post-simple-c-compiler",title:"Simple c compiler",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/chibicc-compiler/"}},{id:"post-calloc-and-malloc",title:"calloc and malloc",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/calloc/"}},{id:"post-lightgbm-dataset",title:"LightGBM dataset",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-dataset/"}},{id:"post-difference-between-deep-copy-and-shallow-copy-in-python",title:"Difference between deep copy and shallow copy in python",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-deep-copy/"}},{id:"post-python-package-path",title:"Python Package Path",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-package-path/"}},{id:"post-micrograd",title:"micrograd",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/micrograd/"}},{id:"post-cpp-thread-local",title:"cpp thread local",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-thread-local/"}},{id:"post-cpp-async",title:"cpp async",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cpp-async/"}},{id:"post-python-dataframe-drop-row",title:"python dataframe drop row",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/df-drop-row/"}},{id:"post-git-merge-file-from-another-branch",title:"Git merge file from another branch",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/git-merge-file-from-another-branch/"}},{id:"post-efficiency-tips",title:"Efficiency tips",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/efficiency-tips/"}},{id:"post-speed-up-matrix-multiplication",title:"Speed up matrix multiplication",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/matrix-multiplication/"}},{id:"post-how-to-write-research-paper",title:"How to write research paper",description:"How to write research paper",section:"Posts",handler:()=>{window.location.href="/blog/2024/how-to-write-research-paper/"}},{id:"post-fast-nano-gpt-training",title:"Fast nano-gpt training",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/gpt-fast/"}},{id:"post-system-for-machine-learning-papers",title:"System for machine learning papers",description:"sysml papers",section:"Posts",handler:()=>{window.location.href="/blog/2024/sysml-papers/"}},{id:"post-nano-gpt-and-transformer",title:"nano-gpt and Transformer",description:"llm",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformer/"}},{id:"post-pytorch-tensor-to",title:"pytorch tensor.to",description:"pytorch",section:"Posts",handler:()=>{window.location.href="/blog/2024/pytorch/"}},{id:"post-install-neovim-with-old-glibc",title:"Install neovim with old glibc",description:"vim",section:"Posts",handler:()=>{window.location.href="/blog/2024/install-neovim/"}},{id:"post-llm-c",title:"llm.c",description:"llm minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/llm-c/"}},{id:"post-basic-digital-electronic",title:"Basic digital electronic",description:"transistor",section:"Posts",handler:()=>{window.location.href="/blog/2024/digital-electronic/"}},{id:"post-k8s-advance",title:"K8s Advance",description:"k8s minikune",section:"Posts",handler:()=>{window.location.href="/blog/2024/k8s-advance/"}},{id:"post-difference-between-dockerfile-and-docker-compose",title:"Difference between Dockerfile and Docker Compose",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/difference-between-dockerfile-and-docker-compose-d6ebdc687785?source=rss-da1663a42461------2","_blank")}},{id:"post-docker-rocksdb-build",title:"Docker Rocksdb build",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/docker-rocksdb-build-18a0bf0e0bb0?source=rss-da1663a42461------2","_blank")}},{id:"post-k3s-beginner",title:"K3s beginner",description:"k3s",section:"Posts",handler:()=>{window.location.href="/blog/2024/k3s/"}},{id:"post-docker-rocksdb",title:"Docker RocksDB",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-rocksdb/"}},{id:"post-docker-beginner",title:"Docker beginner",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/docker-file-compose-diff/"}},{id:"post-git",title:"Git",description:"git",section:"Posts",handler:()=>{window.location.href="/blog/2024/git/"}},{id:"post-lightgbm-usage-and-implementation",title:"LightGBM usage and implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/lightgbm-usage/"}},{id:"post-backpropogation-c-implementation",title:"Backpropogation C++ Implementation",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/back-propagation/"}},{id:"post-conda-usage",title:"Conda usage",description:"Artificial Intelligence",section:"Posts",handler:()=>{window.location.href="/blog/2024/conda/"}},{id:"post-install-k8s-cluster-with-3-ubuntu-nodes",title:"Install K8s cluster with 3 ubuntu nodes",description:"cloud",section:"Posts",handler:()=>{window.location.href="/blog/2024/cloud/"}},{id:"post-convert-svg-figures-to-pdf-latex-before-submitting-to-arxiv",title:"Convert SVG figures to pdf_latex before submitting to arxiv",description:"Convert SVG figures to pdf_latex before submitting to arxiv",section:"Posts",handler:()=>{window.location.href="/blog/2024/arxiv-cleaner/"}},{id:"post-binary-search-algorithm-variant",title:"Binary search algorithm variant",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/binary-search-algorithm-variant-9b5310473471?source=rss-da1663a42461------2","_blank")}},{id:"post-my-question",title:"my question:",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/my-question-a69930f167f0?source=rss-da1663a42461------2","_blank")}},{id:"post-learning-based-memory-allocation-for-c-server-workloads-summary",title:"Learning-based memory allocation for C++ server workloads summary",description:"",section:"Posts",handler:()=>{window.open("https://bilyz.medium.com/learning-based-memory-allocation-for-c-server-workloads-summary-479e9cd6d6f6?source=rss-da1663a42461------2","_blank")}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-paper-dumpkv-accepted-by-vldb-39-25",title:"A paper(DumpKV) accepted by VLDB&#39;25",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%68%75%61%6E%67%7A%68%75%74%61%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/BilyZ98","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>